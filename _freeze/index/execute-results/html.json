{
  "hash": "d3a043b7489aa2f20e69b8db3df3bcf2",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Restricted Boltzmann Machines\"\nsubtitle: \"Theory of RBMs and Applications\"\nauthor: \"Jessica Wells and Jason Gerstenberger (Advisor: Dr. Cohen)\"\ndate: '`r Sys.Date()`'\nformat:\n  html:\n    code-fold: true\ncourse: Capstone Projects in Data Science IDC 6940\nuniversity: University of West Florida, Pensacola, FL\nbibliography: references.bib # file contains bibtex for references\n#always_allow_html: true # this allows to get PDF with HTML features\nself-contained: true\nexecute: \n  warning: false\n  message: false\n  freeze: auto\neditor: \n  markdown: \n    wrap: 72\n---\n\n\n\n\n## Introduction\n\n### Background\n\nRestricted Boltzmann Machines (RBM) are a type of neural network that\nhas been around since the 1980s. As a reminder to the reader, machine\nlearning is generally divided into 3 categories: supervised learning\n(examples: classification tasks, regression), unsupervised learning\n(examples: clustering, dimensionality reduction, generative modeling),\nand reinforcement learning (examples: gaming/robotics). RBMs are\nprimarily used for unsupervised learning tasks like dimensionality\nreduction and feature extraction, which help prepare datasets for\nmachine learning models that may later be trained using supervised\nlearning. They also have other applications which will be discussed\nfurther later.\n\nLike Hopfield networks, Boltzmann machines are undirected graphical\nmodels, but they are different in that they are stochastic and can have\nhidden units. Both models are energy-based, meaning they learn by\nminimizing an energy function for each model\n[@smolensky1986information]. Boltzmann machines use a sigmoid activation\nfunction, which allows for the model to be probabilistic.\n\nIn the \"Restricted\" Boltzmann Machine, there are no interactions between\nneurons in the visible layer or between neurons in the hidden layer,\ncreating a bipartite graph of neurons. Below is a diagram taken from\nGoodfellow, et al. [@Goodfellow-et-al-2016] (p. 577) for visualization\nof the connections. \n\n![](markov_net.png){width=\"60%\"} <br>\n\n<p>(Figure 1)</p>\n\n\nGoodfellow, et al. discuss the expense in drawing samples for most\nundirected graphical models; however, the RBM allows for block Gibbs\nsampling (p. 578) where the network alternates between sampling all\nhidden units simultaneously (etc. for visible). Derivatives are also\nsimplified by the fact that the energy function of the RBM is a linear\nfunction of it's parameters, which will be seen further in Methods.\n\nRBMs are trained using a process called Contrastive Divergence (CD)\n[@hinton2002training] where the weights are updated to minimize the\ndifference between samples from the data and samples from the model.\nLearning rate, batch size, and number of hidden units are all\nhyperparameters that can affect the ability of the training to converge\nsuccessfully and learn the underlying structure of the data.\n\n### Applications\n\nRBMs are probably best known for their success in collaborative\nfiltering. The RBM model was used in the Netflix Prize competition to\npredict user ratings for movies, with the result that it outperformed\nthe Singular Value Decomposition (SVD) method that was state-of-the-art\nat the time [@salakhutdinov2007restricted]. They have also been trained\nto recognize handwritten digits, such as the MNIST dataset\n[@hinton2002training].\n\nRBMs have been successfully used to distinguish normal and anomalous\nnetwork traffic. Their potential use in improving network security for\ncompanies in the future is promising. There is slow progress in network\nanomaly detection due to the difficulty of obtaining datasets for\ntraining and testing networks. Clients are often reluctant to divulge\ninformation that could potentially harm their networks. In a real-life\ndataset where one host had normal traffic and one was infected by a bot,\ndiscriminative RBM (DRBM) was able to successfully distinguish the\nnormal from anomalous traffic. DRBM doesn't rely on knowing the data\ndistribution ahead of time, which is useful, except that it also causes\nthe DRBM to overfit. As a result, when trying to use the same trained\nRBM on the KDD '99 training dataset performance declined.\n[@fiore2013network]\n\nRBMs can provide greatly improved classification of brain disorders in\nMRI images. Generative Adversarial Networks (GANs) use two neural\nnetworks: a generator which generates fake data, and a discriminator\nwhich tries to distinguish between real and fake data. Loss from the\ndiscriminator is backpropagated through the generator so that both part\nare trained simultaneously. The RBM-GAN uses RBM features from real MRI\nimages as inputs to the generator. Features from the discriminator are\nthen used as inputs to a classifier. [@aslan2023automated]\n\nThe many-body quantum wavefunction, which describes the quantum state of\na system of particles is difficult to compute with classical computers.\nRBMs have been used to approximate it using variational Monte Carlo\nmethods. [@melko2019restricted]\n\nRBMs are notoriously slow to train. The process of computing the\nactivation probability requires the calculation of vector dot products.\nLean Constrastive Divergence (LCD) is a method which adds two techniques\nto speed up the process of training RBMs. The first is bounds-based\nfiltering where upper and lower bounds of the probability select only a\nrange of dot products to perform. Second, the delta product involves\nonly recalculating the changed portions of the vector dot product.\n[@ning2018lcd]\n\n## Methods\n\nBelow is the energy function of the RBM.\n\n$$\nE(v,h) = - \\sum_{i} a_i v_i - \\sum_{j} b_j h_j - \\sum_{i} \\sum_{j} v_i w_{i,j} h_j\n$$ {#eq-energy} where v<sub>i</sub> and h<sub>j</sub> represent visible\nand hidden units; a<sub>i</sub> and b<sub>j</sub> are the bias terms of\nthe visible and hidden units; and each w<sub>{i,j}</sub> (weight)\nelement represents the interaction between the visible and hidden units.\n[@fischer2012introduction]\n\nIt is well known neural networks are prone to overfitting and often\ntechniques such as early stopping are employed to prevent it. Some\nmethods to prevent overfitting in RBMs are weight decay (L2\nregularization), dropout, dropconnect, and weight uncertainty\n[@zhang2018overview]. Dropout is a fairly well known concept in deep\nlearning. For example, a dropout value of 0.3 added to a layer means 30%\nof neurons are dropped during training. This prevents the network from\nlearning certain features too well. L2 regularization is also a commonly\nemployed technique in deep learning. It assigns a penalty to large\nweights to allow for more generalization. Dropconnect is a method where\na subset of weights within the network are randomly set to zero during\ntraining. Weight uncertainty is where each weight in the network has\nit's own probability distribution vice a fixed value. This addition\nallows the network to learn more useful features.\n\nIf the learning rate is too high, training of the model may not\nconverge. If it is too low, training may take a long time. To fully\nmaximize the training of the model it is helpful to reduce the learning\nrate over time. This is known as learning rate decay.\n[@hinton2010practical]\n\n#### Model Categories\n\nWe train Logistic Regression (with and without RBM features as input),\nFeed Forward Network (with and without RBM features as input), and\nConvolutional Neural Network. Below is a brief reminder of the basics of\neach model.<br>\n\nFor the models incoroporating the RBM, we take the Fashion MNIST\nfeatures/pixels and train the RBM (unsupervised learning) to extract\nhidden features from the visible layer and then feed these features into\neither logistic regression or feed forward network. We then use the\ntrained model to predict labels for the test data, evaluating how well\nthe RBM-derived features perform in a supervised classification task.\n\n##### 1. Logistic Regression\n\nMathematically, the concept behind binary logistic regression is the\nlogit (the natural logarithm of an odds ratio)[@peng2002introduction].\nHowever, since we have 10 labels, our classification task falls into\n\"Multinomial Logistic Regression.\"\n\n$$\nP(Y = k | X) = \\frac{e^{\\beta_{0k} + \\beta_k^T X}}{\\sum_{l=1}^{K} e^{\\beta_{0l} + \\beta_l^T X}}\n$$ {#eq-probability-lr}\n\n##### 2. Simple Feed Forward Neural Network\n\nThe feed forward network (FNN) is one where information flows in one\ndirection from input to output with no loops or feedback. There can be\nzero hidden layers in between (called single FNN) or one or more hidden\nlayers (multilayer FNN). <br> [@sazli2006brief]\n![](fnn_diagram.png){width=\"60%\"}<br>\n\n<p>(Figure 2)</p>\n\n##### 3. Convolutional Neural Network\n\nThe convolutional neural network (CNN) is a type of feed forward network\nexcept that unlike the traditional ANN, CNNs are primarily used for\npattern recognition with images\n[@oshea2015introductionconvolutionalneuralnetworks]. The CNN has 3\nlayers which are stacked to form the full CNN: convolutional, pooling,\nand fully-connected layers. ![](cnn_diagram_mnist.png){width=\"60%\"}<br>\n\n<p>(Figure 3)</p>\n\n##### Below is our Process for creating the RBM:\n\nStep 1: We first initialize the RBM with random weights and biases and\nset visible units to 784 and hidden units to 256. We also set the number\nof contrastive divergence steps (k) to 1. </br> Step 2: Sample hidden\nunits from visible. The math behind computing the hidden unit\nactivations from the given input can be seen in [@eq-probability-rbm1]\n[@fischer2012introduction] where the probability is used to sample from\nthe Bernoulli distribution. </br> $$\np(H_i = 1 | \\mathbf{v}) = \\sigma \\left( \\sum_{j=1}^{m} w_{ij} v_j + c_i \\right)\n$$ {#eq-probability-rbm1}\n\nwhere p(.) is the probability of the i<sup>th</sup> hidden state being\nactivated (=1) given the visible input vector. σ is the sigmoid\nactivation function (below) which maps the weighted sum to a probability\nbetween 0 and 1. m is the number of visible units. w<sub>ij</sub> is the\nweight connecting visible unit j to hidden unit i. v<sub>j</sub> is the\nvalue of the j<sup>th</sup> visible unit. and c<sub>i</sub> is the bias\nterm for the hidden unit. $$\n\\sigma(x) = \\frac{1}{1 + e^{-x}}\n$$ {#eq-sigma}\n\nStep 3: Sample visible units from hidden. The math behind computing\nvisible unit activations from the hidden layer can be seen in\n[@eq-probability-rbm2] [@fischer2012introduction] Visible states are\nsampled using the Bernoulli distribution. This way we can see how well\nthe RBM learned from the inputs. </br> $$\np(V_j = 1 | \\mathbf{h}) = \\sigma \\left( \\sum_{i=1}^{n} w_{ij} h_i + b_j \\right)\n$$ {#eq-probability-rbm2}\n\nwhere p(.) is the probability of the i<sup>th</sup> visible unit being\nactivated (=1) given the hidden vector h. σ is same as above. n is the\nnumber of hidden units. w<sub>ij</sub> is the weight connecting hidden\nunit i to visible unit j. b<sub>j</sub> is the bias term for the\nj<sup>th</sup> visible unit.\\\nStep 4: K=1 steps of Contrastive Divergence (Feed Forward, Feed\nBackward) which executes steps 2 and 3. Contrastive Divergence updates\nthe RBM’s weights by minimizing the difference between the original\ninput and the reconstructed input created by the RBM. </br> Step 5: Free\nenergy is computed. The free energy F is given by the logarithm of the\npartition function Z [@oh2020entropy] where the partition function is\n<br> $$\nZ(\\theta) \\equiv \\sum_{v,h} e^{-E(v,h; \\theta)}\n$$ {#eq-partition} and the free energy function is <br> $$\nF(\\theta) = -\\ln Z(\\theta)\n$$ {#eq-free-energy} where lower free energy means the RBM learned the\nvisible state well.\n\nStep 6: Train the RBM. Model weights updated via gradient descent.<br>\nStep 7: Feature extraction for classification with LR. The hidden layer\nactivations of the RBM are used as features for Logistic Regression and\nFeed Forward Network.\n\n##### Hyperparameter Tuning\n\nWe use the Tree-structured Parzen Estimator algorithm from Optuna\n[@akiba2019optuna] to tune the hyperparameters of the RBM and the\nclassifier models, and we use MLFlow [@zaharia2018accelerating] to\nrecord and visualize the results of the hyperparameter tuning process.\nThe hyperparameters we tune include the learning rate, batch size,\nnumber of hidden units, and number of epochs.\n\n#### Metrics Used\n\n<b>1. Accuracy</b><br> Accuracy is defined as the number of correct\nclassifications divided by the total number of classifications<br> $$\n\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}} \n$${#eq-acc}\n\n<b>2. Macro F1 Score</b><br> Macro F1 score is the unweighted average of\nthe individual F1 scores of each class. It takes no regard for class\nimbalance; however, we saw earlier the classes are all balanced in\nFashion MNIST. The F1 score for each individual class is as follows \n$$\n\\text{F1} = \\frac{2 \\cdot \\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n$$ {#eq-F1-indiv}\nwhere precision for each class is <br>\n$$\n\\text{Precision} = \\frac{TP}{TP + FP}\n$$ {#eq-Precision-indiv}\nand recall for each class is <br>\n$$\n\\text{Recall} = \\frac{TP}{TP + FN}\n$$ {#eq-Recall-indivi}\nThe definitions of these terms for multiclass problems are more\ncomplicated than binary and are best displayed as examples. <br>\n\n<style type=\"text/css\">\n.tg  {border-collapse:collapse;border-spacing:0;}\n.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg .tg-gmla{border-color:inherit;font-size:16px;text-align:center;vertical-align:top}\n.tg .tg-0lax{text-align:left;vertical-align:top}\n.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}\n</style>\n\n+---------------------+---------------------------------------------+\n| **Acronymn**        | **Example for a trouser image**             |\n+=====================+=============================================+\n| TP = True Positives | the image is a trouser and the model        |\n|                     | predicts a trouser                          |\n+---------------------+---------------------------------------------+\n| TN = True Negatives | the image is not a trouser and the model    |\n|                     | predicts anything but trouser               |\n+---------------------+---------------------------------------------+\n| FP = False          | the image is anything but trouser but the   |\n| Positives           | model predicts trouser                      |\n+---------------------+---------------------------------------------+\n| FN = False          | the image is a trouser and the model        |\n| Negatives           | predicts another class (like shirt)         |\n+---------------------+---------------------------------------------+\n<p>(Table 1)</p>\n\nAs stated earlier, the individual F1 scores for each class are taken and\naveraged to compute the Macro F1 score in a multiclass problem like\nFashion MNIST.\n\n<!-- \n\n-   Detail the models or algorithms used.\n\n-   Justify your choices based on the problem and data.\n\n*The common non-parametric regression model is*\n$Y_i = m(X_i) + \\varepsilon_i$*, where* $Y_i$ *can be defined as the sum\nof the regression function value* $m(x)$ *for* $X_i$*. Here* $m(x)$ *is\nunknown and* $\\varepsilon_i$ *some errors. With the help of this\ndefinition, we can create the estimation for local averaging i.e.*\n$m(x)$ *can be estimated with the product of* $Y_i$ *average and* $X_i$\n*is near to* $x$*. In other words, this means that we are discovering\nthe line through the data points with the help of surrounding data\npoints. The estimation formula is printed below [@R-base]:*\n\n$$\nM_n(x) = \\sum_{i=1}^{n} W_n (X_i) Y_i  \\tag{1}\n$$$W_n(x)$ *is the sum of weights that belongs to all real numbers.\nWeights are positive numbers and small if* $X_i$ *is far from* $x$*.*\n\n\n*Another equation:*\n\n$$\ny_i = \\beta_0 + \\beta_1 X_1 +\\varepsilon_i\n$$\n\n-->\n\n## Analysis and Results\n\n### Data Exploration and Visualization\n\nWe use the Fashion MNIST dataset from Zalando Research\n[@xiao2017/online]. The set includes 70,000 grayscale images of clothing\nitems, 60,000 for training and 10,000 for testing. Each image is 28x28\npixels (784 pixels total). Each pixel has a value associated with it\nranging from 0 (white) to 255 (very dark) -- whole numbers only. There\nare 785 columns in total as one column is dedicated to the label.\n\n![](fmnist.png){width=\"60%\"} <br>\n\n<p>(Figure 4)</p>\n\n<b>There are 10 labels in total:</b><br>\n\n0 T-shirt/top<br> 1 Trouser<br> 2 Pullover<br> 3 Dress<br> 4 Coat<br> 5\nSandal<br> 6 Shirt<br> 7 Sneaker<br> 8 Bag<br> 9 Ankle boot<br>\n\nBelow we load the dataset.\n\n::: {#185c8557 .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nimport torch\nimport torchvision.datasets\nimport torchvision.models\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\n\n\n\ntrain_data = torchvision.datasets.FashionMNIST(\n    root=\"./data\", \n    train=True, \n    download=True, \n    transform=transforms.ToTensor()  # Converts to tensor but does NOT normalize\n)\n\ntest_data = torchvision.datasets.FashionMNIST(\n    root=\"./data\", \n    train=False, \n    download=True, \n    transform=transforms.ToTensor()  \n)\n```\n:::\n\n\n<b>Get the seventh image to show a sample</b>\n\n::: {#f1b0dd4e .cell execution_count=2}\n``` {.python .cell-code}\n# Extract the first image (or choose any index)\nimage_tensor, label = train_data[6]  # shape: [1, 28, 28]\n\n# Convert tensor to NumPy array\nimage_array = image_tensor.numpy().squeeze()  \n\n# Plot the image\nplt.figure(figsize=(5,5))\nplt.imshow(image_array, cmap=\"gray\")\nplt.title(f\"FashionMNIST Image (Label: {label})\")\nplt.axis(\"off\")  # Hide axes\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=389 height=409}\n:::\n:::\n\n\n<p>(Figure 5)</p>\n\n::: {#b863d98c .cell execution_count=3}\n``` {.python .cell-code}\ntrain_images = train_data.data.numpy()  # Raw pixel values (0-255)\ntrain_labels = train_data.targets.numpy()\nX = train_images.reshape(-1, 784)  # Flatten 28x28 images into 1D (60000, 784)\n```\n:::\n\n\n<b>Display head of the data</b><br>\n\n::: {#f7c21781 .cell execution_count=4}\n``` {.python .cell-code}\n#print(train_images[:5])\nflattened = train_images[:5].reshape(5, -1) \n\n# Create a DataFrame\ndf_flat = pd.DataFrame(flattened)\nprint(df_flat.head())\n#train_df.info() #datatypes are integers\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   0    1    2    3    4    5    6    7    8    9    ...  774  775  776  777  \\\n0    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n1    0    0    0    0    0    1    0    0    0    0  ...  119  114  130   76   \n2    0    0    0    0    0    0    0    0    0   22  ...    0    0    1    0   \n3    0    0    0    0    0    0    0    0   33   96  ...    0    0    0    0   \n4    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n\n   778  779  780  781  782  783  \n0    0    0    0    0    0    0  \n1    0    0    0    0    0    0  \n2    0    0    0    0    0    0  \n3    0    0    0    0    0    0  \n4    0    0    0    0    0    0  \n\n[5 rows x 784 columns]\n```\n:::\n:::\n\n\n<b>There are no missing values in the data.</b>\n\n::: {#fc7545d4 .cell execution_count=5}\n``` {.python .cell-code}\nprint(np.isnan(train_images).any()) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFalse\n```\n:::\n:::\n\n\n<b>There appears to be no class imbalance</b>\n\n::: {#c6278bab .cell execution_count=6}\n``` {.python .cell-code}\nunique_labels, counts = np.unique(train_labels, return_counts=True)\n\n# Print the counts sorted by label\nfor label, count in zip(unique_labels, counts):\n    print(f\"Label {label}: {count}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLabel 0: 6000\nLabel 1: 6000\nLabel 2: 6000\nLabel 3: 6000\nLabel 4: 6000\nLabel 5: 6000\nLabel 6: 6000\nLabel 7: 6000\nLabel 8: 6000\nLabel 9: 6000\n```\n:::\n:::\n\n\n::: {#9d1a69c9 .cell execution_count=7}\n``` {.python .cell-code}\nprint(f\"X shape: {X.shape}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nX shape: (60000, 784)\n```\n:::\n:::\n\n\n<b>t-SNE Visualization</b><br> t-distributed Stochastic Neighbor\nEmbedding (t-SNE) is used here to visualize the separation between\nclasses in a high-dimensional dataset.<br> Each point represents a\nsingle fashion item (e.g., T-shirt, Trouser, etc.), and the color\ncorresponds to its true label across the 10 categories listed above.\n\n::: {#519add59 .cell execution_count=8}\n``` {.python .cell-code}\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\n# Run t-SNE to reduce dimensionality\n#embeddings = TSNE(n_jobs=2).fit_transform(X)\n\ntsne = TSNE(n_jobs=-1, random_state=42)  # Use -1 to use all available cores\nembeddings = tsne.fit_transform(X) #use scikitlearn instead\n\n\n# Create scatter plot\nfigure = plt.figure(figsize=(15,7))\nplt.scatter(embeddings[:, 0], embeddings[:, 1], c=train_labels,\n            cmap=plt.cm.get_cmap(\"jet\", 10), marker='.')\nplt.colorbar(ticks=range(10))\nplt.clim(-0.5, 9.5)\nplt.title(\"t-SNE Visualization of Fashion MNIST\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-9-output-1.png){width=1057 height=579}\n:::\n:::\n\n\n<p>(Figure 6)</p><br>\n\n<b>What the visualization shows:</b> <br> Class 1 (blue / Trousers)\nforms a clearly distinct and tightly packed cluster, indicating that the\npixel patterns for trousers are less similar to those of other classes.\nIn contrast, Classes 4 (Coat), 6 (Shirt), and 2 (Pullover) show\nsignificant overlap, suggesting that these clothing items are harder to\ndistinguish visually and may lead to more confusion during\nclassification.\n\n<!-- \n\n-   Describe your data sources and collection process.\n\n-   Present initial findings and insights through visualizations.\n\n-   Highlight unexpected patterns or anomalies.\n\n-->\n\n### Modeling and Results\n\n<!--\n-   Explain your data preprocessing and cleaning steps.\n\n-   Present your key findings in a clear and concise manner.\n\n-   Use visuals to support your claims.\n\n-   **Tell a story about what the data reveals.**\n-->\n\n<b>Our Goal</b><br> We are classifying Fashion MNIST images into one of\n10 categories. To evaluate performance, we’re comparing five different\nmodels — some trained on raw pixel values and others using features\nextracted by a Restricted Boltzmann Machine (RBM). Our objective is to\nassess whether incorporating RBM into the workflow improves\nclassification accuracy compared to using raw image data alone.<br>\n\n<b>Our Models</b><br> 1. Logistic Regression on Fashion MNIST Data<br>\n2. Feed Forward Network on Fashion MNIST Data<br> 3. Convolutional\nNeural Network on Fashion MNIST Data<br> 4. Logistic Regression on RBM\nHidden Features (of Fashion MNIST Data)<br> 5. Feed Forward Network on\nRBM Hidden Features (of Fashion MNIST Data)<br>\n\n<b>Note: Outputs (50 trials) and Code are below for each model. Both the\ncode and output can be toggled by the reader.</b><br> • The first click\nreveals a toggle labeled “Code”.<br> • Clicking “Code” will show the\noutput.<br> • Clicking again will switch from output to the actual\ncode.<br> • Clicking “Show Code and Output” again will collapse both\nviews.<br>\n\n<!-- \n<details>\n  <summary>Click to Show Code and Output</summary>\n-->\n\n<b>Import Libraries and Re-load data for first 3 models</b>\n\n::: {#b4ad34b5 .cell execution_count=9}\n``` {.python .cell-code}\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nimport numpy as np\nimport mlflow\nimport optuna\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom torch.utils.data import DataLoader\n\n# Set device\ndevice = torch.device(\"mps\")\n\n# Load Fashion-MNIST dataset again for the first 3 models\ntransform = transforms.Compose([transforms.ToTensor()])\ntrain_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\ntest_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n```\n:::\n\n\n::: {#5c651c6e .cell execution_count=10}\n``` {.python .cell-code}\n#mlflow.end_run()\n#run this in terminal when need to fully clean out expierment after you delete it in the ui\n#rm -rf mlruns/.trash/*\n```\n:::\n\n\n<!-- \n</details>\n-->\n\n<b>Model 1: Logistic Regression on Fashion MNIST Data</b><br>\n\n<details>\n\n<summary>Click to Show Code and Output</summary>\n\n::: {#32d1140f .cell execution_count=11}\n``` {.python .cell-code}\nfrom sklearn.metrics import f1_score\n\nCLASSIFIER = \"LogisticRegression\"  # Change for FNN, LogisticRegression, or CNN\n\n\n\n# Define CNN model\nclass FashionCNN(nn.Module):\n    def __init__(self, filters1, filters2, kernel1, kernel2):\n        super(FashionCNN, self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=filters1, kernel_size=kernel1, padding=1),\n            nn.BatchNorm2d(filters1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(in_channels=filters1, out_channels=filters2, kernel_size=kernel2),\n            nn.BatchNorm2d(filters2),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        self.fc1 = None #initialize first fully connected layer as none, defined later in fwd\n        self.drop = nn.Dropout2d(0.25)\n        self.fc2 = nn.Linear(in_features=600, out_features=120)\n        self.fc3 = nn.Linear(in_features=120, out_features=10)\n        \n\n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        #Flatten tensor dynamically, preserve batch size\n        out = out.view(out.size(0), -1) \n        if self.fc1 is None:\n            self.fc1 = nn.Linear(out.shape[1], 600).to(x.device)\n        out = self.fc1(out)\n        out = self.drop(out)\n        out = self.fc2(out)\n        out = self.fc3(out)\n        return out\n\n\n# Define Optuna objective function\ndef objective(trial):\n      # Set MLflow experiment name\n    if CLASSIFIER == \"LogisticRegression\":\n        experiment = mlflow.set_experiment(\"new-pytorch-fmnist-lr-noRBM\")\n    elif CLASSIFIER == \"FNN\":\n        experiment = mlflow.set_experiment(\"new-pytorch-fmnist-fnn-noRBM\")\n    elif CLASSIFIER == \"CNN\":\n        experiment = mlflow.set_experiment(\"new-pytorch-fmnist-cnn-noRBM\")\n    batch_size = trial.suggest_int(\"batch_size\", 64, 256, step=32)\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n    mlflow.start_run(experiment_id=experiment.experiment_id)\n    num_classifier_epochs = trial.suggest_int(\"num_classifier_epochs\", 5, 5) \n    mlflow.log_param(\"num_classifier_epochs\", num_classifier_epochs)\n\n    if CLASSIFIER == \"FNN\":\n        hidden_size = trial.suggest_int(\"fnn_hidden\", 192, 384)\n        learning_rate = trial.suggest_float(\"learning_rate\", 0.0001, 0.0025)\n\n        mlflow.log_param(\"classifier\", \"FNN\")\n        mlflow.log_param(\"fnn_hidden\", hidden_size)\n        mlflow.log_param(\"learning_rate\", learning_rate)\n\n        model = nn.Sequential(\n            nn.Linear(784, hidden_size), \n            nn.ReLU(),\n            nn.Linear(hidden_size, 10)\n        ).to(device)\n\n\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n    elif CLASSIFIER == \"CNN\":\n        filters1 = trial.suggest_int(\"filters1\", 16, 64, step=16)\n        filters2 = trial.suggest_int(\"filters2\", 32, 128, step=32)\n        kernel1 = trial.suggest_int(\"kernel1\", 3, 5)\n        kernel2 = trial.suggest_int(\"kernel2\", 3, 5)\n        learning_rate = trial.suggest_float(\"learning_rate\", 0.0001, 0.0025)\n\n        mlflow.log_param(\"classifier\", \"CNN\")\n        mlflow.log_param(\"filters1\", filters1)\n        mlflow.log_param(\"filters2\", filters2)\n        mlflow.log_param(\"kernel1\", kernel1)\n        mlflow.log_param(\"kernel2\", kernel2)\n        mlflow.log_param(\"learning_rate\", learning_rate)\n\n        model = FashionCNN(filters1, filters2, kernel1, kernel2).to(device)\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n      \n    elif CLASSIFIER == \"LogisticRegression\":\n        mlflow.log_param(\"classifier\", \"LogisticRegression\")\n    \n        # Prepare data for Logistic Regression (Flatten 28x28 images to 784 features)\n        train_features = train_dataset.data.view(-1, 784).numpy()\n        train_labels = train_dataset.targets.numpy()\n        test_features = test_dataset.data.view(-1, 784).numpy()\n        test_labels = test_dataset.targets.numpy()\n    \n        # Normalize the pixel values to [0,1] for better convergence\n        train_features = train_features / 255.0\n        test_features = test_features / 255.0\n    \n    \n        C = trial.suggest_float(\"C\", 0.01, 10.0, log=True)  \n        solver = \"saga\" \n    \n        model = LogisticRegression(C=C, max_iter=num_classifier_epochs, solver=solver)\n        model.fit(train_features, train_labels)\n    \n    \n        predictions = model.predict(test_features)\n        accuracy = accuracy_score(test_labels, predictions) * 100\n        \n        macro_f1 = f1_score(test_labels, predictions, average=\"macro\") #for f1\n        print(f\"Logistic Regression Test Accuracy: {accuracy:.2f}%\")\n        print(f\"Macro F1 Score: {macro_f1:.4f}\") #for f1\n    \n        mlflow.log_param(\"C\", C)\n        mlflow.log_metric(\"test_accuracy\", accuracy)\n        mlflow.log_metric(\"macro_f1\", macro_f1) #for f1\n        mlflow.end_run()\n        return accuracy\n\n    # Training Loop for FNN and CNN\n    criterion = nn.CrossEntropyLoss()\n\n\n    model.train()\n    for epoch in range(num_classifier_epochs):\n        running_loss = 0.0\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images) if CLASSIFIER == \"CNN\" else model(images.view(images.size(0), -1))\n\n            optimizer.zero_grad()\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n\n        print(f\"{CLASSIFIER} Epoch {epoch+1}: loss = {running_loss / len(train_loader):.4f}\")\n\n    # Model Evaluation\n    model.eval()\n    correct, total = 0, 0\n    all_preds = []   # for f1\n    all_labels = [] \n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images) if CLASSIFIER == \"CNN\" else model(images.view(images.size(0), -1))\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            all_preds.extend(predicted.cpu().numpy())   #for f1\n            all_labels.extend(labels.cpu().numpy()) #for f1\n\n    accuracy = 100 * correct / total\n    macro_f1 = f1_score(all_labels, all_preds, average=\"macro\") #for f1\n    print(f\"Test Accuracy: {accuracy:.2f}%\")\n    print(f\"Macro F1 Score: {macro_f1:.4f}\") #for f1\n\n    mlflow.log_metric(\"test_accuracy\", accuracy)\n    mlflow.log_metric(\"macro_f1\", macro_f1) #for f1\n    mlflow.end_run()\n    return accuracy\n\nif __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=1) # n_trials set to 1 for quick rendering\n    print(f\"Best Parameters for {CLASSIFIER}:\", study.best_params)\n    print(\"Best Accuracy:\", study.best_value)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Test Accuracy: 84.50%\nMacro F1 Score: 0.8439\nBest Parameters for LogisticRegression: {'batch_size': 160, 'num_classifier_epochs': 5, 'C': 0.7449879259925942}\nBest Accuracy: 84.5\n```\n:::\n:::\n\n\n</details>\n\nTest Accuracy of Logistic Regression by C (inverse regularization\nstrength) ![](logreg_norbm.png){width=\"60%\"}<br>\n<p>(Figure 7)</p><br>\n\n$$\nC = \\frac{1}{\\lambda} \\quad \\text{(inverse regularization strength)}\n$$\n\nLower values of C mean more regularization (higher penalties for larger\nweight coefficients)\n\nWhat the plot shows:<br> Most optuna trials were lower values of C, so\noptimization favors stronger regularization. This is further evidenced\nby the clustering of higher accuracies for lower values of C. A possibly\nanomaly is seen at C=10 with fairly high accuracy; however, it's still\nnot higher than lower values of C. <br>\n\n<b>Model 2: Feed Forward Network on Fashion MNIST Data</b>\n\n<details>\n\n<summary>Click to Show Code and Output</summary>\n\n::: {#439f6880 .cell execution_count=12}\n``` {.python .cell-code}\nfrom sklearn.metrics import f1_score\n\nCLASSIFIER = \"FNN\"  # Change for FNN, LogisticRegression, or CNN\n\n# Define CNN model\nclass FashionCNN(nn.Module):\n    def __init__(self, filters1, filters2, kernel1, kernel2):\n        super(FashionCNN, self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=filters1, kernel_size=kernel1, padding=1),\n            nn.BatchNorm2d(filters1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(in_channels=filters1, out_channels=filters2, kernel_size=kernel2),\n            nn.BatchNorm2d(filters2),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        self.fc1 = None #initialize first fully connected layer as none, defined later in fwd\n        self.drop = nn.Dropout2d(0.25)\n        self.fc2 = nn.Linear(in_features=600, out_features=120)\n        self.fc3 = nn.Linear(in_features=120, out_features=10)\n        \n\n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        #Flatten tensor dynamically\n        out = out.view(out.size(0), -1)\n        if self.fc1 is None:\n            self.fc1 = nn.Linear(out.shape[1], 600).to(x.device)\n        out = self.fc1(out)\n        out = self.drop(out)\n        out = self.fc2(out)\n        out = self.fc3(out)\n        return out\n\n\n\n# Define Optuna objective function\ndef objective(trial):\n      # Set MLflow experiment name\n    if CLASSIFIER == \"LogisticRegression\":\n        experiment = mlflow.set_experiment(\"new-pytorch-fmnist-lr-noRBM\")\n    elif CLASSIFIER == \"FNN\":\n        experiment = mlflow.set_experiment(\"new-pytorch-fmnist-fnn-noRBM\")\n    elif CLASSIFIER == \"CNN\":\n        experiment = mlflow.set_experiment(\"new-pytorch-fmnist-cnn-noRBM\")\n    batch_size = trial.suggest_int(\"batch_size\", 64, 256, step=32)\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n    mlflow.start_run(experiment_id=experiment.experiment_id)\n    num_classifier_epochs = trial.suggest_int(\"num_classifier_epochs\", 5, 5) \n    mlflow.log_param(\"num_classifier_epochs\", num_classifier_epochs)\n\n    if CLASSIFIER == \"FNN\":\n        hidden_size = trial.suggest_int(\"fnn_hidden\", 192, 384)\n        learning_rate = trial.suggest_float(\"learning_rate\", 0.0001, 0.0025)\n\n        mlflow.log_param(\"classifier\", \"FNN\")\n        mlflow.log_param(\"fnn_hidden\", hidden_size)\n        mlflow.log_param(\"learning_rate\", learning_rate)\n\n        model = nn.Sequential(\n            nn.Linear(784, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, 10)\n        ).to(device)\n\n\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n    elif CLASSIFIER == \"CNN\":\n        filters1 = trial.suggest_int(\"filters1\", 16, 64, step=16)\n        filters2 = trial.suggest_int(\"filters2\", 32, 128, step=32)\n        kernel1 = trial.suggest_int(\"kernel1\", 3, 5)\n        kernel2 = trial.suggest_int(\"kernel2\", 3, 5)\n        learning_rate = trial.suggest_float(\"learning_rate\", 0.0001, 0.0025)\n\n        mlflow.log_param(\"classifier\", \"CNN\")\n        mlflow.log_param(\"filters1\", filters1)\n        mlflow.log_param(\"filters2\", filters2)\n        mlflow.log_param(\"kernel1\", kernel1)\n        mlflow.log_param(\"kernel2\", kernel2)\n        mlflow.log_param(\"learning_rate\", learning_rate)\n\n        model = FashionCNN(filters1, filters2, kernel1, kernel2).to(device)\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n      \n    elif CLASSIFIER == \"LogisticRegression\":\n        mlflow.log_param(\"classifier\", \"LogisticRegression\")\n    \n        # Prepare data for Logistic Regression (Flatten 28x28 images to 784 features)\n        train_features = train_dataset.data.view(-1, 784).numpy()\n        train_labels = train_dataset.targets.numpy()\n        test_features = test_dataset.data.view(-1, 784).numpy()\n        test_labels = test_dataset.targets.numpy()\n    \n        # Normalize the pixel values to [0,1] for better convergence\n        train_features = train_features / 255.0\n        test_features = test_features / 255.0\n    \n    \n        C = trial.suggest_float(\"C\", 0.01, 10.0, log=True)  \n        solver = \"saga\" \n    \n        model = LogisticRegression(C=C, max_iter=num_classifier_epochs, solver=solver)\n        model.fit(train_features, train_labels)\n    \n    \n        predictions = model.predict(test_features)\n        accuracy = accuracy_score(test_labels, predictions) * 100\n        \n        macro_f1 = f1_score(test_labels, predictions, average=\"macro\") #for f1\n        print(f\"Logistic Regression Test Accuracy: {accuracy:.2f}%\")\n        print(f\"Macro F1 Score: {macro_f1:.4f}\") #for f1\n    \n        mlflow.log_param(\"C\", C)\n        mlflow.log_metric(\"test_accuracy\", accuracy)\n        mlflow.log_metric(\"macro_f1\", macro_f1) #for f1\n        mlflow.end_run()\n        return accuracy\n\n    # Training Loop for FNN and CNN\n    criterion = nn.CrossEntropyLoss()\n\n\n    model.train()\n    for epoch in range(num_classifier_epochs):\n        running_loss = 0.0\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images) if CLASSIFIER == \"CNN\" else model(images.view(images.size(0), -1))\n\n            optimizer.zero_grad()\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n\n        print(f\"{CLASSIFIER} Epoch {epoch+1}: loss = {running_loss / len(train_loader):.4f}\")\n\n    # Model Evaluation\n    model.eval()\n    correct, total = 0, 0\n    all_preds = []   # for f1\n    all_labels = [] \n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images) if CLASSIFIER == \"CNN\" else model(images.view(images.size(0), -1))\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            all_preds.extend(predicted.cpu().numpy())   #for f1\n            all_labels.extend(labels.cpu().numpy()) #for f1\n\n    accuracy = 100 * correct / total\n    macro_f1 = f1_score(all_labels, all_preds, average=\"macro\") #for f1\n    print(f\"Test Accuracy: {accuracy:.2f}%\")\n    print(f\"Macro F1 Score: {macro_f1:.4f}\") #for f1\n\n    mlflow.log_metric(\"test_accuracy\", accuracy)\n    mlflow.log_metric(\"macro_f1\", macro_f1) #for f1\n    mlflow.end_run()\n    return accuracy\n\nif __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=1) # n_trials set to 1 for quick rendering\n    print(f\"Best Parameters for {CLASSIFIER}:\", study.best_params)\n    print(\"Best Accuracy:\", study.best_value)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFNN Epoch 1: loss = 0.4851\nFNN Epoch 2: loss = 0.3642\nFNN Epoch 3: loss = 0.3291\nFNN Epoch 4: loss = 0.3080\nFNN Epoch 5: loss = 0.2920\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nTest Accuracy: 87.51%\nMacro F1 Score: 0.8759\nBest Parameters for FNN: {'batch_size': 64, 'num_classifier_epochs': 5, 'fnn_hidden': 367, 'learning_rate': 0.0023647855848675362}\nBest Accuracy: 87.51\n```\n:::\n:::\n\n\n</details>\n\nTest Accuracy by FNN Hidden Units<br>\n![](fnn_norbm.png){width=\"60%\"}<br>\n<p>(Figure 8)</p><br>\n\nWhat the plot shows:<br> Higher values of hidden units in the\nfeedforward network were sampled more frequently by Optuna, suggesting a\npreference for more complex models. However, test accuracy appears to\nlevel off between 300 and 375 hidden units, suggesting complexity\nreached its optimal range. Further increases in hidden units would\nlikely not yield higher accuracy.<br>\n\n<b>Model 3: Convolutional Neural Network on Fashion MNIST Data</b><br>\nBase code for CNN structure borrowed from\n[Kaggle](https://www.kaggle.com/code/pankajj/fashion-mnist-with-pytorch-93-accuracy)<br>\n\n<details>\n\n<summary>Click to Show Code and Output</summary>\n\n::: {#4ff1f9cf .cell execution_count=13}\n``` {.python .cell-code}\nfrom sklearn.metrics import f1_score\n\nCLASSIFIER = \"CNN\"  # Change for FNN, LogisticRegression, or CNN\n\n# Define CNN model\nclass FashionCNN(nn.Module):\n    def __init__(self, filters1, filters2, kernel1, kernel2):\n        super(FashionCNN, self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=filters1, kernel_size=kernel1, padding=1),\n            nn.BatchNorm2d(filters1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(in_channels=filters1, out_channels=filters2, kernel_size=kernel2),\n            nn.BatchNorm2d(filters2),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        self.fc1 = None #initialize first fully connected layer as none, defined later in fwd\n        self.drop = nn.Dropout2d(0.25)\n        self.fc2 = nn.Linear(in_features=600, out_features=120)\n        self.fc3 = nn.Linear(in_features=120, out_features=10)\n        \n\n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        #Flatten tensor dynamically\n        out = out.view(out.size(0), -1)\n        if self.fc1 is None:\n            self.fc1 = nn.Linear(out.shape[1], 600).to(x.device)\n        out = self.fc1(out)\n        out = self.drop(out)\n        out = self.fc2(out)\n        out = self.fc3(out)\n        return out\n\n\n\n# Define Optuna objective function\ndef objective(trial):\n        # Set MLflow experiment name\n    if CLASSIFIER == \"LogisticRegression\":\n        experiment = mlflow.set_experiment(\"new-pytorch-fmnist-lr-noRBM\")\n    elif CLASSIFIER == \"FNN\":\n        experiment = mlflow.set_experiment(\"new-pytorch-fmnist-fnn-noRBM\")\n    elif CLASSIFIER == \"CNN\":\n        experiment = mlflow.set_experiment(\"new-pytorch-fmnist-cnn-noRBM\")\n    batch_size = trial.suggest_int(\"batch_size\", 64, 256, step=32)\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n    mlflow.start_run(experiment_id=experiment.experiment_id)\n    num_classifier_epochs = trial.suggest_int(\"num_classifier_epochs\", 5, 5) \n    mlflow.log_param(\"num_classifier_epochs\", num_classifier_epochs)\n\n    if CLASSIFIER == \"FNN\":\n        hidden_size = trial.suggest_int(\"fnn_hidden\", 192, 384)\n        learning_rate = trial.suggest_float(\"learning_rate\", 0.0001, 0.0025)\n\n        mlflow.log_param(\"classifier\", \"FNN\")\n        mlflow.log_param(\"fnn_hidden\", hidden_size)\n        mlflow.log_param(\"learning_rate\", learning_rate)\n\n        model = nn.Sequential(\n            nn.Linear(784, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, 10)\n        ).to(device)\n\n\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n    elif CLASSIFIER == \"CNN\":\n        filters1 = trial.suggest_int(\"filters1\", 16, 64, step=16)\n        filters2 = trial.suggest_int(\"filters2\", 32, 128, step=32)\n        kernel1 = trial.suggest_int(\"kernel1\", 3, 5)\n        kernel2 = trial.suggest_int(\"kernel2\", 3, 5)\n        learning_rate = trial.suggest_float(\"learning_rate\", 0.0001, 0.0025)\n\n        mlflow.log_param(\"classifier\", \"CNN\")\n        mlflow.log_param(\"filters1\", filters1)\n        mlflow.log_param(\"filters2\", filters2)\n        mlflow.log_param(\"kernel1\", kernel1)\n        mlflow.log_param(\"kernel2\", kernel2)\n        mlflow.log_param(\"learning_rate\", learning_rate)\n\n        model = FashionCNN(filters1, filters2, kernel1, kernel2).to(device)\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n      \n    elif CLASSIFIER == \"LogisticRegression\":\n        mlflow.log_param(\"classifier\", \"LogisticRegression\")\n    \n        # Prepare data for Logistic Regression (Flatten 28x28 images to 784 features)\n        train_features = train_dataset.data.view(-1, 784).numpy()\n        train_labels = train_dataset.targets.numpy()\n        test_features = test_dataset.data.view(-1, 784).numpy()\n        test_labels = test_dataset.targets.numpy()\n    \n        # Normalize the pixel values to [0,1] for better convergence\n        train_features = train_features / 255.0\n        test_features = test_features / 255.0\n    \n    \n        C = trial.suggest_float(\"C\", 0.01, 10.0, log=True)  \n        solver = \"saga\" \n    \n        model = LogisticRegression(C=C, max_iter=num_classifier_epochs, solver=solver)\n        model.fit(train_features, train_labels)\n    \n    \n        predictions = model.predict(test_features)\n        accuracy = accuracy_score(test_labels, predictions) * 100\n        \n        macro_f1 = f1_score(test_labels, predictions, average=\"macro\") #for f1\n        print(f\"Logistic Regression Test Accuracy: {accuracy:.2f}%\")\n        print(f\"Macro F1 Score: {macro_f1:.4f}\") #for f1\n    \n        mlflow.log_param(\"C\", C)\n        mlflow.log_metric(\"test_accuracy\", accuracy)\n        mlflow.log_metric(\"macro_f1\", macro_f1) #for f1\n        mlflow.end_run()\n        return accuracy\n\n    # Training Loop for FNN and CNN\n    criterion = nn.CrossEntropyLoss()\n\n\n    model.train()\n    for epoch in range(num_classifier_epochs):\n        running_loss = 0.0\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images) if CLASSIFIER == \"CNN\" else model(images.view(images.size(0), -1))\n\n            optimizer.zero_grad()\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n\n        print(f\"{CLASSIFIER} Epoch {epoch+1}: loss = {running_loss / len(train_loader):.4f}\")\n\n    # Model Evaluation\n    model.eval()\n    correct, total = 0, 0\n    all_preds = []   # for f1\n    all_labels = [] \n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images) if CLASSIFIER == \"CNN\" else model(images.view(images.size(0), -1))\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            all_preds.extend(predicted.cpu().numpy())   #for f1\n            all_labels.extend(labels.cpu().numpy()) #for f1\n\n    accuracy = 100 * correct / total\n    macro_f1 = f1_score(all_labels, all_preds, average=\"macro\") #for f1\n    print(f\"Test Accuracy: {accuracy:.2f}%\")\n    print(f\"Macro F1 Score: {macro_f1:.4f}\") #for f1\n\n    mlflow.log_metric(\"test_accuracy\", accuracy)\n    mlflow.log_metric(\"macro_f1\", macro_f1) #for f1\n    mlflow.end_run()\n    return accuracy\n\nif __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=1) # n_trials set to 1 for quick rendering\n    print(f\"Best Parameters for {CLASSIFIER}:\", study.best_params)\n    print(\"Best Accuracy:\", study.best_value)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCNN Epoch 1: loss = 0.5400\nCNN Epoch 2: loss = 0.3260\nCNN Epoch 3: loss = 0.2866\nCNN Epoch 4: loss = 0.2623\nCNN Epoch 5: loss = 0.2423\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nTest Accuracy: 89.46%\nMacro F1 Score: 0.8916\nBest Parameters for CNN: {'batch_size': 256, 'num_classifier_epochs': 5, 'filters1': 64, 'filters2': 96, 'kernel1': 4, 'kernel2': 5, 'learning_rate': 0.0004118428546844502}\nBest Accuracy: 89.46\n```\n:::\n:::\n\n\n</details>\n\nTest Accuracy Based on the Number of Filters in the First Conv2D\nLayer<br> ![](filters1.png){width=\"60%\"}<br>\n<p>(Figure 9)</p><br>\n\nWhat the plot shows:<br> Although the highest test accuracy was achieved\nwith 64 filters in the first convolutional 2D layer, the number of\nfilters alone isn't a strong predictor of model performance. Each filter\nsize shows high variance (accuracies are spread out for each value\nvertically). This, combined with the fact that accuracies are well\ndistributed across the different filter counts, suggests other factors\nor hyperparameters may play a bigger role in predicting accuracy. <br>\n\nTest Accuracy Based on the Number of Filters in the Second Conv2D\nLayer<br> ![](filters2.png){width=\"60%\"}<br>\n<p>(Figure 10)</p><br>\n\nWhat the plot shows:<br> Like the first Conv2D layer, the number of\nfilters doesn't seem to be a extremely strong predictor in accuracy.\nHowever, Optuna has sampled more frequently from higher number of\nfilters, even 128 for this second layer, suggesting higher filters\nperformed better. However, like before, there is still high variance in\naccuracy for each number of filters. <br>\n\nTest Accuracy Based on Kernel Size in the First Conv2D Layer<br>\n![](kernelsize1.png){width=\"60%\"}<br>\n<p>(Figure 11)</p><br>\n\nWhat the plot shows:<br> Kernel size of 3 was sampled more frequently by\nOptuna and yielded higher accuracies than kernel sizes of 4 or 5.<br>\n\nTest Accuracy Based on Kernel Size in the Second Conv2D Layer<br>\n![](kernelsize2.png){width=\"60%\"}<br>\n<p>(Figure 12)</p><br>\n\nWhat the plot shows:<br> Like with the first convolutional 2D layer,\nkernel size of 3 is highly favored by Optuna and consistently led to\nhigher test accuracies.<br>\n\n<b>Model 4: Logistic Regression on RBM Hidden Features (of Fashion MNIST\nData)</b><br>\n\n<details>\n\n<summary>Click to Show Code and Output</summary>\n\n::: {#856838f7 .cell execution_count=14}\n``` {.python .cell-code}\nfrom sklearn.metrics import accuracy_score, f1_score\nCLASSIFIER = 'LogisticRegression'\n\nif CLASSIFIER == 'LogisticRegression':\n    experiment = mlflow.set_experiment(\"new-pytorch-fmnist-lr-withrbm\")\nelse:\n    experiment = mlflow.set_experiment(\"new-pytorch-fmnist-fnn-withrbm\")\n\n\nclass RBM(nn.Module):\n    def __init__(self, n_visible=784, n_hidden=256, k=1):\n        super(RBM, self).__init__()\n        self.n_visible = n_visible\n        self.n_hidden = n_hidden\n        # Initialize weights and biases\n        self.W = nn.Parameter(torch.randn(n_hidden, n_visible) * 0.1)\n        self.v_bias = nn.Parameter(torch.zeros(n_visible))\n        self.h_bias = nn.Parameter(torch.zeros(n_hidden))\n        self.k = k  # CD-k steps\n\n    def sample_h(self, v):\n        # Given visible v, sample hidden h\n        p_h = torch.sigmoid(F.linear(v, self.W, self.h_bias))  # p(h=1|v)\n        h_sample = torch.bernoulli(p_h)                        # sample Bernoulli\n        return p_h, h_sample\n\n    def sample_v(self, h):\n        # Given hidden h, sample visible v\n        p_v = torch.sigmoid(F.linear(h, self.W.t(), self.v_bias))  # p(v=1|h)\n        v_sample = torch.bernoulli(p_v)\n        return p_v, v_sample\n\n    def forward(self, v):\n        # Perform k steps of contrastive divergence starting from v\n        v_k = v.clone()\n        for _ in range(self.k):\n            _, h_k = self.sample_h(v_k)    # sample hidden from current visible\n            _, v_k = self.sample_v(h_k)    # sample visible from hidden\n        return v_k  # k-step reconstructed visible\n\n    def free_energy(self, v):\n        # Compute the visible bias term for each sample in the batch\n        vbias_term = (v * self.v_bias).sum(dim=1)  # shape: [batch_size]\n        # Compute the activation of the hidden units\n        wx_b = F.linear(v, self.W, self.h_bias)     # shape: [batch_size, n_hidden]\n        # Compute the hidden term\n        hidden_term = torch.sum(torch.log1p(torch.exp(wx_b)), dim=1)  # shape: [batch_size]\n        # Return the mean free energy over the batch\n        return - (vbias_term + hidden_term).mean()\n    \ntransform = transforms.Compose([transforms.ToTensor()])\ntrain_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\ntest_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n\ndef objective(trial):\n    num_rbm_epochs = trial.suggest_int(\"num_rbm_epochs\", 5, 5)# 24, 33)\n    batch_size = trial.suggest_int(\"batch_size\", 192, 1024)\n    rbm_lr = trial.suggest_float(\"rbm_lr\", 0.05, 0.1)\n    rbm_hidden = trial.suggest_int(\"rbm_hidden\", 384, 8192)\n\n    mlflow.start_run(experiment_id=experiment.experiment_id)\n    if CLASSIFIER != 'LogisticRegression':\n        fnn_hidden = trial.suggest_int(\"fnn_hidden\", 192, 384)\n        fnn_lr = trial.suggest_float(\"fnn_lr\", 0.0001, 0.0025)\n        mlflow.log_param(\"fnn_hidden\", fnn_hidden)\n        mlflow.log_param(\"fnn_lr\", fnn_lr)\n\n    num_classifier_epochs = trial.suggest_int(\"num_classifier_epochs\", 5, 5)# 40, 60)\n\n    mlflow.log_param(\"num_rbm_epochs\", num_rbm_epochs)\n    mlflow.log_param(\"batch_size\", batch_size)\n    mlflow.log_param(\"rbm_lr\", rbm_lr)\n    mlflow.log_param(\"rbm_hidden\", rbm_hidden)\n    mlflow.log_param(\"num_classifier_epochs\", num_classifier_epochs)\n\n    # Instantiate RBM and optimizer\n    device = torch.device(\"mps\")\n    rbm = RBM(n_visible=784, n_hidden=rbm_hidden, k=1).to(device)\n    optimizer = torch.optim.SGD(rbm.parameters(), lr=rbm_lr)\n\n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n    rbm_training_failed = False\n    # Training loop (assuming train_loader yields batches of images and labels)\n    for epoch in range(num_rbm_epochs):\n        total_loss = 0.0\n        for images, _ in train_loader:\n            # Flatten images and binarize\n            v0 = images.view(-1, 784).to(rbm.W.device)      # shape [batch_size, 784]\n            v0 = torch.bernoulli(v0)                        # sample binary input\n            vk = rbm(v0)                                    # k-step CD reconstruction\n            # Compute contrastive divergence loss (free energy difference)\n            loss = rbm.free_energy(v0) - rbm.free_energy(vk)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n        print(f\"Epoch {epoch+1}: avg free-energy loss = {total_loss/len(train_loader):.4f}\")\n        if np.isnan(total_loss):\n            rbm_training_failed = True\n            break\n\n        if rbm_training_failed:\n            accuracy = 0.0\n            macro_f1 = 0.0 \n            print(\"RBM training failed — returning 0.0 for accuracy and macro F1\")  \n            mlflow.log_metric(\"test_accuracy\", accuracy)\n            mlflow.log_metric(\"macro_f1\", macro_f1)\n            mlflow.set_tag(\"status\", \"rbm_failed\")  # Optional tag\n            mlflow.end_run()\n            return float(accuracy)\n    else:\n        rbm.eval()  # set in evaluation mode if using any layers that behave differently in training\n        features_list = []\n        labels_list = []\n        for images, labels in train_loader:\n            v = images.view(-1, 784).to(rbm.W.device)\n            v = v  # (optionally binarize or use raw normalized pixels)\n            h_prob, h_sample = rbm.sample_h(v)  # get hidden activations\n            features_list.append(h_prob.cpu().detach().numpy())\n            labels_list.append(labels.numpy())\n        train_features = np.concatenate(features_list)  # shape: [N_train, n_hidden]\n        train_labels = np.concatenate(labels_list)\n\n        # Convert pre-extracted training features and labels to tensors and create a DataLoader\n        train_features_tensor = torch.tensor(train_features, dtype=torch.float32)\n        train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n        train_feature_dataset = torch.utils.data.TensorDataset(train_features_tensor, train_labels_tensor)\n        train_feature_loader = torch.utils.data.DataLoader(train_feature_dataset, batch_size=batch_size, shuffle=True)\n\n            \n        if CLASSIFIER == 'LogisticRegression':\n            # add optuna tuning same as log reg without RBM features...\n            lr_C = trial.suggest_float(\"lr_C\", 0.01, 10.0, log=True)  \n            mlflow.log_param(\"lr_C\", lr_C)  # Log the chosen C value\n\n\n            classifier = LogisticRegression(max_iter=num_classifier_epochs, C=lr_C, solver=\"saga\") \n            classifier.fit(train_features, train_labels)            \n            \n        else:\n            classifier = nn.Sequential(\n                nn.Linear(rbm.n_hidden, fnn_hidden),\n                nn.ReLU(),\n                nn.Linear(fnn_hidden, 10)\n            )\n\n            # Move classifier to the same device as the RBM\n            classifier = classifier.to(device)\n            criterion = nn.CrossEntropyLoss()\n            classifier_optimizer = torch.optim.Adam(classifier.parameters(), lr=fnn_lr)\n\n            classifier.train()\n            for epoch in range(num_classifier_epochs):\n                running_loss = 0.0\n                for features, labels in train_feature_loader:\n                    features = features.to(device)\n                    labels = labels.to(device)\n                    \n                    # Forward pass through classifier\n                    outputs = classifier(features)\n                    loss = criterion(outputs, labels)\n                    \n                    # Backpropagation and optimization\n                    classifier_optimizer.zero_grad()\n                    loss.backward()\n                    classifier_optimizer.step()\n                    \n                    running_loss += loss.item()\n                avg_loss = running_loss / len(train_feature_loader)\n                print(f\"Classifier Epoch {epoch+1}: loss = {avg_loss:.4f}\")\n\n        # Evaluate the classifier on test data.\n        # Here we extract features from the RBM for each test image.\n        if CLASSIFIER != 'LogisticRegression':\n            classifier.eval()\n            correct = 0\n            total = 0\n        features_list = []\n        labels_list = []\n        with torch.no_grad():\n            for images, labels in test_loader:\n                v = images.view(-1, 784).to(device)\n                # Extract hidden activations; you can use either h_prob or h_sample.\n                h_prob, _ = rbm.sample_h(v)\n                if CLASSIFIER == 'LogisticRegression':\n                    features_list.append(h_prob.cpu().detach().numpy())\n                    labels_list.append(labels.numpy())\n                else:\n                    outputs = classifier(h_prob)\n                    _, predicted = torch.max(outputs.data, 1)\n                    total += labels.size(0)\n                    correct += (predicted.cpu() == labels).sum().item()\n\n        if CLASSIFIER == 'LogisticRegression':\n            test_features = np.concatenate(features_list)\n            test_labels = np.concatenate(labels_list)\n            predictions = classifier.predict(test_features)\n            accuracy = accuracy_score(test_labels, predictions) * 100\n        \n            macro_f1 = f1_score(test_labels, predictions, average=\"macro\") \n        \n        else:\n            accuracy = 100 * correct / total\n        \n            all_preds = [] \n            all_labels = [] \n            classifier.eval()\n            with torch.no_grad():\n                for images, labels in test_loader:\n                    v = images.view(-1, 784).to(device)\n                    h_prob, _ = rbm.sample_h(v)\n                    outputs = classifier(h_prob)\n                    _, predicted = torch.max(outputs.data, 1)\n                    all_preds.extend(predicted.cpu().numpy()) \n                    all_labels.extend(labels.numpy()) \n        \n            macro_f1 = f1_score(all_labels, all_preds, average=\"macro\") \n        \n        print(f\"Test Accuracy: {accuracy:.2f}%\")\n        print(f\"Macro F1 Score: {macro_f1:.4f}\") \n        \n        mlflow.log_metric(\"test_accuracy\", accuracy)\n        mlflow.log_metric(\"macro_f1\", macro_f1) \n        mlflow.end_run()\n        return float(accuracy if accuracy is not None else 0.0)\n\nif __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=1) # n_trials set to 1 for quick rendering\n    print(study.best_params)\n    print(study.best_value)\n    print(study.best_trial)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 1: avg free-energy loss = 180.1158\nEpoch 2: avg free-energy loss = 51.5988\nEpoch 3: avg free-energy loss = 34.4182\nEpoch 4: avg free-energy loss = 28.2203\nEpoch 5: avg free-energy loss = 24.0091\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nTest Accuracy: 86.29%\nMacro F1 Score: 0.8621\n{'num_rbm_epochs': 5, 'batch_size': 309, 'rbm_lr': 0.07287144443964044, 'rbm_hidden': 6931, 'num_classifier_epochs': 5, 'lr_C': 0.5601072220671769}\n86.29\nFrozenTrial(number=0, state=1, values=[86.29], datetime_start=datetime.datetime(2025, 4, 10, 10, 2, 58, 530572), datetime_complete=datetime.datetime(2025, 4, 10, 10, 4, 20, 446138), params={'num_rbm_epochs': 5, 'batch_size': 309, 'rbm_lr': 0.07287144443964044, 'rbm_hidden': 6931, 'num_classifier_epochs': 5, 'lr_C': 0.5601072220671769}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'num_rbm_epochs': IntDistribution(high=5, log=False, low=5, step=1), 'batch_size': IntDistribution(high=1024, log=False, low=192, step=1), 'rbm_lr': FloatDistribution(high=0.1, log=False, low=0.05, step=None), 'rbm_hidden': IntDistribution(high=8192, log=False, low=384, step=1), 'num_classifier_epochs': IntDistribution(high=5, log=False, low=5, step=1), 'lr_C': FloatDistribution(high=10.0, log=True, low=0.01, step=None)}, trial_id=0, value=None)\n```\n:::\n:::\n\n\n</details>\n\nTest Accuracy of Logistic Regression on RBM Hidden Features by Inverse\nRegularization Strength<br> ![](logreg_withrbm_C.png){width=\"60%\"}<br>\n<p>(Figure 13)</p><br>\n\nWhat the plot shows:<br> When using RBM-extracted hidden features as\ninput to logistic regression, the inverse regularization strength does\nnot appear to be a strong predictor of test accuracy.<br>\n\nTest Accuracy By Number of RBM Hidden Units<br>\n![](logreg_withrbm_hiddenunits.png){width=\"60%\"}<br>\n<p>(Figure 14)</p><br>\n\nWhat the plot shows:<br> Optuna slightly favors higher number of hidden\nunits in the rbm with a peak at 5340 (and similar peaks 5358, 5341,\netc.). However, after 7000 units, accuracy appears to decline suggesting\nthe optimum number of units was reached around that 5300 mark. <br>\n\n<b>Model 5: Feed Forward Network on RBM Hidden Features (of Fashion\nMNIST Data)</b><br>\n\n<details>\n\n<summary>Click to Show Code and Output</summary>\n\n::: {#677ed7d6 .cell execution_count=15}\n``` {.python .cell-code}\nfrom sklearn.metrics import accuracy_score, f1_score\nCLASSIFIER = 'FNN'\n\nif CLASSIFIER == 'LogisticRegression':\n    experiment = mlflow.set_experiment(\"new-pytorch-fmnist-lr-withrbm\")\nelse:\n    experiment = mlflow.set_experiment(\"new-pytorch-fmnist-fnn-withrbm\")\n\n\nclass RBM(nn.Module):\n    def __init__(self, n_visible=784, n_hidden=256, k=1):\n        super(RBM, self).__init__()\n        self.n_visible = n_visible\n        self.n_hidden = n_hidden\n        # Initialize weights and biases\n        self.W = nn.Parameter(torch.randn(n_hidden, n_visible) * 0.1)\n        self.v_bias = nn.Parameter(torch.zeros(n_visible))\n        self.h_bias = nn.Parameter(torch.zeros(n_hidden))\n        self.k = k  # CD-k steps\n\n    def sample_h(self, v):\n        # Given visible v, sample hidden h\n        p_h = torch.sigmoid(F.linear(v, self.W, self.h_bias))  # p(h=1|v)\n        h_sample = torch.bernoulli(p_h)                        # sample Bernoulli\n        return p_h, h_sample\n\n    def sample_v(self, h):\n        # Given hidden h, sample visible v\n        p_v = torch.sigmoid(F.linear(h, self.W.t(), self.v_bias))  # p(v=1|h)\n        v_sample = torch.bernoulli(p_v)\n        return p_v, v_sample\n\n    def forward(self, v):\n        # Perform k steps of contrastive divergence starting from v\n        v_k = v.clone()\n        for _ in range(self.k):\n            _, h_k = self.sample_h(v_k)    # sample hidden from current visible\n            _, v_k = self.sample_v(h_k)    # sample visible from hidden\n        return v_k  # k-step reconstructed visible\n\n    def free_energy(self, v):\n        # Compute the visible bias term for each sample in the batch\n        vbias_term = (v * self.v_bias).sum(dim=1)  # shape: [batch_size]\n        # Compute the activation of the hidden units\n        wx_b = F.linear(v, self.W, self.h_bias)     # shape: [batch_size, n_hidden]\n        # Compute the hidden term\n        hidden_term = torch.sum(torch.log1p(torch.exp(wx_b)), dim=1)  # shape: [batch_size]\n        # Return the mean free energy over the batch\n        return - (vbias_term + hidden_term).mean()\n    \ntransform = transforms.Compose([transforms.ToTensor()])\ntrain_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\ntest_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n\ndef objective(trial):\n    num_rbm_epochs = trial.suggest_int(\"num_rbm_epochs\", 5, 5)# 24, 33)\n    batch_size = trial.suggest_int(\"batch_size\", 192, 1024)\n    rbm_lr = trial.suggest_float(\"rbm_lr\", 0.05, 0.1)\n    rbm_hidden = trial.suggest_int(\"rbm_hidden\", 384, 8192)\n\n    mlflow.start_run(experiment_id=experiment.experiment_id)\n    if CLASSIFIER != 'LogisticRegression':\n        fnn_hidden = trial.suggest_int(\"fnn_hidden\", 192, 384)\n        fnn_lr = trial.suggest_float(\"fnn_lr\", 0.0001, 0.0025)\n        mlflow.log_param(\"fnn_hidden\", fnn_hidden)\n        mlflow.log_param(\"fnn_lr\", fnn_lr)\n\n    num_classifier_epochs = trial.suggest_int(\"num_classifier_epochs\", 5, 5)# 40, 60)\n\n    mlflow.log_param(\"num_rbm_epochs\", num_rbm_epochs)\n    mlflow.log_param(\"batch_size\", batch_size)\n    mlflow.log_param(\"rbm_lr\", rbm_lr)\n    mlflow.log_param(\"rbm_hidden\", rbm_hidden)\n    mlflow.log_param(\"num_classifier_epochs\", num_classifier_epochs)\n\n    # Instantiate RBM and optimizer\n    device = torch.device(\"mps\")\n    rbm = RBM(n_visible=784, n_hidden=rbm_hidden, k=1).to(device)\n    optimizer = torch.optim.SGD(rbm.parameters(), lr=rbm_lr)\n\n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n    rbm_training_failed = False\n    # Training loop (assuming train_loader yields batches of images and labels)\n    for epoch in range(num_rbm_epochs):\n        total_loss = 0.0\n        for images, _ in train_loader:\n            # Flatten images and binarize\n            v0 = images.view(-1, 784).to(rbm.W.device)      # shape [batch_size, 784]\n            v0 = torch.bernoulli(v0)                        # sample binary input\n            vk = rbm(v0)                                    # k-step CD reconstruction\n            # Compute contrastive divergence loss (free energy difference)\n            loss = rbm.free_energy(v0) - rbm.free_energy(vk)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n        print(f\"Epoch {epoch+1}: avg free-energy loss = {total_loss/len(train_loader):.4f}\")\n        if np.isnan(total_loss):\n            rbm_training_failed = True\n            break\n\n        if rbm_training_failed:\n            accuracy = 0.0\n            macro_f1 = 0.0 \n            print(\"RBM training failed — returning 0.0 for accuracy and macro F1\")  \n            mlflow.log_metric(\"test_accuracy\", accuracy)\n            mlflow.log_metric(\"macro_f1\", macro_f1)\n            mlflow.set_tag(\"status\", \"rbm_failed\")  # Optional tag\n            mlflow.end_run()\n            return float(accuracy)\n    else:\n        rbm.eval()  # set in evaluation mode if using any layers that behave differently in training\n        features_list = []\n        labels_list = []\n        for images, labels in train_loader:\n            v = images.view(-1, 784).to(rbm.W.device)\n            v = v  # (optionally binarize or use raw normalized pixels)\n            h_prob, h_sample = rbm.sample_h(v)  # get hidden activations\n            features_list.append(h_prob.cpu().detach().numpy())\n            labels_list.append(labels.numpy())\n        train_features = np.concatenate(features_list)  # shape: [N_train, n_hidden]\n        train_labels = np.concatenate(labels_list)\n\n        # Convert pre-extracted training features and labels to tensors and create a DataLoader\n        train_features_tensor = torch.tensor(train_features, dtype=torch.float32)\n        train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n        train_feature_dataset = torch.utils.data.TensorDataset(train_features_tensor, train_labels_tensor)\n        train_feature_loader = torch.utils.data.DataLoader(train_feature_dataset, batch_size=batch_size, shuffle=True)\n\n            \n        if CLASSIFIER == 'LogisticRegression':\n            # add optuna tuning same as log reg without RBM features...\n            lr_C = trial.suggest_float(\"lr_C\", 0.01, 10.0, log=True)  \n            mlflow.log_param(\"lr_C\", lr_C)  # Log the chosen C value\n\n\n            classifier = LogisticRegression(max_iter=num_classifier_epochs, C=lr_C, solver=\"saga\") \n            classifier.fit(train_features, train_labels)            \n            \n        else:\n            classifier = nn.Sequential(\n                nn.Linear(rbm.n_hidden, fnn_hidden),\n                nn.ReLU(),\n                nn.Linear(fnn_hidden, 10)\n            )\n\n            # Move classifier to the same device as the RBM\n            classifier = classifier.to(device)\n            criterion = nn.CrossEntropyLoss()\n            classifier_optimizer = torch.optim.Adam(classifier.parameters(), lr=fnn_lr)\n\n            classifier.train()\n            for epoch in range(num_classifier_epochs):\n                running_loss = 0.0\n                for features, labels in train_feature_loader:\n                    features = features.to(device)\n                    labels = labels.to(device)\n                    \n                    # Forward pass through classifier\n                    outputs = classifier(features)\n                    loss = criterion(outputs, labels)\n                    \n                    # Backpropagation and optimization\n                    classifier_optimizer.zero_grad()\n                    loss.backward()\n                    classifier_optimizer.step()\n                    \n                    running_loss += loss.item()\n                avg_loss = running_loss / len(train_feature_loader)\n                print(f\"Classifier Epoch {epoch+1}: loss = {avg_loss:.4f}\")\n\n        # Evaluate the classifier on test data.\n        # Here we extract features from the RBM for each test image.\n        if CLASSIFIER != 'LogisticRegression':\n            classifier.eval()\n            correct = 0\n            total = 0\n        features_list = []\n        labels_list = []\n        with torch.no_grad():\n            for images, labels in test_loader:\n                v = images.view(-1, 784).to(device)\n                # Extract hidden activations; you can use either h_prob or h_sample.\n                h_prob, _ = rbm.sample_h(v)\n                if CLASSIFIER == 'LogisticRegression':\n                    features_list.append(h_prob.cpu().detach().numpy())\n                    labels_list.append(labels.numpy())\n                else:\n                    outputs = classifier(h_prob)\n                    _, predicted = torch.max(outputs.data, 1)\n                    total += labels.size(0)\n                    correct += (predicted.cpu() == labels).sum().item()\n\n        if CLASSIFIER == 'LogisticRegression':\n            test_features = np.concatenate(features_list)\n            test_labels = np.concatenate(labels_list)\n            predictions = classifier.predict(test_features)\n            accuracy = accuracy_score(test_labels, predictions) * 100\n        \n            macro_f1 = f1_score(test_labels, predictions, average=\"macro\") \n        \n        else:\n            accuracy = 100 * correct / total\n        \n            all_preds = [] \n            all_labels = [] \n            classifier.eval()\n            with torch.no_grad():\n                for images, labels in test_loader:\n                    v = images.view(-1, 784).to(device)\n                    h_prob, _ = rbm.sample_h(v)\n                    outputs = classifier(h_prob)\n                    _, predicted = torch.max(outputs.data, 1)\n                    all_preds.extend(predicted.cpu().numpy()) \n                    all_labels.extend(labels.numpy()) \n        \n            macro_f1 = f1_score(all_labels, all_preds, average=\"macro\") \n        \n        print(f\"Test Accuracy: {accuracy:.2f}%\")\n        print(f\"Macro F1 Score: {macro_f1:.4f}\") \n        \n        mlflow.log_metric(\"test_accuracy\", accuracy)\n        mlflow.log_metric(\"macro_f1\", macro_f1) \n        mlflow.end_run()\n        return float(accuracy if accuracy is not None else 0.0)\n\nif __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=1) # n_trials set to 1 for quick rendering\n    print(study.best_params)\n    print(study.best_value)\n    print(study.best_trial)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 1: avg free-energy loss = 113.5944\nEpoch 2: avg free-energy loss = 26.6351\nEpoch 3: avg free-energy loss = 19.6702\nEpoch 4: avg free-energy loss = 16.2455\nEpoch 5: avg free-energy loss = 14.0989\nClassifier Epoch 1: loss = 0.6332\nClassifier Epoch 2: loss = 0.4429\nClassifier Epoch 3: loss = 0.4131\nClassifier Epoch 4: loss = 0.3857\nClassifier Epoch 5: loss = 0.3713\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nTest Accuracy: 84.79%\nMacro F1 Score: 0.8463\n{'num_rbm_epochs': 5, 'batch_size': 395, 'rbm_lr': 0.05106545919626053, 'rbm_hidden': 6122, 'fnn_hidden': 222, 'fnn_lr': 0.0017771107544991736, 'num_classifier_epochs': 5}\n84.79\nFrozenTrial(number=0, state=1, values=[84.79], datetime_start=datetime.datetime(2025, 4, 10, 10, 4, 20, 551191), datetime_complete=datetime.datetime(2025, 4, 10, 10, 4, 56, 80535), params={'num_rbm_epochs': 5, 'batch_size': 395, 'rbm_lr': 0.05106545919626053, 'rbm_hidden': 6122, 'fnn_hidden': 222, 'fnn_lr': 0.0017771107544991736, 'num_classifier_epochs': 5}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'num_rbm_epochs': IntDistribution(high=5, log=False, low=5, step=1), 'batch_size': IntDistribution(high=1024, log=False, low=192, step=1), 'rbm_lr': FloatDistribution(high=0.1, log=False, low=0.05, step=None), 'rbm_hidden': IntDistribution(high=8192, log=False, low=384, step=1), 'fnn_hidden': IntDistribution(high=384, log=False, low=192, step=1), 'fnn_lr': FloatDistribution(high=0.0025, log=False, low=0.0001, step=None), 'num_classifier_epochs': IntDistribution(high=5, log=False, low=5, step=1)}, trial_id=0, value=None)\n```\n:::\n:::\n\n\n</details>\n\nTest Accuracy by RBM Hidden Units<br>\n![](fnn_withrbm_rbmhiddenunits.png){width=\"60%\"}<br>\n<p>(Figure 15)</p><br>\n\nWhat the plot shows:<br> Highest accuracies cluster between 2000 and\n4000 hidden units in the RBM with an outlier at 3764 hidden units. This\npossibly suggests too few hidden units lacks the complexity needed to\nexplain the data; however, too many hidden units is perhaps causing some\noverfitting--resulting in poor generalization of the FNN classifier that\nreceives the RBM hidden features.<br>\n\nTest Accuracy by FNN Hidden Units<br>\n![](fnn_withrbm_fnnhiddenunits.png){width=\"60%\"}<br>\n<p>(Figure 16)</p><br>\n\nWhat the plot shows:<br> Surprisingly, the number of hidden units in the\nFNN does not show a strong correlation with test accuracy. All hidden\nunits tested seem to result in similar performance. This suggests the\nFNN is able to learn from the RBM features sufficently, and additional\nneurons do not significantly improve generalization.<br>\n\n<style type=\"text/css\">\n.tg  {border-collapse:collapse;border-spacing:0;}\n.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg .tg-9wq8{border-color:inherit;text-align:center;vertical-align:middle}\n.tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top}\n.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}\n.tg .tg-uzvj{border-color:inherit;font-weight:bold;text-align:center;vertical-align:middle}\n</style>\n\n+----------------------+----------------------+-------------------+\n| **Model**            | **Optuna Best        | **Macro F1        |\n|                      | Trial**\\             | Score**           |\n|                      | **MLflow Test        |                   |\n|                      | Accuracy(%)**        |                   |\n+======================+======================+===================+\n| Logistic Regression  | 84.71                | 0.846             |\n+----------------------+----------------------+-------------------+\n| Feed Forward Network | 88.06                | 0.879             |\n+----------------------+----------------------+-------------------+\n| Convolutional Neural | 91.29                | 0.913             |\n| Network              |                      |                   |\n+----------------------+----------------------+-------------------+\n| Logistic Regression  | 87.14                | 0.871             |\n| (on RBM Hidden       |                      |                   |\n| Features)            |                      |                   |\n+----------------------+----------------------+-------------------+\n| Feed Forward Network | 86.95                | 0.869             |\n| (on RBM Hidden       |                      |                   |\n| Features)            |                      |                   |\n+----------------------+----------------------+-------------------+\n<p>(Table 2)</p>\n### Conclusion\n\n-   Summarize your key findings.\n\nCNN clearly outperforms other models. Logistic Regression, which\ntypically performs well for binary classifications tasks, underperforms\non Fashion MNIST multiclassification task. Logistic Regression is\nimproved by using a Restricted Boltzmann Machine first to extract the\nhidden features from the input data prior to classification. Feed\nForward Network is not improved by the use of RBM. These findings\nclearly show the progress in machine and deep learning and how more\nadvanced neural networks on raw pixels can outperform models that use\nRBM hidden features.\n\n-   Discuss the implications of your results.\n\nRestricted Boltzmann Machines are no longer considered state-of-the-art\nfor machine learning tasks. While contrastive divergence made training\nRBMs easier, supervised training of deep feedforward networks and\nconvolutional neural networks using backpropagation proved to be more\neffective and began to dominate the field. This was largely the result\nof overcoming challenges with exploding or vanishing gradients and the\nintroduction of techniques such as batch normalization, dropout, and\nbetter weight initialization methods.\n\nHowever, for the student of machine learning, learning RBMs is still\nvaluable for understanding the foundations of unsupervised learning and\nenergy-based models. Today, Stable Diffusion is a popular type of\ngenerative AI model with an energy-based foundation. The mechanics of\nRBM training, like Gibbs sampling, and the probabilistic nature of the\nmodel provide a demonstration of the application of probability theory\nand concepts like Markov chains and Boltzmann distributions in machine\nlearning.\n\n## References\n\n",
    "supporting": [
      "index_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}