---
title: "Restricted Boltzmann Machines"
subtitle: "Theory of RBMs and Applications"
author: "Jessica Wells and Jason Gerstenberger (Advisor: Dr. Cohen)"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: Capstone Projects in Data Science IDC 6940
university: University of West Florida, Pensacola, FL
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

Slides: [slides.html](slides.html){target="_blank"} ( Go to `slides.qmd`
to edit)

<!--  

::: callout-important
**Remember:** Your goal is to make your audience understand and care
about your findings. By crafting a compelling story, you can effectively
communicate the value of your data science project.

Carefully read this template since it has instructions and tips to
writing!
:::

-->

## Introduction
Restricted Boltzmann Machines are a type of neural network that has been around since the 1980s. They are used for dimensionality reduction/feature extraction which is great for preparing a dataset for unsupervised learning. They also have other applications which will be discussed further later. As a reminder to the reader, machine learning is generally divided into 3 categories: supervised learning (examples: classification tasks, regression), unsupervised learning (examples: clustering, dimensionality reduction, generative modeling), and reinforcement learning (examples: gaming/robotics). So, like most neural networks, RBMs can kinda dip into all 3 categories.
<!-- 
The introduction should:

-   Develop a storyline that captures attention and maintains interest.

-   Your audience is your peers

-   Clearly state the problem or question you're addressing.

-   Introduce why it is relevant needs.

-   Provide an overview of your approach.

Example of writing including citing references:

*This is an introduction to ..... regression, which is a non-parametric
estimator that estimates the conditional expectation of two variables
which is random. The goal of a kernel regression is to discover the
non-linear relationship between two random variables. To discover the
non-linear relationship, kernel estimator or kernel smoothing is the
main method to estimate the curve for non-parametric statistics. In
kernel estimator, weight function is known as kernel function
[@efr2008]. Cite this paper [@bro2014principal]. The GEE [@wang2014].
The PCA [@daffertshofer2004pca]*. Topology can be used in machine learning [@adams2021topology]

*This is my work and I want to add more work...*
-->


The following research articles will be summarized with the following formatting:

1) goal of paper 2) why important 3) how solved/methods 4) results/limitations

<h4 style="font-weight: bold;">Research Article 1:</h4> 
An Introduction to Restricted Boltzmann Machines [@fischer2012introduction]

1) Goal of Paper:
The goal of the paper was to describe the mathematics and theory behind RBMs. First, the authors go into detail about how Boltzmann Machines are undirected graphical models (Markov Random Fields--MRFs) and explain the theory behind them. The important point being that the probability distribution is a complex Gibbs distribution and sampling from it can be difficult to solve without some restrictions applied. The restriction applied here is that the RBM is an MRF where the graph's connections are only between the hidden and visible layers but NOT between any nodes/variables in the same layer (this is the restricted bit) which means hidden and visible variables are independent. The authors describe how this simplifies the Gibbs sampling: all variables in a layer can be sampled in a block instead of sampling new variables one by one. This increase in efficiency can allow scientists to apply RBMs to their dataset, getting optimal weights and biases for the RBM and use this information to later feed into a classifier. More specifically, the researchers discuss how a trained RBM or deep belief network (DBN) is a neural network where the units in the output layer represent labels that correspond to observations and then you can use this network for further training by standard supervised learning algorithms (page 15 of the article).

2) Why is the Article Important:
The article was very dense with a lot of probability theory probably not familiar to the average graduate student. This foundational overview though is necessary to understand how RBMs work for further analysis and application.  

3) How was the Problem Solved/Methods Used:
There was no real probelm solved here. They simply explain the theory behind the RBMs and some possible applications. 

4) Results/Limitations:
No real results; the article was a combination of the authors' research. The biggest takeaway I had personally was my experience in training classifiers (supervised learning) is large datasets can cause them to train super slow. It's almost like GPU is needed to run these classifiers. Like the article mentions, the RBM might be a great first step for a large dataset before feeding data to a classifier since the units in the output layer are basically like labels.

<h4 style="font-weight: bold;">Research Article 2:</h4> 
Restricted Boltzmann machines in quantum physics [@melko2019restricted]

1) Goal of Paper:
The goal of the paper was to introduce RBMs as a tool used in solving complex wave functions (wave functions are used in quantum physics to explain features--postion, momentum, etc.--of a particle or group of particles). The authors describe how RBMs can learn an unknown probability distribution from a set of data pulled from that distribution (this was discussed in Research Article 1 as well). The goal in this training of RBMs is to find the optimal parameters (weights and biases) that minimize the energy functional. Traditionally, complex wave functions in many body complex quantum problems have been solved with tensor networks (TNs) but these can't really work to solve systems that are subject to volume law entanglement vice area law. Entanglement here means particles in a system do not act independently; they're movements are correlated. So area law entanglement means entropy of the system scales with area (etc. for volume). I was able to read more about area law on page 2 here [@eisert2008area]

2) Why is the Article Important:
This article is important for anyone working in the field of physics interested in the possibility of applying RBMs to their problems. For graduate students, this article would likely interest chemistry and physics students more than data science students. I personally struggled way less with this article as my undergraduate degree in Biochemistry and Molecular Biology required a lot of physics and understanding of particle interactions and energy states. This article delved into some probability theory of course since the Markov Random Field/undirected graphical model makes up the Restricted Boltzmann Machine, but familiarity with the Research Article 1 will allow the graduate student to understand this Research Article 2 as well.

3) How was the Problem Solved/Methods Used:
The authors reference another article [@chen2018equivalence] reference 27, to explain how RBMs can be translated into tensor network states (TNS) which can allow for solving many-body quantum systems (which is where instead of assessing an individual particle, all particles in the system are assessed especially with regard to their correlations with each other). Wave functions are the answer to this and there are many many possible wave functions depending on the quantum state. The ability of the RBM to handle volume law entanglement problems makes them useful in quantum physics.

4) Results/Limitations:
The authors discuss on page 889 how although RBMs take advantage of the excellence of neural networks and machine learning algorithms; they also inherit the drawbacks. The authors discuss optimization problems: basically, how many layers should the network have and other network-architecture questions that properly explain the physics problem at hand. An inefficient network architecture means a less-than-optimal algorithm for solving a cost function/minimizing energy states.


## Methods

-   Detail the models or algorithms used.

-   Justify your choices based on the problem and data.

*The common non-parametric regression model is*
$Y_i = m(X_i) + \varepsilon_i$*, where* $Y_i$ *can be defined as the sum
of the regression function value* $m(x)$ *for* $X_i$*. Here* $m(x)$ *is
unknown and* $\varepsilon_i$ *some errors. With the help of this
definition, we can create the estimation for local averaging i.e.*
$m(x)$ *can be estimated with the product of* $Y_i$ *average and* $X_i$
*is near to* $x$*. In other words, this means that we are discovering
the line through the data points with the help of surrounding data
points. The estimation formula is printed below [@R-base]:*

$$
M_n(x) = \sum_{i=1}^{n} W_n (X_i) Y_i  \tag{1}
$$$W_n(x)$ *is the sum of weights that belongs to all real numbers.
Weights are positive numbers and small if* $X_i$ *is far from* $x$*.*


*Another equation:*

$$
y_i = \beta_0 + \beta_1 X_1 +\varepsilon_i
$$



## Analysis and Results

### Data Exploration and Visualization

-   Describe your data sources and collection process.

-   Present initial findings and insights through visualizations.

-   Highlight unexpected patterns or anomalies.

A study was conducted to determine how...

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
```

```{r, warning=FALSE, echo=TRUE}
# Load Data
kable(head(murders))

ggplot1 = murders %>% ggplot(mapping = aes(x=population/10^6, y=total)) 

  ggplot1 + geom_point(aes(col=region), size = 4) +
  geom_text_repel(aes(label=abb)) +
  scale_x_log10() +
  scale_y_log10() +
  geom_smooth(formula = "y~x", method=lm,se = F)+
  xlab("Populations in millions (log10 scale)") + 
  ylab("Total number of murders (log10 scale)") +
  ggtitle("US Gun Murders in 2010") +
  scale_color_discrete(name = "Region")+
      theme_bw()
  

```

### Modeling and Results

-   Explain your data preprocessing and cleaning steps.

-   Present your key findings in a clear and concise manner.

-   Use visuals to support your claims.

-   **Tell a story about what the data reveals.**

```{r}

```

### Conclusion

-   Summarize your key findings.

-   Discuss the implications of your results.

## References
