[
  {
    "objectID": "litreview.html",
    "href": "litreview.html",
    "title": "Literature Review",
    "section": "",
    "text": "The goal of the paper was to describe the mathematics and theory behind RBMs. First, the authors go into detail about how Boltzmann Machines are undirected graphical models (Markov Random Fields–MRFs) and explain the theory behind them. The important point being that the probability distribution is a complex Gibbs distribution and sampling from it can be difficult to solve without some restrictions applied. The restriction applied here is that the RBM is an MRF where the graph’s connections are only between the hidden and visible layers but NOT between any nodes/variables in the same layer (this is the restricted bit) which means hidden and visible variables are independent. The authors describe how this simplifies the Gibbs sampling: all variables in a layer can be sampled in a block instead of sampling new variables one by one. This increase in efficiency can allow scientists to apply RBMs to their dataset, getting optimal weights and biases for the RBM and use this information to later feed into a classifier. More specifically, the researchers discuss how a trained RBM or deep belief network (DBN) is a neural network where the units in the output layer represent labels that correspond to observations and then you can use this network for further training by standard supervised learning algorithms (page 15 of the article).\n\n\n\nThe article was very dense with a lot of probability theory probably not familiar to the average graduate student. This foundational overview though is necessary to understand how RBMs work for further analysis and application.\n\n\n\nThere was no real probelm solved here. They simply explain the theory behind the RBMs and some possible applications.\n\n\n\nNo real results; the article was a combination of the authors’ research. The biggest takeaway I had personally was my experience in training classifiers (supervised learning) is large datasets can cause them to train super slow. It’s almost like GPU is needed to run these classifiers. Like the article mentions, the RBM might be a great first step for a large dataset before feeding data to a classifier since the units in the output layer are basically like labels."
  },
  {
    "objectID": "litreview.html#goal-of-paper",
    "href": "litreview.html#goal-of-paper",
    "title": "Literature Review",
    "section": "",
    "text": "The goal of the paper was to describe the mathematics and theory behind RBMs. First, the authors go into detail about how Boltzmann Machines are undirected graphical models (Markov Random Fields–MRFs) and explain the theory behind them. The important point being that the probability distribution is a complex Gibbs distribution and sampling from it can be difficult to solve without some restrictions applied. The restriction applied here is that the RBM is an MRF where the graph’s connections are only between the hidden and visible layers but NOT between any nodes/variables in the same layer (this is the restricted bit) which means hidden and visible variables are independent. The authors describe how this simplifies the Gibbs sampling: all variables in a layer can be sampled in a block instead of sampling new variables one by one. This increase in efficiency can allow scientists to apply RBMs to their dataset, getting optimal weights and biases for the RBM and use this information to later feed into a classifier. More specifically, the researchers discuss how a trained RBM or deep belief network (DBN) is a neural network where the units in the output layer represent labels that correspond to observations and then you can use this network for further training by standard supervised learning algorithms (page 15 of the article)."
  },
  {
    "objectID": "litreview.html#why-is-the-article-important",
    "href": "litreview.html#why-is-the-article-important",
    "title": "Literature Review",
    "section": "",
    "text": "The article was very dense with a lot of probability theory probably not familiar to the average graduate student. This foundational overview though is necessary to understand how RBMs work for further analysis and application."
  },
  {
    "objectID": "litreview.html#how-was-the-problem-solvedmethods-used",
    "href": "litreview.html#how-was-the-problem-solvedmethods-used",
    "title": "Literature Review",
    "section": "",
    "text": "There was no real probelm solved here. They simply explain the theory behind the RBMs and some possible applications."
  },
  {
    "objectID": "litreview.html#resultslimitations",
    "href": "litreview.html#resultslimitations",
    "title": "Literature Review",
    "section": "",
    "text": "No real results; the article was a combination of the authors’ research. The biggest takeaway I had personally was my experience in training classifiers (supervised learning) is large datasets can cause them to train super slow. It’s almost like GPU is needed to run these classifiers. Like the article mentions, the RBM might be a great first step for a large dataset before feeding data to a classifier since the units in the output layer are basically like labels."
  },
  {
    "objectID": "litreview.html#goal-of-paper-1",
    "href": "litreview.html#goal-of-paper-1",
    "title": "Literature Review",
    "section": "Goal of Paper:",
    "text": "Goal of Paper:\nThe goal of the paper was to introduce RBMs as a tool used in solving complex wave functions (wave functions are used in quantum physics to explain features–postion, momentum, etc.–of a particle or group of particles). The authors describe how RBMs can learn an unknown probability distribution from a set of data pulled from that distribution (this was discussed in Research Article 1 as well). The goal in this training of RBMs is to find the optimal parameters (weights and biases) that minimize the energy functional. Traditionally, complex wave functions in many body complex quantum problems have been solved with tensor networks (TNs) but these can’t really work to solve systems that are subject to volume law entanglement vice area law. Entanglement here means particles in a system do not act independently; they’re movements are correlated. So area law entanglement means entropy of the system scales with area (etc. for volume). I was able to read more about area law on page 2 here (Eisert, Cramer, and Plenio 2008)"
  },
  {
    "objectID": "litreview.html#why-is-the-article-important-1",
    "href": "litreview.html#why-is-the-article-important-1",
    "title": "Literature Review",
    "section": "Why is the Article Important:",
    "text": "Why is the Article Important:\nThis article is important for anyone working in the field of physics interested in the possibility of applying RBMs to their problems. For graduate students, this article would likely interest chemistry and physics students more than data science students. I personally struggled way less with this article as my undergraduate degree in Biochemistry and Molecular Biology required a lot of physics and understanding of particle interactions and energy states. This article delved into some probability theory of course since the Markov Random Field/undirected graphical model makes up the Restricted Boltzmann Machine, but familiarity with the Research Article 1 will allow the graduate student to understand this Research Article 2 as well."
  },
  {
    "objectID": "litreview.html#how-was-the-problem-solvedmethods-used-1",
    "href": "litreview.html#how-was-the-problem-solvedmethods-used-1",
    "title": "Literature Review",
    "section": "How was the Problem Solved/Methods Used:",
    "text": "How was the Problem Solved/Methods Used:\nThe authors reference another article (Chen et al. 2018) reference 27, to explain how RBMs can be translated into tensor network states (TNS) which can allow for solving many-body quantum systems (which is where instead of assessing an individual particle, all particles in the system are assessed especially with regard to their correlations with each other). Wave functions are the answer to this and there are many many possible wave functions depending on the quantum state. The ability of the RBM to handle volume law entanglement problems makes them useful in quantum physics."
  },
  {
    "objectID": "litreview.html#resultslimitations-1",
    "href": "litreview.html#resultslimitations-1",
    "title": "Literature Review",
    "section": "Results/Limitations:",
    "text": "Results/Limitations:\nThe authors discuss on page 889 how although RBMs take advantage of the excellence of neural networks and machine learning algorithms; they also inherit the drawbacks. The authors discuss optimization problems: basically, how many layers should the network have and other network-architecture questions that properly explain the physics problem at hand. An inefficient network architecture means a less-than-optimal algorithm for solving a cost function/minimizing energy states."
  },
  {
    "objectID": "litreview.html#goal-of-paper-2",
    "href": "litreview.html#goal-of-paper-2",
    "title": "Literature Review",
    "section": "Goal of Paper:",
    "text": "Goal of Paper:\nThe goal of the paper was to share their model (they refer to as R-E-BLS) and it’s superiority in predictive analysis tasks. The authors first explain the drawbacks of deep neural networks, explaining that building them deeply seriously increases computation time. Random vector function linked neural networks–and their successor, broad learning systems(BLS)–help by building the network wider instead of deeper. The Echo State Netowrk (ESN) contains a sparse reservoir (this contrasts a dense reservoir ex: fully connected network where every neuron in the reservoir is connected to every other neuron by having non zero weights). The authors introduce Broad Echo State Networks (BESNs) which combines broad learning system and echo state networks. Finally, they extend the thought further for time series forecasting problems by suggesting use of RBM in mapping layer, ESN in enhancement layer (R-E-BLS). They show the model and run experiments on it on 3 separate datasets. They explain issues with current time series forecasting like Moving Average (MA) and Autoregressive Moving Average (ARMA) where there is high dependence on linearity. They describe how the data input into the RBM mapping layer where RBM generates mapping nodes. The number of feature mapping nodes can be adjusted by adjusting network parameters of the RBM (connection weights between visible and hidden units, bias of visible unit, bias of hidden unit). Final output combines mapping and enhancement layers. The authors then discuss a huge advantage of R-E-BLS is the ability to incrementally learn (train only feature nodes or input data that needs to be added to the model without rebuilding the entire network (like standard deep learning networks))."
  },
  {
    "objectID": "litreview.html#why-is-the-article-important-2",
    "href": "litreview.html#why-is-the-article-important-2",
    "title": "Literature Review",
    "section": "Why is the Article Important:",
    "text": "Why is the Article Important:\nThe article explains how deep learning networks have such profound prediction capability, but often experience high computational load. The authors explain how using RBM prior to ESN in a broad learning system can increase predication accuracy while reducing computational workload. This is important to anyone seeking a strong predictive model with high performance without debilitating computational load."
  },
  {
    "objectID": "litreview.html#how-was-the-problem-solvedmethods-used-2",
    "href": "litreview.html#how-was-the-problem-solvedmethods-used-2",
    "title": "Literature Review",
    "section": "How was the Problem Solved/Methods Used:",
    "text": "How was the Problem Solved/Methods Used:\nThe authors used 3 datasets (air quality index, pm2.5, and electric power load) and tested various models (their R-E-BLS, LSTM, GRU, ESN, etc.) for predictive analysis (predictive boxplots, prediction error of model, and error scatter plots). They showed consistently that R-E-BLS outperforms with all 3 measures for all 3 datasets. In addition, they found the R-E-BLS fits the true data more closely even with datasets with serious fluctuations in data like the pm2.5 and power load datasets."
  },
  {
    "objectID": "litreview.html#resultslimitations-2",
    "href": "litreview.html#resultslimitations-2",
    "title": "Literature Review",
    "section": "Results/Limitations:",
    "text": "Results/Limitations:\nNone mentioned or alluded to in the article. However, future ideas were mentioned. The authors said they would like to apply the model to multi-column data prediction tasks in the future."
  },
  {
    "objectID": "litreview.html#goal-of-paper-3",
    "href": "litreview.html#goal-of-paper-3",
    "title": "Literature Review",
    "section": "Goal of Paper:",
    "text": "Goal of Paper:\nOverall, their goal was to test the Discriminative Restricted Boltzmann Machine (DRBM) on it’s capability to distinguish normal from anomalous network traffic. The authors discuss the difficulty in network anomaly detection where anomaly is “unusual” meaning non-normal traffic patterns that could be indicative of an attack. They describe the difficulty of obtaining datasets for supervised classification as many clients are reluctant to divulge information that could expose the internal structure of their networks and how this has basically led to slow progress in the industry of predicting anomalous network activity in general. The authors are testing whether or not there’s enough similarity in normal network behaviors for a model to learn all nuisances of normal traffic when faced with unseen anomalous network traffic. This led them to to the DRBM because of it’s classification ability with all the power of a generative model."
  },
  {
    "objectID": "litreview.html#why-is-the-article-important-3",
    "href": "litreview.html#why-is-the-article-important-3",
    "title": "Literature Review",
    "section": "Why is the Article Important:",
    "text": "Why is the Article Important:\nNetwork security is a very big deal. Administrators need to be able to prepare for zero day attacks by understanding what traffic patterns are normal and which are anomalous. By having a model that correctly predicts this, clients can protect their networks."
  },
  {
    "objectID": "litreview.html#how-was-the-problem-solvedmethods-used-3",
    "href": "litreview.html#how-was-the-problem-solvedmethods-used-3",
    "title": "Literature Review",
    "section": "How was the Problem Solved/Methods Used:",
    "text": "How was the Problem Solved/Methods Used:\nThe authors describe semi-supervised anomaly detection as useful because the classifier can be trained on the normal class so that anomalous events can be detected without having to know what they look like. However, this can cause a lot of misclassification where normal events are incorrectly classified as anomalous. Their experiment involves two datasets: a real network traffic dataset with two hosts (one with normal traffic and one with traffic infected by a bot) and the KDD ’99 training dataset which was tested against the real data. They used 28 features related to network traffic in the training and used accuracy, speed, comprehensibility, and time to learn as evaluation parameters. The free energy patterns seen in the diagrams of normal and anomalous activity show the RBM was able to distinguish between the two."
  },
  {
    "objectID": "litreview.html#resultslimitations-3",
    "href": "litreview.html#resultslimitations-3",
    "title": "Literature Review",
    "section": "Results/Limitations:",
    "text": "Results/Limitations:\nThe authors discuss that when a classifier is tested in a network vastly different from the one it was trained on, performance declines. For the DRBM which can learn directly from data without relying on a distribution, it can decipher new network traffic traffic; however, the downfall of not having a distribution means the model depends heavily on the training data and can overfit (explaining why performance declines on different network traffic). They noticed a significant drop in performance when training with the KDD vice the real dataset."
  },
  {
    "objectID": "litreview.html#goal-of-paper-4",
    "href": "litreview.html#goal-of-paper-4",
    "title": "Literature Review",
    "section": "Goal of Paper:",
    "text": "Goal of Paper:\nThe overall goal was to show how RBM-GAN (Restricted Boltzmann Machine Generative Adversarial Network) can significantly improve classification performance of MRI images, especially of the brain. The authors first discuss all the various applications of GAN to MRI imaging studies and the use of RBMs with deep learning to classify brain diseases. The diseases of the brain studied here are: brain atrophy, ischemia, and white matter density with a control group of a normal brain. The first step is data augmentation and resizing of the brain MR images, then normalization. Then, the RBM extracts features of only the brain region and feeds this input to the GAN generator and the pre-processed real image data are fed to the GAN discriminator where the generator tries to generate fake data similar to the real data it’s learned and the discriminator chooses what data is real and fake (these two parts of the GAN are trained simultaneously). After training with GAN, several classifiers were tested (tree, linear discriminant, naiive bayes, SVM, KNN, ensemble, neural network and K-mean)."
  },
  {
    "objectID": "litreview.html#why-is-the-article-important-4",
    "href": "litreview.html#why-is-the-article-important-4",
    "title": "Literature Review",
    "section": "Why is the Article Important:",
    "text": "Why is the Article Important:\nIt shows future physicians and/or individuals studying MRI images that applying RBM before GAN can significantly improve overall classification performance. They may additionally learn that SVM worked best for final classification."
  },
  {
    "objectID": "litreview.html#how-was-the-problem-solvedmethods-used-4",
    "href": "litreview.html#how-was-the-problem-solvedmethods-used-4",
    "title": "Literature Review",
    "section": "How was the Problem Solved/Methods Used:",
    "text": "How was the Problem Solved/Methods Used:\nThere wasn’t much of a problem to be solved as the Kaggle dataset they used was pre-labeled, but it allowed the researchers to test the hypothesis of RBM improving the overall classification process by selecting the most important features."
  },
  {
    "objectID": "litreview.html#resultslimitations-4",
    "href": "litreview.html#resultslimitations-4",
    "title": "Literature Review",
    "section": "Results/Limitations:",
    "text": "Results/Limitations:\nThey found native GAN significantly under-performed RBM-GAN and that SVM was the best classifier. The limitations discussed are that the data is from a single medical center and a small database. They also discussed hyperparameters were set to default and there’s perhaps room for optimization by adjusting hyperparameters."
  },
  {
    "objectID": "litreview.html#goal-of-paper-5",
    "href": "litreview.html#goal-of-paper-5",
    "title": "Literature Review",
    "section": "Goal of Paper:",
    "text": "Goal of Paper:\nEven with GPU, RBMs are still pretty slow during calculation of vector dot products which is done to compute the activation probability. Lean Contrastive Divergence (LCD) adds two optimization techniques which speed up the progress significantly, as discussed in results. The two optimization techniques are as follows. 1. Bounds-based filtering: uses lower and upper bounds of P(hj = 1, given v) to select a range of dot products to perform, avoiding any unnecessary dot products. The conservative bounds are found with triangle inequality. 2. Delta product: uses only the necessary operations in calculating dot products. The authors discuss how during RBM training, the network updates states/neurons across the Gibbs sampling steps/epochs. Toward the beginning, some neurons switch states (flip between 0 and 1) but later in training, less units flip states. They took advantage of the fact that many don’t flip states by realizing the non-flipped units are repeating computations unnecessarily. So, they only re-calculate the changed parts of the dot product. In Gaussian-Bernoulli RBM, visible units are real values while hidden units are binary. The authors used bounds based filtered on the hidden units and delta product to sample visible units."
  },
  {
    "objectID": "litreview.html#why-is-the-article-important-5",
    "href": "litreview.html#why-is-the-article-important-5",
    "title": "Literature Review",
    "section": "Why is the Article Important:",
    "text": "Why is the Article Important:\nRBMs are widely used in training deep belief networks which are becoming increasingly popular. The downside of RBMs is the computational expense during the process of calculating vector dot products during contrastive divergence. The authors propose the LCD with it’s two optimizing techniques (bounds-based filtering and delta product) to speed up this process."
  },
  {
    "objectID": "litreview.html#how-was-the-problem-solvedmethods-used-5",
    "href": "litreview.html#how-was-the-problem-solvedmethods-used-5",
    "title": "Literature Review",
    "section": "How was the Problem Solved/Methods Used:",
    "text": "How was the Problem Solved/Methods Used:\nThey use seven public datasets to test their theory that LCD can significantly speed up RBM training since it allows skipping of some calculations of vector dot products. They also solved an issue where although GPUs are good at handling regular calculations, LCD doesn’t work great with GPU. Because of this, the authors implemented two things. 1) Aggregated warp filterings. The RBM has to keep track of which nodes flipped states during iterations. Each thread checks if a difference was detected (neuron flipped states between iterations). Warps are groups of threads (ex: NVIDIA GPU a warp is 32 threads). Aggregated warp filterings means only the warp leader (first active thread in the warp) chooses where the writing is done in the array. Then the other threads write their differences in the locations the warp leader chose. This is all done in parallel, avoiding thread conflicts. ## Storing extra copies of W transpose. GPUs are bandwidth sensitive and in RBMs, the W matrix is used in sample the visible layer and it’s transpose is used in sampling the hidden layer so the authors store a copy of W transpose in it’s own array so it doesn’t need to be recomputed."
  },
  {
    "objectID": "litreview.html#resultslimitations-5",
    "href": "litreview.html#resultslimitations-5",
    "title": "Literature Review",
    "section": "Results/Limitations:",
    "text": "Results/Limitations:\nThe results show that LCD speeds up the training of RBM on GPU by 2–3X. No limitations were discussed or alluded to."
  },
  {
    "objectID": "litreview.html#goal-of-the-paper",
    "href": "litreview.html#goal-of-the-paper",
    "title": "Literature Review",
    "section": "Goal of the paper",
    "text": "Goal of the paper\nThe paper explains how Restricted Boltzmann Machines can be trained to predict user ratings in collaborative filtering. It details the training of such a model for predicting movie ratings using a large dataset of user ratings from Netflix."
  },
  {
    "objectID": "litreview.html#why-is-it-important",
    "href": "litreview.html#why-is-it-important",
    "title": "Literature Review",
    "section": "Why is it important?",
    "text": "Why is it important?\nCollaborative filtering is an important technique for recommendation systems and models at the time of the paper were not able to handle well the large datasets that were becoming available."
  },
  {
    "objectID": "litreview.html#how-is-it-solved-methods",
    "href": "litreview.html#how-is-it-solved-methods",
    "title": "Literature Review",
    "section": "How is it solved? – methods",
    "text": "How is it solved? – methods\nRestricted Boltzmann Machines are used, which are a type of neural network with one visible layer and one hidden layer. All nodes in the visible layer are connected to all nodes in the hidden layer, but there are no connections between nodes in the same layer. The contrastive divergence algorithm is used to train the model and works by updating the weights of the connections between the visible and hidden layers using an estimate of the gradient. The model also uses conditional RBMs to incorporate information about which movies a user has rated."
  },
  {
    "objectID": "litreview.html#resultslimitations-if-any",
    "href": "litreview.html#resultslimitations-if-any",
    "title": "Literature Review",
    "section": "Results/limitations, if any",
    "text": "Results/limitations, if any\nThe RBM model is only slightly better than SVD on the Netflix dataset, but since its errors are different from SVD, it can be combined with SVD to improve predictions."
  },
  {
    "objectID": "litreview.html#goal-of-the-paper-1",
    "href": "litreview.html#goal-of-the-paper-1",
    "title": "Literature Review",
    "section": "Goal of the paper",
    "text": "Goal of the paper\nThe paper describes Products of Experts where distributions are combined through multiplication instead of the mixture of experts approach where distributions are combined through addition. The contrastive divergence algorithm is used to train the model."
  },
  {
    "objectID": "litreview.html#why-is-it-important-1",
    "href": "litreview.html#why-is-it-important-1",
    "title": "Literature Review",
    "section": "Why is it important?",
    "text": "Why is it important?\nPoEs are able to model complex, high-dimensional data distributions. Since an RBM is a PoE with one expert per hidden unit, the contrastive divergence algorithm can be used to train RBMs."
  },
  {
    "objectID": "litreview.html#how-is-it-solved-methods-1",
    "href": "litreview.html#how-is-it-solved-methods-1",
    "title": "Literature Review",
    "section": "How is it solved? – methods",
    "text": "How is it solved? – methods\nTo train with contrastive divergence, one starts by setting the visible units to a training example and calculating hidden unit values based on randomized weights. Then, the inputs are reconstructed from the hidden units using sampling and the hidden unit values are re-computed from the reconstruction. Weights are incremented between active inputs and active hidden units for the real data and are decremented for the reconstructed data."
  },
  {
    "objectID": "litreview.html#resultslimitations-if-any-1",
    "href": "litreview.html#resultslimitations-if-any-1",
    "title": "Literature Review",
    "section": "Results/limitations, if any",
    "text": "Results/limitations, if any\nThe CD algorithm is significantly faster than other training algorithms for RBMs. PoEs can effective model complex data distributions. However, CD is an approximation and may not always converge to the best possible model."
  },
  {
    "objectID": "litreview.html#goal-of-the-paper-2",
    "href": "litreview.html#goal-of-the-paper-2",
    "title": "Literature Review",
    "section": "Goal of the paper",
    "text": "Goal of the paper\nThe paper describes a learning algorithm for deep belief networks, which are essentially stacks of RBMs. The algorithm is based on the contrastive divergence algorithm used to train RBMs."
  },
  {
    "objectID": "litreview.html#why-is-it-important-2",
    "href": "litreview.html#why-is-it-important-2",
    "title": "Literature Review",
    "section": "Why is it important?",
    "text": "Why is it important?\nDeep belief networks are able to model complex data as the paper demonstrates in the case of MNIST digits."
  },
  {
    "objectID": "litreview.html#how-is-it-solved-methods-2",
    "href": "litreview.html#how-is-it-solved-methods-2",
    "title": "Literature Review",
    "section": "How is it solved? – methods",
    "text": "How is it solved? – methods\nThe algorithm trains deep belief networks by training each layer of the network as an RBM. The weights learned in the first layer are then used to initialize the weights of the second layer. This is repeated for each layer in the network. As such, this is an example of a “greedy” algorithm, with each layer receiving a different representation of the data."
  },
  {
    "objectID": "litreview.html#resultslimitations-if-any-2",
    "href": "litreview.html#resultslimitations-if-any-2",
    "title": "Literature Review",
    "section": "Results/limitations, if any",
    "text": "Results/limitations, if any\nThe DBN model trained in the paper achieves a lower error rate on the MNIST dataset than other models. However, the model does not learn to attend to the most informative parts of the image. The DBN shows how a generative model can learn low-level features with requiring labeled data."
  },
  {
    "objectID": "litreview.html#goal-of-the-paper-3",
    "href": "litreview.html#goal-of-the-paper-3",
    "title": "Literature Review",
    "section": "Goal of the paper",
    "text": "Goal of the paper\nThe paper introduces the Persistent Contrastive Divergence algorithm for training RBMs. This algorithm preserves the state of the Markov chain (Gibbs sampling) between training examples, which makes training faster."
  },
  {
    "objectID": "litreview.html#why-is-it-important-3",
    "href": "litreview.html#why-is-it-important-3",
    "title": "Literature Review",
    "section": "Why is it important?",
    "text": "Why is it important?\nThe PCD algorithm is faster than CD which allows for training of larger models and datasets."
  },
  {
    "objectID": "litreview.html#how-is-it-solved-methods-3",
    "href": "litreview.html#how-is-it-solved-methods-3",
    "title": "Literature Review",
    "section": "How is it solved? – methods",
    "text": "How is it solved? – methods\nThe PCD algorithm is similar to CD, but instead of starting from a random state for each training example, it uses the sample from the previous training example as the starting point for the next one. The paper trains RBM models on MNIST digits, email data, and images of horses used to test image segmentation. A mini batch of training examples is used to calculate the gradient for each update of the weights."
  },
  {
    "objectID": "litreview.html#resultslimitations-if-any-3",
    "href": "litreview.html#resultslimitations-if-any-3",
    "title": "Literature Review",
    "section": "Results/limitations, if any",
    "text": "Results/limitations, if any\nFor the models trained in the paper, PCD was able to train the models faster than CD and typically achieved better results. However, PCD is still an approximation and may not always converge to the best possible model. PCD also requires a low learning rate."
  },
  {
    "objectID": "litreview.html#goal-of-the-paper-4",
    "href": "litreview.html#goal-of-the-paper-4",
    "title": "Literature Review",
    "section": "Goal of the paper",
    "text": "Goal of the paper\nSmolensky sought to encourage the exploration of mathematical analysis in the field of cognitive science, which he referenced as the subsymbolic paradigm, in contrast to the predominant focus on symbolic processing at the time. He bridged the two paradigms by demonstrating how graphical models could represent symbolic information."
  },
  {
    "objectID": "litreview.html#why-is-it-important-4",
    "href": "litreview.html#why-is-it-important-4",
    "title": "Literature Review",
    "section": "Why is it important?",
    "text": "Why is it important?\nThe harmonium model described in the paper is essentially a restricted Boltzmann machine and the harmony measure parallels the concept of energy in the Boltzmann machine. This paper encouraged further investigation into physics-based models of cognition and the discovery of more efficient learning algorithms for neural networks. The paper also reinforced the idea that effective models would possess information in the proability distribution of the data."
  },
  {
    "objectID": "litreview.html#how-is-it-solved---methods",
    "href": "litreview.html#how-is-it-solved---methods",
    "title": "Literature Review",
    "section": "How is it solved? - methods",
    "text": "How is it solved? - methods\nThe harmonium model is a bipartite graph with visible and hidden units, called representational features and knowledge atoms. A Hebbian learning rule is used to update the weights between the visible and hidden units, increasing the weights when both units are active and decreasing them when one is active and the other is not."
  },
  {
    "objectID": "litreview.html#resultslimitations-if-any-4",
    "href": "litreview.html#resultslimitations-if-any-4",
    "title": "Literature Review",
    "section": "Results/limitations, if any",
    "text": "Results/limitations, if any\nThe harmonium model is applied to some relatively trivial examples in the paper. Only later with the creation of the contrastive divergence algorithm was it possible to train RBMs on more complex data."
  },
  {
    "objectID": "litreview.html#goal-of-the-paper-5",
    "href": "litreview.html#goal-of-the-paper-5",
    "title": "Literature Review",
    "section": "Goal of the paper",
    "text": "Goal of the paper\nThe paper provides practical guidance on training Restricted Boltzmann Machines, including methods for maximizing the efficiency of the learning algorithm and choosing useful hyperparameter values."
  },
  {
    "objectID": "litreview.html#why-is-it-important-5",
    "href": "litreview.html#why-is-it-important-5",
    "title": "Literature Review",
    "section": "Why is it important?",
    "text": "Why is it important?\nRBM models are able to learn complex data distributions and can be used for a variety of tasks, including collaborative filtering and image recognition. However, without careful training procedures and hyperparameter selection, the models may not perform well."
  },
  {
    "objectID": "litreview.html#how-is-it-solved---methods-1",
    "href": "litreview.html#how-is-it-solved---methods-1",
    "title": "Literature Review",
    "section": "How is it solved? - methods",
    "text": "How is it solved? - methods\nThe paper provides guidance on how to effectively use contrastive divergence and update the weights of the model during training. It describes the considerations for choosing the size of mini-batches and the number of hidden units in the model. It provides details on choosing an initial learning rate and how to adjust it during training, and also how to use momentum to speed up training."
  },
  {
    "objectID": "litreview.html#resultslimitations-if-any-5",
    "href": "litreview.html#resultslimitations-if-any-5",
    "title": "Literature Review",
    "section": "Results/limitations, if any",
    "text": "Results/limitations, if any\nAs this paper focuses on training methods, it does not present any new results. It discusses problems such as hidden units being stuck with extremely small weights and overfitting and suggests methods for addressing these issues."
  },
  {
    "objectID": "slides.html#introduction",
    "href": "slides.html#introduction",
    "title": "Restricted Boltzmann Machines",
    "section": "Introduction",
    "text": "Introduction\nBackground\nRestricted Boltzmann Machines (RBM) are a type of neural network that has been around since the 1980s. RBMs are primarily used for unsupervised learning tasks like dimensionality reduction and feature extraction, which help prepare datasets for machine learning models that may later be trained using supervised learning.\nLike Hopfield networks, Boltzmann machines are undirected graphical models, but they are different in that they are stochastic and can have hidden units. Both models are energy-based, meaning they learn by minimizing an energy function for each model (Smolensky et al. 1986). Boltzmann machines use a sigmoid activation function, which allows for the model to be probabilistic.\nIn the “Restricted” Boltzmann Machine, there are no interactions between neurons in the visible layer or between neurons in the hidden layer, creating a bipartite graph of neurons. Below is a diagram taken from Goodfellow, et al. (Goodfellow, Bengio, and Courville 2016) (p. 577) for visualization of the connections."
  },
  {
    "objectID": "slides.html#methods",
    "href": "slides.html#methods",
    "title": "Restricted Boltzmann Machines",
    "section": "Methods",
    "text": "Methods\nBelow is the energy function of the RBM.\n\\[\nE(v,h) = - \\sum_{i} a_i v_i - \\sum_{j} b_j h_j - \\sum_{i} \\sum_{j} v_i w_{i,j} h_j\n\\qquad(1)\\] where vi and hj represent visible and hidden units; ai and bj are the bias terms of the visible and hidden units; and each w{i,j} (weight) element represents the interaction between the visible and hidden units. (Fischer and Igel 2012)"
  },
  {
    "objectID": "slides.html#data-exploration-and-visualization",
    "href": "slides.html#data-exploration-and-visualization",
    "title": "Restricted Boltzmann Machines",
    "section": "Data Exploration and Visualization",
    "text": "Data Exploration and Visualization\nWe use the Fashion MNIST dataset from Zalando Research (Xiao, Rasul, and Vollgraf 2017). The set includes 70,000 grayscale images of clothing items, 60,000 for training and 10,000 for testing. Each image is 28x28 pixels (784 pixels total). Each pixel has a value associated with it ranging from 0 (white) to 255 (very dark) – whole numbers only. There are 785 columns in total as one column is dedicated to the label."
  },
  {
    "objectID": "slides.html#data-exploration-and-visualization-1",
    "href": "slides.html#data-exploration-and-visualization-1",
    "title": "Present a great story for data science projects",
    "section": "Data Exploration and Visualization",
    "text": "Data Exploration and Visualization\nA study was conducted to determine how…"
  },
  {
    "objectID": "slides.html#modeling-and-results",
    "href": "slides.html#modeling-and-results",
    "title": "Restricted Boltzmann Machines",
    "section": "Modeling and Results",
    "text": "Modeling and Results\nOur Goal We are classifying Fashion MNIST images into one of 10 categories. To evaluate performance, we’re comparing five different models — some trained on raw pixel values and others using features extracted by a Restricted Boltzmann Machine (RBM). Our objective is to assess whether incorporating RBM into the workflow improves classification accuracy compared to using raw image data alone."
  },
  {
    "objectID": "slides.html#conclusion",
    "href": "slides.html#conclusion",
    "title": "Restricted Boltzmann Machines",
    "section": "Conclusion",
    "text": "Conclusion\n\nCNN clearly outperforms other models. Logistic Regression, which typically performs well for binary classifications tasks, underperforms on Fashion MNIST multiclassification task, but is improved by using an RBM first to extract the hidden features from the input data prior to classification. Feed Forward Network is not improved by the use of RBM. These findings show how more advanced neural networks on raw pixels can outperform models that use RBM hidden features.\nRBMs are no longer considered state-of-the-art for machine learning tasks. While contrastive divergence made training RBMs easier, supervised training of deep feedforward networks and convolutional neural networks using backpropagation proved to be more effective and began to dominate the field. However, learning RBMs is still valuable for understanding the foundations of unsupervised learning and energy-based models. The mechanics of RBM training, like Gibbs sampling, and the probabilistic nature of the model provide a demonstration of the application of probability theory and concepts like Markov chains and Boltzmann distributions in machine learning."
  },
  {
    "objectID": "slides.html#references",
    "href": "slides.html#references",
    "title": "Restricted Boltzmann Machines",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\nAkiba, Takuya, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. 2019. “Optuna: A Next-Generation Hyperparameter Optimization Framework.” In The 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2623–31.\n\n\nAslan, Narin, Sengul Dogan, and Gonca Ozmen Koca. 2023. “Automated Classification of Brain Diseases Using the Restricted Boltzmann Machine and the Generative Adversarial Network.” Engineering Applications of Artificial Intelligence 126: 106794.\n\n\nFiore, Ugo, Francesco Palmieri, Aniello Castiglione, and Alfredo De Santis. 2013. “Network Anomaly Detection with the Restricted Boltzmann Machine.” Neurocomputing 122: 13–23.\n\n\nFischer, Asja, and Christian Igel. 2012. “An Introduction to Restricted Boltzmann Machines.” In Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications: 17th Iberoamerican Congress, CIARP 2012, Buenos Aires, Argentina, September 3-6, 2012. Proceedings 17, 14–36. Springer.\n\n\nHinton, Geoffrey E. 2002. “Training Products of Experts by Minimizing Contrastive Divergence.” Neural Computation 14 (8): 1771–1800.\n\n\nMelko, Roger G, Giuseppe Carleo, Juan Carrasquilla, and J Ignacio Cirac. 2019. “Restricted Boltzmann Machines in Quantum Physics.” Nature Physics 15 (9): 887–92.\n\n\nO’Shea, Keiron, and Ryan Nash. 2015. “An Introduction to Convolutional Neural Networks.” https://arxiv.org/abs/1511.08458.\n\n\nOh, Sangchul, Abdelkader Baggag, and Hyunchul Nha. 2020. “Entropy, Free Energy, and Work of Restricted Boltzmann Machines.” Entropy 22 (5): 538.\n\n\nPeng, Chao-Ying Joanne, Kuk Lida Lee, and Gary M Ingersoll. 2002. “An Introduction to Logistic Regression Analysis and Reporting.” The Journal of Educational Research 96 (1): 3–14.\n\n\nSalakhutdinov, Ruslan, Andriy Mnih, and Geoffrey Hinton. 2007. “Restricted Boltzmann Machines for Collaborative Filtering.” In Proceedings of the 24th International Conference on Machine Learning, 791–98.\n\n\nSazlı, Murat H. 2006. “A Brief Review of Feed-Forward Neural Networks.” Communications Faculty of Sciences University of Ankara Series A2-A3 Physical Sciences and Engineering 50 (01).\n\n\nXiao, Han, Kashif Rasul, and Roland Vollgraf. 2017. “Fashion-MNIST: A Novel Image Dataset for Benchmarking Machine Learning Algorithms.” August 28, 2017. https://arxiv.org/abs/cs.LG/1708.07747.\n\n\nZaharia, Matei, Andrew Chen, Aaron Davidson, Ali Ghodsi, Sue Ann Hong, Andy Konwinski, Siddharth Murching, et al. 2018. “Accelerating the Machine Learning Lifecycle with MLflow.” IEEE Data Eng. Bull. 41 (4): 39–45."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Restricted Boltzmann Machines",
    "section": "",
    "text": "Restricted Boltzmann Machines (RBM) are a type of neural network that has been around since the 1980s. As a reminder to the reader, machine learning is generally divided into 3 categories: supervised learning (examples: classification tasks, regression), unsupervised learning (examples: clustering, dimensionality reduction, generative modeling), and reinforcement learning (examples: gaming/robotics). RBMs are primarily used for unsupervised learning tasks like dimensionality reduction and feature extraction, which help prepare datasets for machine learning models that may later be trained using supervised learning. They also have other applications which will be discussed further later.\nLike Hopfield networks, Boltzmann machines are undirected graphical models, but they are different in that they are stochastic and can have hidden units. Both models are energy-based, meaning they learn by minimizing an energy function for each model (Smolensky et al. 1986). Boltzmann machines use a sigmoid activation function, which allows for the model to be probabilistic.\nIn the “Restricted” Boltzmann Machine, there are no interactions between neurons in the visible layer or between neurons in the hidden layer, creating a bipartite graph of neurons. Below is a diagram taken from Goodfellow, et al. (Goodfellow, Bengio, and Courville 2016) (p. 577) for visualization of the connections.\n \n\n(Figure 1)\n\nGoodfellow, et al. discuss the expense in drawing samples for most undirected graphical models; however, the RBM allows for block Gibbs sampling (p. 578) where the network alternates between sampling all hidden units simultaneously (etc. for visible). Derivatives are also simplified by the fact that the energy function of the RBM is a linear function of it’s parameters, which will be seen further in Methods.\nRBMs are trained using a process called Contrastive Divergence (CD) (G. E. Hinton 2002) where the weights are updated to minimize the difference between samples from the data and samples from the model. Learning rate, batch size, and number of hidden units are all hyperparameters that can affect the ability of the training to converge successfully and learn the underlying structure of the data.\n\n\n\nRBMs are probably best known for their success in collaborative filtering. The RBM model was used in the Netflix Prize competition to predict user ratings for movies, with the result that it outperformed the Singular Value Decomposition (SVD) method that was state-of-the-art at the time (Salakhutdinov, Mnih, and Hinton 2007). They have also been trained to recognize handwritten digits, such as the MNIST dataset (G. E. Hinton 2002).\nRBMs have been successfully used to distinguish normal and anomalous network traffic. Their potential use in improving network security for companies in the future is promising. There is slow progress in network anomaly detection due to the difficulty of obtaining datasets for training and testing networks. Clients are often reluctant to divulge information that could potentially harm their networks. In a real-life dataset where one host had normal traffic and one was infected by a bot, discriminative RBM (DRBM) was able to successfully distinguish the normal from anomalous traffic. DRBM doesn’t rely on knowing the data distribution ahead of time, which is useful, except that it also causes the DRBM to overfit. As a result, when trying to use the same trained RBM on the KDD ’99 training dataset performance declined. (Fiore et al. 2013)\nRBMs can provide greatly improved classification of brain disorders in MRI images. Generative Adversarial Networks (GANs) use two neural networks: a generator which generates fake data, and a discriminator which tries to distinguish between real and fake data. Loss from the discriminator is backpropagated through the generator so that both part are trained simultaneously. The RBM-GAN uses RBM features from real MRI images as inputs to the generator. Features from the discriminator are then used as inputs to a classifier. (Aslan, Dogan, and Koca 2023)\nThe many-body quantum wavefunction, which describes the quantum state of a system of particles is difficult to compute with classical computers. RBMs have been used to approximate it using variational Monte Carlo methods. (Melko et al. 2019)\nRBMs are notoriously slow to train. The process of computing the activation probability requires the calculation of vector dot products. Lean Constrastive Divergence (LCD) is a method which adds two techniques to speed up the process of training RBMs. The first is bounds-based filtering where upper and lower bounds of the probability select only a range of dot products to perform. Second, the delta product involves only recalculating the changed portions of the vector dot product. (Ning, Pittman, and Shen 2018)"
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Restricted Boltzmann Machines",
    "section": "",
    "text": "Restricted Boltzmann Machines (RBM) are a type of neural network that has been around since the 1980s. As a reminder to the reader, machine learning is generally divided into 3 categories: supervised learning (examples: classification tasks, regression), unsupervised learning (examples: clustering, dimensionality reduction, generative modeling), and reinforcement learning (examples: gaming/robotics). RBMs are primarily used for unsupervised learning tasks like dimensionality reduction and feature extraction, which help prepare datasets for machine learning models that may later be trained using supervised learning. They also have other applications which will be discussed further later.\nLike Hopfield networks, Boltzmann machines are undirected graphical models, but they are different in that they are stochastic and can have hidden units. Both models are energy-based, meaning they learn by minimizing an energy function for each model (Smolensky et al. 1986). Boltzmann machines use a sigmoid activation function, which allows for the model to be probabilistic.\nIn the “Restricted” Boltzmann Machine, there are no interactions between neurons in the visible layer or between neurons in the hidden layer, creating a bipartite graph of neurons. Below is a diagram taken from Goodfellow, et al. (Goodfellow, Bengio, and Courville 2016) (p. 577) for visualization of the connections.\n \n\n(Figure 1)\n\nGoodfellow, et al. discuss the expense in drawing samples for most undirected graphical models; however, the RBM allows for block Gibbs sampling (p. 578) where the network alternates between sampling all hidden units simultaneously (etc. for visible). Derivatives are also simplified by the fact that the energy function of the RBM is a linear function of it’s parameters, which will be seen further in Methods.\nRBMs are trained using a process called Contrastive Divergence (CD) (G. E. Hinton 2002) where the weights are updated to minimize the difference between samples from the data and samples from the model. Learning rate, batch size, and number of hidden units are all hyperparameters that can affect the ability of the training to converge successfully and learn the underlying structure of the data.\n\n\n\nRBMs are probably best known for their success in collaborative filtering. The RBM model was used in the Netflix Prize competition to predict user ratings for movies, with the result that it outperformed the Singular Value Decomposition (SVD) method that was state-of-the-art at the time (Salakhutdinov, Mnih, and Hinton 2007). They have also been trained to recognize handwritten digits, such as the MNIST dataset (G. E. Hinton 2002).\nRBMs have been successfully used to distinguish normal and anomalous network traffic. Their potential use in improving network security for companies in the future is promising. There is slow progress in network anomaly detection due to the difficulty of obtaining datasets for training and testing networks. Clients are often reluctant to divulge information that could potentially harm their networks. In a real-life dataset where one host had normal traffic and one was infected by a bot, discriminative RBM (DRBM) was able to successfully distinguish the normal from anomalous traffic. DRBM doesn’t rely on knowing the data distribution ahead of time, which is useful, except that it also causes the DRBM to overfit. As a result, when trying to use the same trained RBM on the KDD ’99 training dataset performance declined. (Fiore et al. 2013)\nRBMs can provide greatly improved classification of brain disorders in MRI images. Generative Adversarial Networks (GANs) use two neural networks: a generator which generates fake data, and a discriminator which tries to distinguish between real and fake data. Loss from the discriminator is backpropagated through the generator so that both part are trained simultaneously. The RBM-GAN uses RBM features from real MRI images as inputs to the generator. Features from the discriminator are then used as inputs to a classifier. (Aslan, Dogan, and Koca 2023)\nThe many-body quantum wavefunction, which describes the quantum state of a system of particles is difficult to compute with classical computers. RBMs have been used to approximate it using variational Monte Carlo methods. (Melko et al. 2019)\nRBMs are notoriously slow to train. The process of computing the activation probability requires the calculation of vector dot products. Lean Constrastive Divergence (LCD) is a method which adds two techniques to speed up the process of training RBMs. The first is bounds-based filtering where upper and lower bounds of the probability select only a range of dot products to perform. Second, the delta product involves only recalculating the changed portions of the vector dot product. (Ning, Pittman, and Shen 2018)"
  },
  {
    "objectID": "index.html#methods",
    "href": "index.html#methods",
    "title": "Restricted Boltzmann Machines",
    "section": "Methods",
    "text": "Methods\nBelow is the energy function of the RBM.\n\\[\nE(v,h) = - \\sum_{i} a_i v_i - \\sum_{j} b_j h_j - \\sum_{i} \\sum_{j} v_i w_{i,j} h_j\n\\tag{1}\\] where vi and hj represent visible and hidden units; ai and bj are the bias terms of the visible and hidden units; and each w{i,j} (weight) element represents the interaction between the visible and hidden units. (Fischer and Igel 2012)\nIt is well known neural networks are prone to overfitting and often techniques such as early stopping are employed to prevent it. Some methods to prevent overfitting in RBMs are weight decay (L2 regularization), dropout, dropconnect, and weight uncertainty (Zhang et al. 2018). Dropout is a fairly well known concept in deep learning. For example, a dropout value of 0.3 added to a layer means 30% of neurons are dropped during training. This prevents the network from learning certain features too well. L2 regularization is also a commonly employed technique in deep learning. It assigns a penalty to large weights to allow for more generalization. Dropconnect is a method where a subset of weights within the network are randomly set to zero during training. Weight uncertainty is where each weight in the network has it’s own probability distribution vice a fixed value. This addition allows the network to learn more useful features.\nIf the learning rate is too high, training of the model may not converge. If it is too low, training may take a long time. To fully maximize the training of the model it is helpful to reduce the learning rate over time. This is known as learning rate decay. (G. Hinton 2010)\n\nModel Categories\nWe train Logistic Regression (with and without RBM features as input), Feed Forward Network (with and without RBM features as input), and Convolutional Neural Network. Below is a brief reminder of the basics of each model.\nFor the models incoroporating the RBM, we take the Fashion MNIST features/pixels and train the RBM (unsupervised learning) to extract hidden features from the visible layer and then feed these features into either logistic regression or feed forward network. We then use the trained model to predict labels for the test data, evaluating how well the RBM-derived features perform in a supervised classification task.\n\n1. Logistic Regression\nMathematically, the concept behind binary logistic regression is the logit (the natural logarithm of an odds ratio)(Peng, Lee, and Ingersoll 2002). However, since we have 10 labels, our classification task falls into “Multinomial Logistic Regression.”\n\\[\nP(Y = k | X) = \\frac{e^{\\beta_{0k} + \\beta_k^T X}}{\\sum_{l=1}^{K} e^{\\beta_{0l} + \\beta_l^T X}}\n\\tag{2}\\]\n\n\n2. Simple Feed Forward Neural Network\nThe feed forward network (FNN) is one where information flows in one direction from input to output with no loops or feedback. There can be zero hidden layers in between (called single FNN) or one or more hidden layers (multilayer FNN).  (Sazlı 2006) \n\n(Figure 2)\n\n\n\n3. Convolutional Neural Network\nThe convolutional neural network (CNN) is a type of feed forward network except that unlike the traditional ANN, CNNs are primarily used for pattern recognition with images (O’Shea and Nash 2015). The CNN has 3 layers which are stacked to form the full CNN: convolutional, pooling, and fully-connected layers. \n\n(Figure 3)\n\n\n\nBelow is our Process for creating the RBM:\nStep 1: We first initialize the RBM with random weights and biases and set visible units to 784 and hidden units to 256. We also set the number of contrastive divergence steps (k) to 1.  Step 2: Sample hidden units from visible. The math behind computing the hidden unit activations from the given input can be seen in Equation 3 (Fischer and Igel 2012) where the probability is used to sample from the Bernoulli distribution.  \\[\np(H_i = 1 | \\mathbf{v}) = \\sigma \\left( \\sum_{j=1}^{m} w_{ij} v_j + c_i \\right)\n\\tag{3}\\]\nwhere p(.) is the probability of the ith hidden state being activated (=1) given the visible input vector. σ is the sigmoid activation function (below) which maps the weighted sum to a probability between 0 and 1. m is the number of visible units. wij is the weight connecting visible unit j to hidden unit i. vj is the value of the jth visible unit. and ci is the bias term for the hidden unit. \\[\n\\sigma(x) = \\frac{1}{1 + e^{-x}}\n\\tag{4}\\]\nStep 3: Sample visible units from hidden. The math behind computing visible unit activations from the hidden layer can be seen in Equation 5 (Fischer and Igel 2012) Visible states are sampled using the Bernoulli distribution. This way we can see how well the RBM learned from the inputs.  \\[\np(V_j = 1 | \\mathbf{h}) = \\sigma \\left( \\sum_{i=1}^{n} w_{ij} h_i + b_j \\right)\n\\tag{5}\\]\nwhere p(.) is the probability of the ith visible unit being activated (=1) given the hidden vector h. σ is same as above. n is the number of hidden units. wij is the weight connecting hidden unit i to visible unit j. bj is the bias term for the jth visible unit.\nStep 4: K=1 steps of Contrastive Divergence (Feed Forward, Feed Backward) which executes steps 2 and 3. Contrastive Divergence updates the RBM’s weights by minimizing the difference between the original input and the reconstructed input created by the RBM.  Step 5: Free energy is computed. The free energy F is given by the logarithm of the partition function Z (Oh, Baggag, and Nha 2020) where the partition function is  \\[\nZ(\\theta) \\equiv \\sum_{v,h} e^{-E(v,h; \\theta)}\n\\tag{6}\\] and the free energy function is  \\[\nF(\\theta) = -\\ln Z(\\theta)\n\\tag{7}\\] where lower free energy means the RBM learned the visible state well.\nStep 6: Train the RBM. Model weights updated via gradient descent. Step 7: Feature extraction for classification with LR. The hidden layer activations of the RBM are used as features for Logistic Regression and Feed Forward Network.\n\n\nHyperparameter Tuning\nWe use the Tree-structured Parzen Estimator algorithm from Optuna (Akiba et al. 2019) to tune the hyperparameters of the RBM and the classifier models, and we use MLFlow (Zaharia et al. 2018) to record and visualize the results of the hyperparameter tuning process. The hyperparameters we tune include the learning rate, batch size, number of hidden units, and number of epochs.\n\n\n\nMetrics Used\n1. Accuracy Accuracy is defined as the number of correct classifications divided by the total number of classifications \\[\n\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}\n\\tag{8}\\]\n2. Macro F1 Score Macro F1 score is the unweighted average of the individual F1 scores of each class. It takes no regard for class imbalance; however, we saw earlier the classes are all balanced in Fashion MNIST. The F1 score for each individual class is as follows \\[\n\\text{F1} = \\frac{2 \\cdot \\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n\\tag{9}\\] where precision for each class is  \\[\n\\text{Precision} = \\frac{TP}{TP + FP}\n\\tag{10}\\] and recall for each class is  \\[\n\\text{Recall} = \\frac{TP}{TP + FN}\n\\tag{11}\\] The definitions of these terms for multiclass problems are more complicated than binary and are best displayed as examples. \n\n\n\n\n\n\n\n\nAcronymn\nExample for a trouser image\n\n\n\n\nTP = True Positives\nthe image is a trouser and the model predicts a trouser\n\n\nTN = True Negatives\nthe image is not a trouser and the model predicts anything but trouser\n\n\nFP = False Positives\nthe image is anything but trouser but the model predicts trouser\n\n\nFN = False Negatives\nthe image is a trouser and the model predicts another class (like shirt)\n\n\n\n\n(Table 1)\n\nAs stated earlier, the individual F1 scores for each class are taken and averaged to compute the Macro F1 score in a multiclass problem like Fashion MNIST."
  },
  {
    "objectID": "index.html#analysis-and-results",
    "href": "index.html#analysis-and-results",
    "title": "Restricted Boltzmann Machines",
    "section": "Analysis and Results",
    "text": "Analysis and Results\n\nData Exploration and Visualization\nWe use the Fashion MNIST dataset from Zalando Research (Xiao, Rasul, and Vollgraf 2017). The set includes 70,000 grayscale images of clothing items, 60,000 for training and 10,000 for testing. Each image is 28x28 pixels (784 pixels total). Each pixel has a value associated with it ranging from 0 (white) to 255 (very dark) – whole numbers only. There are 785 columns in total as one column is dedicated to the label.\n \n\n(Figure 4)\n\nThere are 10 labels in total:\n0 T-shirt/top 1 Trouser 2 Pullover 3 Dress 4 Coat 5 Sandal 6 Shirt 7 Sneaker 8 Bag 9 Ankle boot\nBelow we load the dataset.\n\n\nCode\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nimport torch\nimport torchvision.datasets\nimport torchvision.models\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\n\n\n\ntrain_data = torchvision.datasets.FashionMNIST(\n    root=\"./data\", \n    train=True, \n    download=True, \n    transform=transforms.ToTensor()  # Converts to tensor but does NOT normalize\n)\n\ntest_data = torchvision.datasets.FashionMNIST(\n    root=\"./data\", \n    train=False, \n    download=True, \n    transform=transforms.ToTensor()  \n)\n\n\nGet the seventh image to show a sample\n\n\nCode\n# Extract the first image (or choose any index)\nimage_tensor, label = train_data[6]  # shape: [1, 28, 28]\n\n# Convert tensor to NumPy array\nimage_array = image_tensor.numpy().squeeze()  \n\n# Plot the image\nplt.figure(figsize=(5,5))\nplt.imshow(image_array, cmap=\"gray\")\nplt.title(f\"FashionMNIST Image (Label: {label})\")\nplt.axis(\"off\")  # Hide axes\nplt.show()\n\n\n\n\n\n\n\n\n\n\n(Figure 5)\n\n\n\nCode\ntrain_images = train_data.data.numpy()  # Raw pixel values (0-255)\ntrain_labels = train_data.targets.numpy()\nX = train_images.reshape(-1, 784)  # Flatten 28x28 images into 1D (60000, 784)\n\n\nDisplay head of the data\n\n\nCode\n#print(train_images[:5])\nflattened = train_images[:5].reshape(5, -1) \n\n# Create a DataFrame\ndf_flat = pd.DataFrame(flattened)\nprint(df_flat.head())\n#train_df.info() #datatypes are integers\n\n\n   0    1    2    3    4    5    6    7    8    9    ...  774  775  776  777  \\\n0    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n1    0    0    0    0    0    1    0    0    0    0  ...  119  114  130   76   \n2    0    0    0    0    0    0    0    0    0   22  ...    0    0    1    0   \n3    0    0    0    0    0    0    0    0   33   96  ...    0    0    0    0   \n4    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n\n   778  779  780  781  782  783  \n0    0    0    0    0    0    0  \n1    0    0    0    0    0    0  \n2    0    0    0    0    0    0  \n3    0    0    0    0    0    0  \n4    0    0    0    0    0    0  \n\n[5 rows x 784 columns]\n\n\nThere are no missing values in the data.\n\n\nCode\nprint(np.isnan(train_images).any()) \n\n\nFalse\n\n\nThere appears to be no class imbalance\n\n\nCode\nunique_labels, counts = np.unique(train_labels, return_counts=True)\n\n# Print the counts sorted by label\nfor label, count in zip(unique_labels, counts):\n    print(f\"Label {label}: {count}\")\n\n\nLabel 0: 6000\nLabel 1: 6000\nLabel 2: 6000\nLabel 3: 6000\nLabel 4: 6000\nLabel 5: 6000\nLabel 6: 6000\nLabel 7: 6000\nLabel 8: 6000\nLabel 9: 6000\n\n\n\n\nCode\nprint(f\"X shape: {X.shape}\")\n\n\nX shape: (60000, 784)\n\n\nt-SNE Visualization t-distributed Stochastic Neighbor Embedding (t-SNE) is used here to visualize the separation between classes in a high-dimensional dataset. Each point represents a single fashion item (e.g., T-shirt, Trouser, etc.), and the color corresponds to its true label across the 10 categories listed above.\n\n\nCode\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\n# Run t-SNE to reduce dimensionality\n#embeddings = TSNE(n_jobs=2).fit_transform(X)\n\ntsne = TSNE(n_jobs=-1, random_state=42)  # Use -1 to use all available cores\nembeddings = tsne.fit_transform(X) #use scikitlearn instead\n\n\n# Create scatter plot\nfigure = plt.figure(figsize=(15,7))\nplt.scatter(embeddings[:, 0], embeddings[:, 1], c=train_labels,\n            cmap=plt.cm.get_cmap(\"jet\", 10), marker='.')\nplt.colorbar(ticks=range(10))\nplt.clim(-0.5, 9.5)\nplt.title(\"t-SNE Visualization of Fashion MNIST\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n(Figure 6)\n\n\nWhat the visualization shows:  Class 1 (blue / Trousers) forms a clearly distinct and tightly packed cluster, indicating that the pixel patterns for trousers are less similar to those of other classes. In contrast, Classes 4 (Coat), 6 (Shirt), and 2 (Pullover) show significant overlap, suggesting that these clothing items are harder to distinguish visually and may lead to more confusion during classification.\n\n\n\nModeling and Results\n\nOur Goal We are classifying Fashion MNIST images into one of 10 categories. To evaluate performance, we’re comparing five different models — some trained on raw pixel values and others using features extracted by a Restricted Boltzmann Machine (RBM). Our objective is to assess whether incorporating RBM into the workflow improves classification accuracy compared to using raw image data alone.\nOur Models 1. Logistic Regression on Fashion MNIST Data 2. Feed Forward Network on Fashion MNIST Data 3. Convolutional Neural Network on Fashion MNIST Data 4. Logistic Regression on RBM Hidden Features (of Fashion MNIST Data) 5. Feed Forward Network on RBM Hidden Features (of Fashion MNIST Data)\nNote: Outputs (50 trials) and Code are below for each model. Both the code and output can be toggled by the reader. • The first click reveals a toggle labeled “Code”. • Clicking “Code” will show the output. • Clicking again will switch from output to the actual code. • Clicking “Show Code and Output” again will collapse both views.\n\nImport Libraries and Re-load data for first 3 models\n\n\nCode\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nimport numpy as np\nimport mlflow\nimport optuna\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom torch.utils.data import DataLoader\n\n# Set device\ndevice = torch.device(\"mps\")\n\n# Load Fashion-MNIST dataset again for the first 3 models\ntransform = transforms.Compose([transforms.ToTensor()])\ntrain_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\ntest_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n\n\n\n\nCode\n#mlflow.end_run()\n#run this in terminal when need to fully clean out expierment after you delete it in the ui\n#rm -rf mlruns/.trash/*\n\n\n\nModel 1: Logistic Regression on Fashion MNIST Data\n\n\nClick to Show Code and Output\n\n\n\nCode\nfrom sklearn.metrics import f1_score\n\nCLASSIFIER = \"LogisticRegression\"  # Change for FNN, LogisticRegression, or CNN\n\n\n\n# Define CNN model\nclass FashionCNN(nn.Module):\n    def __init__(self, filters1, filters2, kernel1, kernel2):\n        super(FashionCNN, self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=filters1, kernel_size=kernel1, padding=1),\n            nn.BatchNorm2d(filters1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(in_channels=filters1, out_channels=filters2, kernel_size=kernel2),\n            nn.BatchNorm2d(filters2),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        self.fc1 = None #initialize first fully connected layer as none, defined later in fwd\n        self.drop = nn.Dropout2d(0.25)\n        self.fc2 = nn.Linear(in_features=600, out_features=120)\n        self.fc3 = nn.Linear(in_features=120, out_features=10)\n        \n\n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        #Flatten tensor dynamically, preserve batch size\n        out = out.view(out.size(0), -1) \n        if self.fc1 is None:\n            self.fc1 = nn.Linear(out.shape[1], 600).to(x.device)\n        out = self.fc1(out)\n        out = self.drop(out)\n        out = self.fc2(out)\n        out = self.fc3(out)\n        return out\n\n\n# Define Optuna objective function\ndef objective(trial):\n      # Set MLflow experiment name\n    if CLASSIFIER == \"LogisticRegression\":\n        experiment = mlflow.set_experiment(\"new-pytorch-fmnist-lr-noRBM\")\n    elif CLASSIFIER == \"FNN\":\n        experiment = mlflow.set_experiment(\"new-pytorch-fmnist-fnn-noRBM\")\n    elif CLASSIFIER == \"CNN\":\n        experiment = mlflow.set_experiment(\"new-pytorch-fmnist-cnn-noRBM\")\n    batch_size = trial.suggest_int(\"batch_size\", 64, 256, step=32)\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n    mlflow.start_run(experiment_id=experiment.experiment_id)\n    num_classifier_epochs = trial.suggest_int(\"num_classifier_epochs\", 5, 5) \n    mlflow.log_param(\"num_classifier_epochs\", num_classifier_epochs)\n\n    if CLASSIFIER == \"FNN\":\n        hidden_size = trial.suggest_int(\"fnn_hidden\", 192, 384)\n        learning_rate = trial.suggest_float(\"learning_rate\", 0.0001, 0.0025)\n\n        mlflow.log_param(\"classifier\", \"FNN\")\n        mlflow.log_param(\"fnn_hidden\", hidden_size)\n        mlflow.log_param(\"learning_rate\", learning_rate)\n\n        model = nn.Sequential(\n            nn.Linear(784, hidden_size), \n            nn.ReLU(),\n            nn.Linear(hidden_size, 10)\n        ).to(device)\n\n\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n    elif CLASSIFIER == \"CNN\":\n        filters1 = trial.suggest_int(\"filters1\", 16, 64, step=16)\n        filters2 = trial.suggest_int(\"filters2\", 32, 128, step=32)\n        kernel1 = trial.suggest_int(\"kernel1\", 3, 5)\n        kernel2 = trial.suggest_int(\"kernel2\", 3, 5)\n        learning_rate = trial.suggest_float(\"learning_rate\", 0.0001, 0.0025)\n\n        mlflow.log_param(\"classifier\", \"CNN\")\n        mlflow.log_param(\"filters1\", filters1)\n        mlflow.log_param(\"filters2\", filters2)\n        mlflow.log_param(\"kernel1\", kernel1)\n        mlflow.log_param(\"kernel2\", kernel2)\n        mlflow.log_param(\"learning_rate\", learning_rate)\n\n        model = FashionCNN(filters1, filters2, kernel1, kernel2).to(device)\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n      \n    elif CLASSIFIER == \"LogisticRegression\":\n        mlflow.log_param(\"classifier\", \"LogisticRegression\")\n    \n        # Prepare data for Logistic Regression (Flatten 28x28 images to 784 features)\n        train_features = train_dataset.data.view(-1, 784).numpy()\n        train_labels = train_dataset.targets.numpy()\n        test_features = test_dataset.data.view(-1, 784).numpy()\n        test_labels = test_dataset.targets.numpy()\n    \n        # Normalize the pixel values to [0,1] for better convergence\n        train_features = train_features / 255.0\n        test_features = test_features / 255.0\n    \n    \n        C = trial.suggest_float(\"C\", 0.01, 10.0, log=True)  \n        solver = \"saga\" \n    \n        model = LogisticRegression(C=C, max_iter=num_classifier_epochs, solver=solver)\n        model.fit(train_features, train_labels)\n    \n    \n        predictions = model.predict(test_features)\n        accuracy = accuracy_score(test_labels, predictions) * 100\n        \n        macro_f1 = f1_score(test_labels, predictions, average=\"macro\") #for f1\n        print(f\"Logistic Regression Test Accuracy: {accuracy:.2f}%\")\n        print(f\"Macro F1 Score: {macro_f1:.4f}\") #for f1\n    \n        mlflow.log_param(\"C\", C)\n        mlflow.log_metric(\"test_accuracy\", accuracy)\n        mlflow.log_metric(\"macro_f1\", macro_f1) #for f1\n        mlflow.end_run()\n        return accuracy\n\n    # Training Loop for FNN and CNN\n    criterion = nn.CrossEntropyLoss()\n\n\n    model.train()\n    for epoch in range(num_classifier_epochs):\n        running_loss = 0.0\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images) if CLASSIFIER == \"CNN\" else model(images.view(images.size(0), -1))\n\n            optimizer.zero_grad()\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n\n        print(f\"{CLASSIFIER} Epoch {epoch+1}: loss = {running_loss / len(train_loader):.4f}\")\n\n    # Model Evaluation\n    model.eval()\n    correct, total = 0, 0\n    all_preds = []   # for f1\n    all_labels = [] \n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images) if CLASSIFIER == \"CNN\" else model(images.view(images.size(0), -1))\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            all_preds.extend(predicted.cpu().numpy())   #for f1\n            all_labels.extend(labels.cpu().numpy()) #for f1\n\n    accuracy = 100 * correct / total\n    macro_f1 = f1_score(all_labels, all_preds, average=\"macro\") #for f1\n    print(f\"Test Accuracy: {accuracy:.2f}%\")\n    print(f\"Macro F1 Score: {macro_f1:.4f}\") #for f1\n\n    mlflow.log_metric(\"test_accuracy\", accuracy)\n    mlflow.log_metric(\"macro_f1\", macro_f1) #for f1\n    mlflow.end_run()\n    return accuracy\n\nif __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=1) # n_trials set to 1 for quick rendering\n    print(f\"Best Parameters for {CLASSIFIER}:\", study.best_params)\n    print(\"Best Accuracy:\", study.best_value)\n\n\nLogistic Regression Test Accuracy: 84.50%\nMacro F1 Score: 0.8439\nBest Parameters for LogisticRegression: {'batch_size': 160, 'num_classifier_epochs': 5, 'C': 0.7449879259925942}\nBest Accuracy: 84.5\n\n\n\nTest Accuracy of Logistic Regression by C (inverse regularization strength) \n\n(Figure 7)\n\n\n\\[\nC = \\frac{1}{\\lambda} \\quad \\text{(inverse regularization strength)}\n\\]\nLower values of C mean more regularization (higher penalties for larger weight coefficients)\nWhat the plot shows: Most optuna trials were lower values of C, so optimization favors stronger regularization. This is further evidenced by the clustering of higher accuracies for lower values of C. A possibly anomaly is seen at C=10 with fairly high accuracy; however, it’s still not higher than lower values of C. \nModel 2: Feed Forward Network on Fashion MNIST Data\n\n\nClick to Show Code and Output\n\n\n\nCode\nfrom sklearn.metrics import f1_score\n\nCLASSIFIER = \"FNN\"  # Change for FNN, LogisticRegression, or CNN\n\n# Define CNN model\nclass FashionCNN(nn.Module):\n    def __init__(self, filters1, filters2, kernel1, kernel2):\n        super(FashionCNN, self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=filters1, kernel_size=kernel1, padding=1),\n            nn.BatchNorm2d(filters1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(in_channels=filters1, out_channels=filters2, kernel_size=kernel2),\n            nn.BatchNorm2d(filters2),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        self.fc1 = None #initialize first fully connected layer as none, defined later in fwd\n        self.drop = nn.Dropout2d(0.25)\n        self.fc2 = nn.Linear(in_features=600, out_features=120)\n        self.fc3 = nn.Linear(in_features=120, out_features=10)\n        \n\n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        #Flatten tensor dynamically\n        out = out.view(out.size(0), -1)\n        if self.fc1 is None:\n            self.fc1 = nn.Linear(out.shape[1], 600).to(x.device)\n        out = self.fc1(out)\n        out = self.drop(out)\n        out = self.fc2(out)\n        out = self.fc3(out)\n        return out\n\n\n\n# Define Optuna objective function\ndef objective(trial):\n      # Set MLflow experiment name\n    if CLASSIFIER == \"LogisticRegression\":\n        experiment = mlflow.set_experiment(\"new-pytorch-fmnist-lr-noRBM\")\n    elif CLASSIFIER == \"FNN\":\n        experiment = mlflow.set_experiment(\"new-pytorch-fmnist-fnn-noRBM\")\n    elif CLASSIFIER == \"CNN\":\n        experiment = mlflow.set_experiment(\"new-pytorch-fmnist-cnn-noRBM\")\n    batch_size = trial.suggest_int(\"batch_size\", 64, 256, step=32)\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n    mlflow.start_run(experiment_id=experiment.experiment_id)\n    num_classifier_epochs = trial.suggest_int(\"num_classifier_epochs\", 5, 5) \n    mlflow.log_param(\"num_classifier_epochs\", num_classifier_epochs)\n\n    if CLASSIFIER == \"FNN\":\n        hidden_size = trial.suggest_int(\"fnn_hidden\", 192, 384)\n        learning_rate = trial.suggest_float(\"learning_rate\", 0.0001, 0.0025)\n\n        mlflow.log_param(\"classifier\", \"FNN\")\n        mlflow.log_param(\"fnn_hidden\", hidden_size)\n        mlflow.log_param(\"learning_rate\", learning_rate)\n\n        model = nn.Sequential(\n            nn.Linear(784, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, 10)\n        ).to(device)\n\n\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n    elif CLASSIFIER == \"CNN\":\n        filters1 = trial.suggest_int(\"filters1\", 16, 64, step=16)\n        filters2 = trial.suggest_int(\"filters2\", 32, 128, step=32)\n        kernel1 = trial.suggest_int(\"kernel1\", 3, 5)\n        kernel2 = trial.suggest_int(\"kernel2\", 3, 5)\n        learning_rate = trial.suggest_float(\"learning_rate\", 0.0001, 0.0025)\n\n        mlflow.log_param(\"classifier\", \"CNN\")\n        mlflow.log_param(\"filters1\", filters1)\n        mlflow.log_param(\"filters2\", filters2)\n        mlflow.log_param(\"kernel1\", kernel1)\n        mlflow.log_param(\"kernel2\", kernel2)\n        mlflow.log_param(\"learning_rate\", learning_rate)\n\n        model = FashionCNN(filters1, filters2, kernel1, kernel2).to(device)\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n      \n    elif CLASSIFIER == \"LogisticRegression\":\n        mlflow.log_param(\"classifier\", \"LogisticRegression\")\n    \n        # Prepare data for Logistic Regression (Flatten 28x28 images to 784 features)\n        train_features = train_dataset.data.view(-1, 784).numpy()\n        train_labels = train_dataset.targets.numpy()\n        test_features = test_dataset.data.view(-1, 784).numpy()\n        test_labels = test_dataset.targets.numpy()\n    \n        # Normalize the pixel values to [0,1] for better convergence\n        train_features = train_features / 255.0\n        test_features = test_features / 255.0\n    \n    \n        C = trial.suggest_float(\"C\", 0.01, 10.0, log=True)  \n        solver = \"saga\" \n    \n        model = LogisticRegression(C=C, max_iter=num_classifier_epochs, solver=solver)\n        model.fit(train_features, train_labels)\n    \n    \n        predictions = model.predict(test_features)\n        accuracy = accuracy_score(test_labels, predictions) * 100\n        \n        macro_f1 = f1_score(test_labels, predictions, average=\"macro\") #for f1\n        print(f\"Logistic Regression Test Accuracy: {accuracy:.2f}%\")\n        print(f\"Macro F1 Score: {macro_f1:.4f}\") #for f1\n    \n        mlflow.log_param(\"C\", C)\n        mlflow.log_metric(\"test_accuracy\", accuracy)\n        mlflow.log_metric(\"macro_f1\", macro_f1) #for f1\n        mlflow.end_run()\n        return accuracy\n\n    # Training Loop for FNN and CNN\n    criterion = nn.CrossEntropyLoss()\n\n\n    model.train()\n    for epoch in range(num_classifier_epochs):\n        running_loss = 0.0\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images) if CLASSIFIER == \"CNN\" else model(images.view(images.size(0), -1))\n\n            optimizer.zero_grad()\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n\n        print(f\"{CLASSIFIER} Epoch {epoch+1}: loss = {running_loss / len(train_loader):.4f}\")\n\n    # Model Evaluation\n    model.eval()\n    correct, total = 0, 0\n    all_preds = []   # for f1\n    all_labels = [] \n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images) if CLASSIFIER == \"CNN\" else model(images.view(images.size(0), -1))\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            all_preds.extend(predicted.cpu().numpy())   #for f1\n            all_labels.extend(labels.cpu().numpy()) #for f1\n\n    accuracy = 100 * correct / total\n    macro_f1 = f1_score(all_labels, all_preds, average=\"macro\") #for f1\n    print(f\"Test Accuracy: {accuracy:.2f}%\")\n    print(f\"Macro F1 Score: {macro_f1:.4f}\") #for f1\n\n    mlflow.log_metric(\"test_accuracy\", accuracy)\n    mlflow.log_metric(\"macro_f1\", macro_f1) #for f1\n    mlflow.end_run()\n    return accuracy\n\nif __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=1) # n_trials set to 1 for quick rendering\n    print(f\"Best Parameters for {CLASSIFIER}:\", study.best_params)\n    print(\"Best Accuracy:\", study.best_value)\n\n\nFNN Epoch 1: loss = 0.4851\nFNN Epoch 2: loss = 0.3642\nFNN Epoch 3: loss = 0.3291\nFNN Epoch 4: loss = 0.3080\nFNN Epoch 5: loss = 0.2920\n\n\nTest Accuracy: 87.51%\nMacro F1 Score: 0.8759\nBest Parameters for FNN: {'batch_size': 64, 'num_classifier_epochs': 5, 'fnn_hidden': 367, 'learning_rate': 0.0023647855848675362}\nBest Accuracy: 87.51\n\n\n\nTest Accuracy by FNN Hidden Units \n\n(Figure 8)\n\n\nWhat the plot shows: Higher values of hidden units in the feedforward network were sampled more frequently by Optuna, suggesting a preference for more complex models. However, test accuracy appears to level off between 300 and 375 hidden units, suggesting complexity reached its optimal range. Further increases in hidden units would likely not yield higher accuracy.\nModel 3: Convolutional Neural Network on Fashion MNIST Data Base code for CNN structure borrowed from Kaggle\n\n\nClick to Show Code and Output\n\n\n\nCode\nfrom sklearn.metrics import f1_score\n\nCLASSIFIER = \"CNN\"  # Change for FNN, LogisticRegression, or CNN\n\n# Define CNN model\nclass FashionCNN(nn.Module):\n    def __init__(self, filters1, filters2, kernel1, kernel2):\n        super(FashionCNN, self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=filters1, kernel_size=kernel1, padding=1),\n            nn.BatchNorm2d(filters1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(in_channels=filters1, out_channels=filters2, kernel_size=kernel2),\n            nn.BatchNorm2d(filters2),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        self.fc1 = None #initialize first fully connected layer as none, defined later in fwd\n        self.drop = nn.Dropout2d(0.25)\n        self.fc2 = nn.Linear(in_features=600, out_features=120)\n        self.fc3 = nn.Linear(in_features=120, out_features=10)\n        \n\n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        #Flatten tensor dynamically\n        out = out.view(out.size(0), -1)\n        if self.fc1 is None:\n            self.fc1 = nn.Linear(out.shape[1], 600).to(x.device)\n        out = self.fc1(out)\n        out = self.drop(out)\n        out = self.fc2(out)\n        out = self.fc3(out)\n        return out\n\n\n\n# Define Optuna objective function\ndef objective(trial):\n        # Set MLflow experiment name\n    if CLASSIFIER == \"LogisticRegression\":\n        experiment = mlflow.set_experiment(\"new-pytorch-fmnist-lr-noRBM\")\n    elif CLASSIFIER == \"FNN\":\n        experiment = mlflow.set_experiment(\"new-pytorch-fmnist-fnn-noRBM\")\n    elif CLASSIFIER == \"CNN\":\n        experiment = mlflow.set_experiment(\"new-pytorch-fmnist-cnn-noRBM\")\n    batch_size = trial.suggest_int(\"batch_size\", 64, 256, step=32)\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n    mlflow.start_run(experiment_id=experiment.experiment_id)\n    num_classifier_epochs = trial.suggest_int(\"num_classifier_epochs\", 5, 5) \n    mlflow.log_param(\"num_classifier_epochs\", num_classifier_epochs)\n\n    if CLASSIFIER == \"FNN\":\n        hidden_size = trial.suggest_int(\"fnn_hidden\", 192, 384)\n        learning_rate = trial.suggest_float(\"learning_rate\", 0.0001, 0.0025)\n\n        mlflow.log_param(\"classifier\", \"FNN\")\n        mlflow.log_param(\"fnn_hidden\", hidden_size)\n        mlflow.log_param(\"learning_rate\", learning_rate)\n\n        model = nn.Sequential(\n            nn.Linear(784, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, 10)\n        ).to(device)\n\n\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n    elif CLASSIFIER == \"CNN\":\n        filters1 = trial.suggest_int(\"filters1\", 16, 64, step=16)\n        filters2 = trial.suggest_int(\"filters2\", 32, 128, step=32)\n        kernel1 = trial.suggest_int(\"kernel1\", 3, 5)\n        kernel2 = trial.suggest_int(\"kernel2\", 3, 5)\n        learning_rate = trial.suggest_float(\"learning_rate\", 0.0001, 0.0025)\n\n        mlflow.log_param(\"classifier\", \"CNN\")\n        mlflow.log_param(\"filters1\", filters1)\n        mlflow.log_param(\"filters2\", filters2)\n        mlflow.log_param(\"kernel1\", kernel1)\n        mlflow.log_param(\"kernel2\", kernel2)\n        mlflow.log_param(\"learning_rate\", learning_rate)\n\n        model = FashionCNN(filters1, filters2, kernel1, kernel2).to(device)\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n      \n    elif CLASSIFIER == \"LogisticRegression\":\n        mlflow.log_param(\"classifier\", \"LogisticRegression\")\n    \n        # Prepare data for Logistic Regression (Flatten 28x28 images to 784 features)\n        train_features = train_dataset.data.view(-1, 784).numpy()\n        train_labels = train_dataset.targets.numpy()\n        test_features = test_dataset.data.view(-1, 784).numpy()\n        test_labels = test_dataset.targets.numpy()\n    \n        # Normalize the pixel values to [0,1] for better convergence\n        train_features = train_features / 255.0\n        test_features = test_features / 255.0\n    \n    \n        C = trial.suggest_float(\"C\", 0.01, 10.0, log=True)  \n        solver = \"saga\" \n    \n        model = LogisticRegression(C=C, max_iter=num_classifier_epochs, solver=solver)\n        model.fit(train_features, train_labels)\n    \n    \n        predictions = model.predict(test_features)\n        accuracy = accuracy_score(test_labels, predictions) * 100\n        \n        macro_f1 = f1_score(test_labels, predictions, average=\"macro\") #for f1\n        print(f\"Logistic Regression Test Accuracy: {accuracy:.2f}%\")\n        print(f\"Macro F1 Score: {macro_f1:.4f}\") #for f1\n    \n        mlflow.log_param(\"C\", C)\n        mlflow.log_metric(\"test_accuracy\", accuracy)\n        mlflow.log_metric(\"macro_f1\", macro_f1) #for f1\n        mlflow.end_run()\n        return accuracy\n\n    # Training Loop for FNN and CNN\n    criterion = nn.CrossEntropyLoss()\n\n\n    model.train()\n    for epoch in range(num_classifier_epochs):\n        running_loss = 0.0\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images) if CLASSIFIER == \"CNN\" else model(images.view(images.size(0), -1))\n\n            optimizer.zero_grad()\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n\n        print(f\"{CLASSIFIER} Epoch {epoch+1}: loss = {running_loss / len(train_loader):.4f}\")\n\n    # Model Evaluation\n    model.eval()\n    correct, total = 0, 0\n    all_preds = []   # for f1\n    all_labels = [] \n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images) if CLASSIFIER == \"CNN\" else model(images.view(images.size(0), -1))\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            all_preds.extend(predicted.cpu().numpy())   #for f1\n            all_labels.extend(labels.cpu().numpy()) #for f1\n\n    accuracy = 100 * correct / total\n    macro_f1 = f1_score(all_labels, all_preds, average=\"macro\") #for f1\n    print(f\"Test Accuracy: {accuracy:.2f}%\")\n    print(f\"Macro F1 Score: {macro_f1:.4f}\") #for f1\n\n    mlflow.log_metric(\"test_accuracy\", accuracy)\n    mlflow.log_metric(\"macro_f1\", macro_f1) #for f1\n    mlflow.end_run()\n    return accuracy\n\nif __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=1) # n_trials set to 1 for quick rendering\n    print(f\"Best Parameters for {CLASSIFIER}:\", study.best_params)\n    print(\"Best Accuracy:\", study.best_value)\n\n\nCNN Epoch 1: loss = 0.5400\nCNN Epoch 2: loss = 0.3260\nCNN Epoch 3: loss = 0.2866\nCNN Epoch 4: loss = 0.2623\nCNN Epoch 5: loss = 0.2423\n\n\nTest Accuracy: 89.46%\nMacro F1 Score: 0.8916\nBest Parameters for CNN: {'batch_size': 256, 'num_classifier_epochs': 5, 'filters1': 64, 'filters2': 96, 'kernel1': 4, 'kernel2': 5, 'learning_rate': 0.0004118428546844502}\nBest Accuracy: 89.46\n\n\n\nTest Accuracy Based on the Number of Filters in the First Conv2D Layer \n\n(Figure 9)\n\n\nWhat the plot shows: Although the highest test accuracy was achieved with 64 filters in the first convolutional 2D layer, the number of filters alone isn’t a strong predictor of model performance. Each filter size shows high variance (accuracies are spread out for each value vertically). This, combined with the fact that accuracies are well distributed across the different filter counts, suggests other factors or hyperparameters may play a bigger role in predicting accuracy. \nTest Accuracy Based on the Number of Filters in the Second Conv2D Layer \n\n(Figure 10)\n\n\nWhat the plot shows: Like the first Conv2D layer, the number of filters doesn’t seem to be a extremely strong predictor in accuracy. However, Optuna has sampled more frequently from higher number of filters, even 128 for this second layer, suggesting higher filters performed better. However, like before, there is still high variance in accuracy for each number of filters. \nTest Accuracy Based on Kernel Size in the First Conv2D Layer \n\n(Figure 11)\n\n\nWhat the plot shows: Kernel size of 3 was sampled more frequently by Optuna and yielded higher accuracies than kernel sizes of 4 or 5.\nTest Accuracy Based on Kernel Size in the Second Conv2D Layer \n\n(Figure 12)\n\n\nWhat the plot shows: Like with the first convolutional 2D layer, kernel size of 3 is highly favored by Optuna and consistently led to higher test accuracies.\nModel 4: Logistic Regression on RBM Hidden Features (of Fashion MNIST Data)\n\n\nClick to Show Code and Output\n\n\n\nCode\nfrom sklearn.metrics import accuracy_score, f1_score\nCLASSIFIER = 'LogisticRegression'\n\nif CLASSIFIER == 'LogisticRegression':\n    experiment = mlflow.set_experiment(\"new-pytorch-fmnist-lr-withrbm\")\nelse:\n    experiment = mlflow.set_experiment(\"new-pytorch-fmnist-fnn-withrbm\")\n\n\nclass RBM(nn.Module):\n    def __init__(self, n_visible=784, n_hidden=256, k=1):\n        super(RBM, self).__init__()\n        self.n_visible = n_visible\n        self.n_hidden = n_hidden\n        # Initialize weights and biases\n        self.W = nn.Parameter(torch.randn(n_hidden, n_visible) * 0.1)\n        self.v_bias = nn.Parameter(torch.zeros(n_visible))\n        self.h_bias = nn.Parameter(torch.zeros(n_hidden))\n        self.k = k  # CD-k steps\n\n    def sample_h(self, v):\n        # Given visible v, sample hidden h\n        p_h = torch.sigmoid(F.linear(v, self.W, self.h_bias))  # p(h=1|v)\n        h_sample = torch.bernoulli(p_h)                        # sample Bernoulli\n        return p_h, h_sample\n\n    def sample_v(self, h):\n        # Given hidden h, sample visible v\n        p_v = torch.sigmoid(F.linear(h, self.W.t(), self.v_bias))  # p(v=1|h)\n        v_sample = torch.bernoulli(p_v)\n        return p_v, v_sample\n\n    def forward(self, v):\n        # Perform k steps of contrastive divergence starting from v\n        v_k = v.clone()\n        for _ in range(self.k):\n            _, h_k = self.sample_h(v_k)    # sample hidden from current visible\n            _, v_k = self.sample_v(h_k)    # sample visible from hidden\n        return v_k  # k-step reconstructed visible\n\n    def free_energy(self, v):\n        # Compute the visible bias term for each sample in the batch\n        vbias_term = (v * self.v_bias).sum(dim=1)  # shape: [batch_size]\n        # Compute the activation of the hidden units\n        wx_b = F.linear(v, self.W, self.h_bias)     # shape: [batch_size, n_hidden]\n        # Compute the hidden term\n        hidden_term = torch.sum(torch.log1p(torch.exp(wx_b)), dim=1)  # shape: [batch_size]\n        # Return the mean free energy over the batch\n        return - (vbias_term + hidden_term).mean()\n    \ntransform = transforms.Compose([transforms.ToTensor()])\ntrain_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\ntest_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n\ndef objective(trial):\n    num_rbm_epochs = trial.suggest_int(\"num_rbm_epochs\", 5, 5)# 24, 33)\n    batch_size = trial.suggest_int(\"batch_size\", 192, 1024)\n    rbm_lr = trial.suggest_float(\"rbm_lr\", 0.05, 0.1)\n    rbm_hidden = trial.suggest_int(\"rbm_hidden\", 384, 8192)\n\n    mlflow.start_run(experiment_id=experiment.experiment_id)\n    if CLASSIFIER != 'LogisticRegression':\n        fnn_hidden = trial.suggest_int(\"fnn_hidden\", 192, 384)\n        fnn_lr = trial.suggest_float(\"fnn_lr\", 0.0001, 0.0025)\n        mlflow.log_param(\"fnn_hidden\", fnn_hidden)\n        mlflow.log_param(\"fnn_lr\", fnn_lr)\n\n    num_classifier_epochs = trial.suggest_int(\"num_classifier_epochs\", 5, 5)# 40, 60)\n\n    mlflow.log_param(\"num_rbm_epochs\", num_rbm_epochs)\n    mlflow.log_param(\"batch_size\", batch_size)\n    mlflow.log_param(\"rbm_lr\", rbm_lr)\n    mlflow.log_param(\"rbm_hidden\", rbm_hidden)\n    mlflow.log_param(\"num_classifier_epochs\", num_classifier_epochs)\n\n    # Instantiate RBM and optimizer\n    device = torch.device(\"mps\")\n    rbm = RBM(n_visible=784, n_hidden=rbm_hidden, k=1).to(device)\n    optimizer = torch.optim.SGD(rbm.parameters(), lr=rbm_lr)\n\n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n    rbm_training_failed = False\n    # Training loop (assuming train_loader yields batches of images and labels)\n    for epoch in range(num_rbm_epochs):\n        total_loss = 0.0\n        for images, _ in train_loader:\n            # Flatten images and binarize\n            v0 = images.view(-1, 784).to(rbm.W.device)      # shape [batch_size, 784]\n            v0 = torch.bernoulli(v0)                        # sample binary input\n            vk = rbm(v0)                                    # k-step CD reconstruction\n            # Compute contrastive divergence loss (free energy difference)\n            loss = rbm.free_energy(v0) - rbm.free_energy(vk)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n        print(f\"Epoch {epoch+1}: avg free-energy loss = {total_loss/len(train_loader):.4f}\")\n        if np.isnan(total_loss):\n            rbm_training_failed = True\n            break\n\n        if rbm_training_failed:\n            accuracy = 0.0\n            macro_f1 = 0.0 \n            print(\"RBM training failed — returning 0.0 for accuracy and macro F1\")  \n            mlflow.log_metric(\"test_accuracy\", accuracy)\n            mlflow.log_metric(\"macro_f1\", macro_f1)\n            mlflow.set_tag(\"status\", \"rbm_failed\")  # Optional tag\n            mlflow.end_run()\n            return float(accuracy)\n    else:\n        rbm.eval()  # set in evaluation mode if using any layers that behave differently in training\n        features_list = []\n        labels_list = []\n        for images, labels in train_loader:\n            v = images.view(-1, 784).to(rbm.W.device)\n            v = v  # (optionally binarize or use raw normalized pixels)\n            h_prob, h_sample = rbm.sample_h(v)  # get hidden activations\n            features_list.append(h_prob.cpu().detach().numpy())\n            labels_list.append(labels.numpy())\n        train_features = np.concatenate(features_list)  # shape: [N_train, n_hidden]\n        train_labels = np.concatenate(labels_list)\n\n        # Convert pre-extracted training features and labels to tensors and create a DataLoader\n        train_features_tensor = torch.tensor(train_features, dtype=torch.float32)\n        train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n        train_feature_dataset = torch.utils.data.TensorDataset(train_features_tensor, train_labels_tensor)\n        train_feature_loader = torch.utils.data.DataLoader(train_feature_dataset, batch_size=batch_size, shuffle=True)\n\n            \n        if CLASSIFIER == 'LogisticRegression':\n            # add optuna tuning same as log reg without RBM features...\n            lr_C = trial.suggest_float(\"lr_C\", 0.01, 10.0, log=True)  \n            mlflow.log_param(\"lr_C\", lr_C)  # Log the chosen C value\n\n\n            classifier = LogisticRegression(max_iter=num_classifier_epochs, C=lr_C, solver=\"saga\") \n            classifier.fit(train_features, train_labels)            \n            \n        else:\n            classifier = nn.Sequential(\n                nn.Linear(rbm.n_hidden, fnn_hidden),\n                nn.ReLU(),\n                nn.Linear(fnn_hidden, 10)\n            )\n\n            # Move classifier to the same device as the RBM\n            classifier = classifier.to(device)\n            criterion = nn.CrossEntropyLoss()\n            classifier_optimizer = torch.optim.Adam(classifier.parameters(), lr=fnn_lr)\n\n            classifier.train()\n            for epoch in range(num_classifier_epochs):\n                running_loss = 0.0\n                for features, labels in train_feature_loader:\n                    features = features.to(device)\n                    labels = labels.to(device)\n                    \n                    # Forward pass through classifier\n                    outputs = classifier(features)\n                    loss = criterion(outputs, labels)\n                    \n                    # Backpropagation and optimization\n                    classifier_optimizer.zero_grad()\n                    loss.backward()\n                    classifier_optimizer.step()\n                    \n                    running_loss += loss.item()\n                avg_loss = running_loss / len(train_feature_loader)\n                print(f\"Classifier Epoch {epoch+1}: loss = {avg_loss:.4f}\")\n\n        # Evaluate the classifier on test data.\n        # Here we extract features from the RBM for each test image.\n        if CLASSIFIER != 'LogisticRegression':\n            classifier.eval()\n            correct = 0\n            total = 0\n        features_list = []\n        labels_list = []\n        with torch.no_grad():\n            for images, labels in test_loader:\n                v = images.view(-1, 784).to(device)\n                # Extract hidden activations; you can use either h_prob or h_sample.\n                h_prob, _ = rbm.sample_h(v)\n                if CLASSIFIER == 'LogisticRegression':\n                    features_list.append(h_prob.cpu().detach().numpy())\n                    labels_list.append(labels.numpy())\n                else:\n                    outputs = classifier(h_prob)\n                    _, predicted = torch.max(outputs.data, 1)\n                    total += labels.size(0)\n                    correct += (predicted.cpu() == labels).sum().item()\n\n        if CLASSIFIER == 'LogisticRegression':\n            test_features = np.concatenate(features_list)\n            test_labels = np.concatenate(labels_list)\n            predictions = classifier.predict(test_features)\n            accuracy = accuracy_score(test_labels, predictions) * 100\n        \n            macro_f1 = f1_score(test_labels, predictions, average=\"macro\") \n        \n        else:\n            accuracy = 100 * correct / total\n        \n            all_preds = [] \n            all_labels = [] \n            classifier.eval()\n            with torch.no_grad():\n                for images, labels in test_loader:\n                    v = images.view(-1, 784).to(device)\n                    h_prob, _ = rbm.sample_h(v)\n                    outputs = classifier(h_prob)\n                    _, predicted = torch.max(outputs.data, 1)\n                    all_preds.extend(predicted.cpu().numpy()) \n                    all_labels.extend(labels.numpy()) \n        \n            macro_f1 = f1_score(all_labels, all_preds, average=\"macro\") \n        \n        print(f\"Test Accuracy: {accuracy:.2f}%\")\n        print(f\"Macro F1 Score: {macro_f1:.4f}\") \n        \n        mlflow.log_metric(\"test_accuracy\", accuracy)\n        mlflow.log_metric(\"macro_f1\", macro_f1) \n        mlflow.end_run()\n        return float(accuracy if accuracy is not None else 0.0)\n\nif __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=1) # n_trials set to 1 for quick rendering\n    print(study.best_params)\n    print(study.best_value)\n    print(study.best_trial)\n\n\nEpoch 1: avg free-energy loss = 180.1158\nEpoch 2: avg free-energy loss = 51.5988\nEpoch 3: avg free-energy loss = 34.4182\nEpoch 4: avg free-energy loss = 28.2203\nEpoch 5: avg free-energy loss = 24.0091\n\n\nTest Accuracy: 86.29%\nMacro F1 Score: 0.8621\n{'num_rbm_epochs': 5, 'batch_size': 309, 'rbm_lr': 0.07287144443964044, 'rbm_hidden': 6931, 'num_classifier_epochs': 5, 'lr_C': 0.5601072220671769}\n86.29\nFrozenTrial(number=0, state=1, values=[86.29], datetime_start=datetime.datetime(2025, 4, 10, 10, 2, 58, 530572), datetime_complete=datetime.datetime(2025, 4, 10, 10, 4, 20, 446138), params={'num_rbm_epochs': 5, 'batch_size': 309, 'rbm_lr': 0.07287144443964044, 'rbm_hidden': 6931, 'num_classifier_epochs': 5, 'lr_C': 0.5601072220671769}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'num_rbm_epochs': IntDistribution(high=5, log=False, low=5, step=1), 'batch_size': IntDistribution(high=1024, log=False, low=192, step=1), 'rbm_lr': FloatDistribution(high=0.1, log=False, low=0.05, step=None), 'rbm_hidden': IntDistribution(high=8192, log=False, low=384, step=1), 'num_classifier_epochs': IntDistribution(high=5, log=False, low=5, step=1), 'lr_C': FloatDistribution(high=10.0, log=True, low=0.01, step=None)}, trial_id=0, value=None)\n\n\n\nTest Accuracy of Logistic Regression on RBM Hidden Features by Inverse Regularization Strength \n\n(Figure 13)\n\n\nWhat the plot shows: When using RBM-extracted hidden features as input to logistic regression, the inverse regularization strength does not appear to be a strong predictor of test accuracy.\nTest Accuracy By Number of RBM Hidden Units \n\n(Figure 14)\n\n\nWhat the plot shows: Optuna slightly favors higher number of hidden units in the rbm with a peak at 5340 (and similar peaks 5358, 5341, etc.). However, after 7000 units, accuracy appears to decline suggesting the optimum number of units was reached around that 5300 mark. \nModel 5: Feed Forward Network on RBM Hidden Features (of Fashion MNIST Data)\n\n\nClick to Show Code and Output\n\n\n\nCode\nfrom sklearn.metrics import accuracy_score, f1_score\nCLASSIFIER = 'FNN'\n\nif CLASSIFIER == 'LogisticRegression':\n    experiment = mlflow.set_experiment(\"new-pytorch-fmnist-lr-withrbm\")\nelse:\n    experiment = mlflow.set_experiment(\"new-pytorch-fmnist-fnn-withrbm\")\n\n\nclass RBM(nn.Module):\n    def __init__(self, n_visible=784, n_hidden=256, k=1):\n        super(RBM, self).__init__()\n        self.n_visible = n_visible\n        self.n_hidden = n_hidden\n        # Initialize weights and biases\n        self.W = nn.Parameter(torch.randn(n_hidden, n_visible) * 0.1)\n        self.v_bias = nn.Parameter(torch.zeros(n_visible))\n        self.h_bias = nn.Parameter(torch.zeros(n_hidden))\n        self.k = k  # CD-k steps\n\n    def sample_h(self, v):\n        # Given visible v, sample hidden h\n        p_h = torch.sigmoid(F.linear(v, self.W, self.h_bias))  # p(h=1|v)\n        h_sample = torch.bernoulli(p_h)                        # sample Bernoulli\n        return p_h, h_sample\n\n    def sample_v(self, h):\n        # Given hidden h, sample visible v\n        p_v = torch.sigmoid(F.linear(h, self.W.t(), self.v_bias))  # p(v=1|h)\n        v_sample = torch.bernoulli(p_v)\n        return p_v, v_sample\n\n    def forward(self, v):\n        # Perform k steps of contrastive divergence starting from v\n        v_k = v.clone()\n        for _ in range(self.k):\n            _, h_k = self.sample_h(v_k)    # sample hidden from current visible\n            _, v_k = self.sample_v(h_k)    # sample visible from hidden\n        return v_k  # k-step reconstructed visible\n\n    def free_energy(self, v):\n        # Compute the visible bias term for each sample in the batch\n        vbias_term = (v * self.v_bias).sum(dim=1)  # shape: [batch_size]\n        # Compute the activation of the hidden units\n        wx_b = F.linear(v, self.W, self.h_bias)     # shape: [batch_size, n_hidden]\n        # Compute the hidden term\n        hidden_term = torch.sum(torch.log1p(torch.exp(wx_b)), dim=1)  # shape: [batch_size]\n        # Return the mean free energy over the batch\n        return - (vbias_term + hidden_term).mean()\n    \ntransform = transforms.Compose([transforms.ToTensor()])\ntrain_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\ntest_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n\ndef objective(trial):\n    num_rbm_epochs = trial.suggest_int(\"num_rbm_epochs\", 5, 5)# 24, 33)\n    batch_size = trial.suggest_int(\"batch_size\", 192, 1024)\n    rbm_lr = trial.suggest_float(\"rbm_lr\", 0.05, 0.1)\n    rbm_hidden = trial.suggest_int(\"rbm_hidden\", 384, 8192)\n\n    mlflow.start_run(experiment_id=experiment.experiment_id)\n    if CLASSIFIER != 'LogisticRegression':\n        fnn_hidden = trial.suggest_int(\"fnn_hidden\", 192, 384)\n        fnn_lr = trial.suggest_float(\"fnn_lr\", 0.0001, 0.0025)\n        mlflow.log_param(\"fnn_hidden\", fnn_hidden)\n        mlflow.log_param(\"fnn_lr\", fnn_lr)\n\n    num_classifier_epochs = trial.suggest_int(\"num_classifier_epochs\", 5, 5)# 40, 60)\n\n    mlflow.log_param(\"num_rbm_epochs\", num_rbm_epochs)\n    mlflow.log_param(\"batch_size\", batch_size)\n    mlflow.log_param(\"rbm_lr\", rbm_lr)\n    mlflow.log_param(\"rbm_hidden\", rbm_hidden)\n    mlflow.log_param(\"num_classifier_epochs\", num_classifier_epochs)\n\n    # Instantiate RBM and optimizer\n    device = torch.device(\"mps\")\n    rbm = RBM(n_visible=784, n_hidden=rbm_hidden, k=1).to(device)\n    optimizer = torch.optim.SGD(rbm.parameters(), lr=rbm_lr)\n\n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n    rbm_training_failed = False\n    # Training loop (assuming train_loader yields batches of images and labels)\n    for epoch in range(num_rbm_epochs):\n        total_loss = 0.0\n        for images, _ in train_loader:\n            # Flatten images and binarize\n            v0 = images.view(-1, 784).to(rbm.W.device)      # shape [batch_size, 784]\n            v0 = torch.bernoulli(v0)                        # sample binary input\n            vk = rbm(v0)                                    # k-step CD reconstruction\n            # Compute contrastive divergence loss (free energy difference)\n            loss = rbm.free_energy(v0) - rbm.free_energy(vk)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n        print(f\"Epoch {epoch+1}: avg free-energy loss = {total_loss/len(train_loader):.4f}\")\n        if np.isnan(total_loss):\n            rbm_training_failed = True\n            break\n\n        if rbm_training_failed:\n            accuracy = 0.0\n            macro_f1 = 0.0 \n            print(\"RBM training failed — returning 0.0 for accuracy and macro F1\")  \n            mlflow.log_metric(\"test_accuracy\", accuracy)\n            mlflow.log_metric(\"macro_f1\", macro_f1)\n            mlflow.set_tag(\"status\", \"rbm_failed\")  # Optional tag\n            mlflow.end_run()\n            return float(accuracy)\n    else:\n        rbm.eval()  # set in evaluation mode if using any layers that behave differently in training\n        features_list = []\n        labels_list = []\n        for images, labels in train_loader:\n            v = images.view(-1, 784).to(rbm.W.device)\n            v = v  # (optionally binarize or use raw normalized pixels)\n            h_prob, h_sample = rbm.sample_h(v)  # get hidden activations\n            features_list.append(h_prob.cpu().detach().numpy())\n            labels_list.append(labels.numpy())\n        train_features = np.concatenate(features_list)  # shape: [N_train, n_hidden]\n        train_labels = np.concatenate(labels_list)\n\n        # Convert pre-extracted training features and labels to tensors and create a DataLoader\n        train_features_tensor = torch.tensor(train_features, dtype=torch.float32)\n        train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n        train_feature_dataset = torch.utils.data.TensorDataset(train_features_tensor, train_labels_tensor)\n        train_feature_loader = torch.utils.data.DataLoader(train_feature_dataset, batch_size=batch_size, shuffle=True)\n\n            \n        if CLASSIFIER == 'LogisticRegression':\n            # add optuna tuning same as log reg without RBM features...\n            lr_C = trial.suggest_float(\"lr_C\", 0.01, 10.0, log=True)  \n            mlflow.log_param(\"lr_C\", lr_C)  # Log the chosen C value\n\n\n            classifier = LogisticRegression(max_iter=num_classifier_epochs, C=lr_C, solver=\"saga\") \n            classifier.fit(train_features, train_labels)            \n            \n        else:\n            classifier = nn.Sequential(\n                nn.Linear(rbm.n_hidden, fnn_hidden),\n                nn.ReLU(),\n                nn.Linear(fnn_hidden, 10)\n            )\n\n            # Move classifier to the same device as the RBM\n            classifier = classifier.to(device)\n            criterion = nn.CrossEntropyLoss()\n            classifier_optimizer = torch.optim.Adam(classifier.parameters(), lr=fnn_lr)\n\n            classifier.train()\n            for epoch in range(num_classifier_epochs):\n                running_loss = 0.0\n                for features, labels in train_feature_loader:\n                    features = features.to(device)\n                    labels = labels.to(device)\n                    \n                    # Forward pass through classifier\n                    outputs = classifier(features)\n                    loss = criterion(outputs, labels)\n                    \n                    # Backpropagation and optimization\n                    classifier_optimizer.zero_grad()\n                    loss.backward()\n                    classifier_optimizer.step()\n                    \n                    running_loss += loss.item()\n                avg_loss = running_loss / len(train_feature_loader)\n                print(f\"Classifier Epoch {epoch+1}: loss = {avg_loss:.4f}\")\n\n        # Evaluate the classifier on test data.\n        # Here we extract features from the RBM for each test image.\n        if CLASSIFIER != 'LogisticRegression':\n            classifier.eval()\n            correct = 0\n            total = 0\n        features_list = []\n        labels_list = []\n        with torch.no_grad():\n            for images, labels in test_loader:\n                v = images.view(-1, 784).to(device)\n                # Extract hidden activations; you can use either h_prob or h_sample.\n                h_prob, _ = rbm.sample_h(v)\n                if CLASSIFIER == 'LogisticRegression':\n                    features_list.append(h_prob.cpu().detach().numpy())\n                    labels_list.append(labels.numpy())\n                else:\n                    outputs = classifier(h_prob)\n                    _, predicted = torch.max(outputs.data, 1)\n                    total += labels.size(0)\n                    correct += (predicted.cpu() == labels).sum().item()\n\n        if CLASSIFIER == 'LogisticRegression':\n            test_features = np.concatenate(features_list)\n            test_labels = np.concatenate(labels_list)\n            predictions = classifier.predict(test_features)\n            accuracy = accuracy_score(test_labels, predictions) * 100\n        \n            macro_f1 = f1_score(test_labels, predictions, average=\"macro\") \n        \n        else:\n            accuracy = 100 * correct / total\n        \n            all_preds = [] \n            all_labels = [] \n            classifier.eval()\n            with torch.no_grad():\n                for images, labels in test_loader:\n                    v = images.view(-1, 784).to(device)\n                    h_prob, _ = rbm.sample_h(v)\n                    outputs = classifier(h_prob)\n                    _, predicted = torch.max(outputs.data, 1)\n                    all_preds.extend(predicted.cpu().numpy()) \n                    all_labels.extend(labels.numpy()) \n        \n            macro_f1 = f1_score(all_labels, all_preds, average=\"macro\") \n        \n        print(f\"Test Accuracy: {accuracy:.2f}%\")\n        print(f\"Macro F1 Score: {macro_f1:.4f}\") \n        \n        mlflow.log_metric(\"test_accuracy\", accuracy)\n        mlflow.log_metric(\"macro_f1\", macro_f1) \n        mlflow.end_run()\n        return float(accuracy if accuracy is not None else 0.0)\n\nif __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=1) # n_trials set to 1 for quick rendering\n    print(study.best_params)\n    print(study.best_value)\n    print(study.best_trial)\n\n\nEpoch 1: avg free-energy loss = 113.5944\nEpoch 2: avg free-energy loss = 26.6351\nEpoch 3: avg free-energy loss = 19.6702\nEpoch 4: avg free-energy loss = 16.2455\nEpoch 5: avg free-energy loss = 14.0989\nClassifier Epoch 1: loss = 0.6332\nClassifier Epoch 2: loss = 0.4429\nClassifier Epoch 3: loss = 0.4131\nClassifier Epoch 4: loss = 0.3857\nClassifier Epoch 5: loss = 0.3713\n\n\nTest Accuracy: 84.79%\nMacro F1 Score: 0.8463\n{'num_rbm_epochs': 5, 'batch_size': 395, 'rbm_lr': 0.05106545919626053, 'rbm_hidden': 6122, 'fnn_hidden': 222, 'fnn_lr': 0.0017771107544991736, 'num_classifier_epochs': 5}\n84.79\nFrozenTrial(number=0, state=1, values=[84.79], datetime_start=datetime.datetime(2025, 4, 10, 10, 4, 20, 551191), datetime_complete=datetime.datetime(2025, 4, 10, 10, 4, 56, 80535), params={'num_rbm_epochs': 5, 'batch_size': 395, 'rbm_lr': 0.05106545919626053, 'rbm_hidden': 6122, 'fnn_hidden': 222, 'fnn_lr': 0.0017771107544991736, 'num_classifier_epochs': 5}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'num_rbm_epochs': IntDistribution(high=5, log=False, low=5, step=1), 'batch_size': IntDistribution(high=1024, log=False, low=192, step=1), 'rbm_lr': FloatDistribution(high=0.1, log=False, low=0.05, step=None), 'rbm_hidden': IntDistribution(high=8192, log=False, low=384, step=1), 'fnn_hidden': IntDistribution(high=384, log=False, low=192, step=1), 'fnn_lr': FloatDistribution(high=0.0025, log=False, low=0.0001, step=None), 'num_classifier_epochs': IntDistribution(high=5, log=False, low=5, step=1)}, trial_id=0, value=None)\n\n\n\nTest Accuracy by RBM Hidden Units \n\n(Figure 15)\n\n\nWhat the plot shows: Highest accuracies cluster between 2000 and 4000 hidden units in the RBM with an outlier at 3764 hidden units. This possibly suggests too few hidden units lacks the complexity needed to explain the data; however, too many hidden units is perhaps causing some overfitting–resulting in poor generalization of the FNN classifier that receives the RBM hidden features.\nTest Accuracy by FNN Hidden Units \n\n(Figure 16)\n\n\nWhat the plot shows: Surprisingly, the number of hidden units in the FNN does not show a strong correlation with test accuracy. All hidden units tested seem to result in similar performance. This suggests the FNN is able to learn from the RBM features sufficently, and additional neurons do not significantly improve generalization.\n\n\n\n\n\n\n\n\n\nModel\nOptuna Best Trial\nMLflow Test Accuracy(%)\nMacro F1 Score\n\n\n\n\nLogistic Regression\n84.71\n0.846\n\n\nFeed Forward Network\n88.06\n0.879\n\n\nConvolutional Neural Network\n91.29\n0.913\n\n\nLogistic Regression (on RBM Hidden Features)\n87.14\n0.871\n\n\nFeed Forward Network (on RBM Hidden Features)\n86.95\n0.869\n\n\n\n\n(Table 2)\n\n\n\nConclusion\n\nSummarize your key findings.\n\nCNN clearly outperforms other models. Logistic Regression, which typically performs well for binary classifications tasks, underperforms on Fashion MNIST multiclassification task. Logistic Regression is improved by using a Restricted Boltzmann Machine first to extract the hidden features from the input data prior to classification. Feed Forward Network is not improved by the use of RBM. These findings clearly show the progress in machine and deep learning and how more advanced neural networks on raw pixels can outperform models that use RBM hidden features.\n\nDiscuss the implications of your results.\n\nRestricted Boltzmann Machines are no longer considered state-of-the-art for machine learning tasks. While contrastive divergence made training RBMs easier, supervised training of deep feedforward networks and convolutional neural networks using backpropagation proved to be more effective and began to dominate the field. This was largely the result of overcoming challenges with exploding or vanishing gradients and the introduction of techniques such as batch normalization, dropout, and better weight initialization methods.\nHowever, for the student of machine learning, learning RBMs is still valuable for understanding the foundations of unsupervised learning and energy-based models. Today, Stable Diffusion is a popular type of generative AI model with an energy-based foundation. The mechanics of RBM training, like Gibbs sampling, and the probabilistic nature of the model provide a demonstration of the application of probability theory and concepts like Markov chains and Boltzmann distributions in machine learning."
  },
  {
    "objectID": "testrender.html",
    "href": "testrender.html",
    "title": "test",
    "section": "",
    "text": "Code\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nimport numpy as np\nimport mlflow\nimport optuna\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom torch.utils.data import DataLoader\n\n# Set device\ndevice = torch.device(\"mps\")\n\n# Load Fashion-MNIST dataset again for the first 3 models\ntransform = transforms.Compose([transforms.ToTensor()])\ntrain_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\ntest_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n\n\n\n\nClick to Show Code and Output\n\n\n\nCode\nCLASSIFIER = \"LogisticRegression\"  # Change for FNN, LogisticRegression, or CNN\n\n# Set MLflow experiment name\nif CLASSIFIER == \"LogisticRegression\":\n    experiment = mlflow.set_experiment(\"pytorch-fmnist-lr-noRBM\")\nelif CLASSIFIER == \"FNN\":\n    experiment = mlflow.set_experiment(\"pytorch-fmnist-fnn-noRBM\")\nelif CLASSIFIER == \"CNN\":\n    experiment = mlflow.set_experiment(\"pytorch-fmnist-cnn-noRBM\")\n\n# Define CNN model\nclass FashionCNN(nn.Module):\n    def __init__(self, filters1, filters2, kernel1, kernel2):\n        super(FashionCNN, self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=filters1, kernel_size=kernel1, padding=1),\n            nn.BatchNorm2d(filters1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(in_channels=filters1, out_channels=filters2, kernel_size=kernel2),\n            nn.BatchNorm2d(filters2),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        self.fc1 = None\n        self.drop = nn.Dropout2d(0.25)\n        self.fc2 = nn.Linear(in_features=600, out_features=120)\n        self.fc3 = nn.Linear(in_features=120, out_features=10)\n        \n\n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = out.view(out.size(0), -1)\n        if self.fc1 is None:\n            self.fc1 = nn.Linear(out.shape[1], 600).to(x.device)\n        out = self.fc1(out)\n        out = self.drop(out)\n        out = self.fc2(out)\n        out = self.fc3(out)\n        return out\n\n\n\n        # Dynamically calculate flattened size\n        out = out.view(out.size(0), -1)  # Flatten\n        if self.fc1 is None:\n            self.fc1 = nn.Linear(out.shape[1], 600).to(x.device)  # ✅ Update FC layer dynamically\n\n        out = self.fc1(out)\n        out = self.drop(out)\n        out = self.fc2(out)\n        out = self.fc3(out)\n        return out\n\n\n\n\n# Define Optuna objective function\ndef objective(trial):\n    batch_size = trial.suggest_int(\"batch_size\", 64, 256, step=32)\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n    mlflow.start_run(experiment_id=experiment.experiment_id)\n    num_classifier_epochs = trial.suggest_int(\"num_classifier_epochs\", 5, 5) \n    mlflow.log_param(\"num_classifier_epochs\", num_classifier_epochs)\n\n    if CLASSIFIER == \"FNN\":\n        hidden_size = trial.suggest_int(\"fnn_hidden\", 192, 384)\n        learning_rate = trial.suggest_float(\"learning_rate\", 0.0001, 0.0025)\n\n        mlflow.log_param(\"classifier\", \"FNN\")\n        mlflow.log_param(\"fnn_hidden\", hidden_size)\n        mlflow.log_param(\"learning_rate\", learning_rate)\n\n        model = nn.Sequential(\n            nn.Linear(784, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, 10)\n        ).to(device)\n\n\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n    elif CLASSIFIER == \"CNN\":\n        filters1 = trial.suggest_int(\"filters1\", 16, 64, step=16)\n        filters2 = trial.suggest_int(\"filters2\", 32, 128, step=32)\n        kernel1 = trial.suggest_int(\"kernel1\", 3, 5)\n        kernel2 = trial.suggest_int(\"kernel2\", 3, 5)\n        learning_rate = trial.suggest_float(\"learning_rate\", 0.0001, 0.0025)\n\n        mlflow.log_param(\"classifier\", \"CNN\")\n        mlflow.log_param(\"filters1\", filters1)\n        mlflow.log_param(\"filters2\", filters2)\n        mlflow.log_param(\"kernel1\", kernel1)\n        mlflow.log_param(\"kernel2\", kernel2)\n        mlflow.log_param(\"learning_rate\", learning_rate)\n\n        model = FashionCNN(filters1, filters2, kernel1, kernel2).to(device)\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n      \n    elif CLASSIFIER == \"LogisticRegression\":\n        mlflow.log_param(\"classifier\", \"LogisticRegression\")\n    \n        # Prepare data for Logistic Regression (Flatten 28x28 images to 784 features)\n        train_features = train_dataset.data.view(-1, 784).numpy()\n        train_labels = train_dataset.targets.numpy()\n        test_features = test_dataset.data.view(-1, 784).numpy()\n        test_labels = test_dataset.targets.numpy()\n    \n        # Normalize the pixel values to [0,1] for better convergence\n        train_features = train_features / 255.0\n        test_features = test_features / 255.0\n    \n    \n        C = trial.suggest_float(\"C\", 0.01, 10.0, log=True)  \n        solver = \"saga\" \n    \n        model = LogisticRegression(C=C, max_iter=num_classifier_epochs, solver=solver)\n        model.fit(train_features, train_labels)\n    \n    \n        predictions = model.predict(test_features)\n        accuracy = accuracy_score(test_labels, predictions) * 100\n        print(f\"Logistic Regression Test Accuracy: {accuracy:.2f}%\")\n    \n        mlflow.log_param(\"C\", C)\n        mlflow.log_metric(\"test_accuracy\", accuracy)\n        mlflow.end_run()\n        return accuracy\n\n    # Training Loop for FNN and CNN\n    criterion = nn.CrossEntropyLoss()\n\n\n    model.train()\n    for epoch in range(num_classifier_epochs):\n        running_loss = 0.0\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images) if CLASSIFIER == \"CNN\" else model(images.view(images.size(0), -1))\n\n            optimizer.zero_grad()\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n\n        print(f\"{CLASSIFIER} Epoch {epoch+1}: loss = {running_loss / len(train_loader):.4f}\")\n\n    # Model Evaluation\n    model.eval()\n    correct, total = 0, 0\n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images) if CLASSIFIER == \"CNN\" else model(images.view(images.size(0), -1))\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    accuracy = 100 * correct / total\n    print(f\"Test Accuracy: {accuracy:.2f}%\")\n\n    mlflow.log_metric(\"test_accuracy\", accuracy)\n    mlflow.end_run()\n    return accuracy\n\nif __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=50)\n    print(f\"Best Parameters for {CLASSIFIER}:\", study.best_params)\n    print(\"Best Accuracy:\", study.best_value)\n\n\nLogistic Regression Test Accuracy: 84.59%\n\n\nLogistic Regression Test Accuracy: 84.49%\n\n\nLogistic Regression Test Accuracy: 84.49%\n\n\nLogistic Regression Test Accuracy: 84.59%\n\n\nLogistic Regression Test Accuracy: 84.57%\n\n\nLogistic Regression Test Accuracy: 84.57%\n\n\nLogistic Regression Test Accuracy: 84.58%\n\n\nLogistic Regression Test Accuracy: 84.53%\n\n\nLogistic Regression Test Accuracy: 84.56%\n\n\nLogistic Regression Test Accuracy: 84.15%\n\n\nLogistic Regression Test Accuracy: 84.41%\n\n\nLogistic Regression Test Accuracy: 84.69%\n\n\nLogistic Regression Test Accuracy: 84.48%\n\n\nLogistic Regression Test Accuracy: 84.45%\n\n\nLogistic Regression Test Accuracy: 84.49%\n\n\nLogistic Regression Test Accuracy: 84.46%\n\n\nLogistic Regression Test Accuracy: 84.33%\n\n\nLogistic Regression Test Accuracy: 84.56%\n\n\nLogistic Regression Test Accuracy: 84.42%\n\n\nLogistic Regression Test Accuracy: 84.36%\n\n\nLogistic Regression Test Accuracy: 84.63%\n\n\nLogistic Regression Test Accuracy: 84.42%\n\n\nLogistic Regression Test Accuracy: 84.68%\n\n\nLogistic Regression Test Accuracy: 84.57%\n\n\nLogistic Regression Test Accuracy: 84.48%\n\n\nLogistic Regression Test Accuracy: 84.46%\n\n\nLogistic Regression Test Accuracy: 84.47%\n\n\nLogistic Regression Test Accuracy: 84.55%\n\n\nLogistic Regression Test Accuracy: 84.62%\n\n\nLogistic Regression Test Accuracy: 84.58%\n\n\nLogistic Regression Test Accuracy: 84.52%\n\n\nLogistic Regression Test Accuracy: 84.68%\n\n\nLogistic Regression Test Accuracy: 84.58%\n\n\nLogistic Regression Test Accuracy: 84.40%\n\n\nLogistic Regression Test Accuracy: 84.52%\n\n\nLogistic Regression Test Accuracy: 84.35%\n\n\nLogistic Regression Test Accuracy: 84.52%\n\n\nLogistic Regression Test Accuracy: 84.53%\n\n\nLogistic Regression Test Accuracy: 84.55%\n\n\nLogistic Regression Test Accuracy: 84.54%\n\n\nLogistic Regression Test Accuracy: 84.57%\n\n\nLogistic Regression Test Accuracy: 84.67%\n\n\nLogistic Regression Test Accuracy: 84.49%\n\n\nLogistic Regression Test Accuracy: 84.50%\n\n\nLogistic Regression Test Accuracy: 84.46%\n\n\nLogistic Regression Test Accuracy: 84.57%\n\n\nLogistic Regression Test Accuracy: 84.45%\n\n\nLogistic Regression Test Accuracy: 84.48%\n\n\nLogistic Regression Test Accuracy: 84.44%\n\n\nLogistic Regression Test Accuracy: 84.41%\nBest Parameters for LogisticRegression: {'batch_size': 96, 'num_classifier_epochs': 5, 'C': 0.35628734474915175}\nBest Accuracy: 84.69"
  },
  {
    "objectID": "slides.html#analysis-and-results",
    "href": "slides.html#analysis-and-results",
    "title": "Restricted Boltzmann Machines",
    "section": "Analysis and Results",
    "text": "Analysis and Results\nData Exploration and Visualization\nWe use the Fashion MNIST dataset from Zalando Research (Xiao, Rasul, and Vollgraf 2017). The set includes 70,000 grayscale images of clothing items, 60,000 for training and 10,000 for testing. Each image is 28x28 pixels (784 pixels total). Each pixel has a value associated with it ranging from 0 (white) to 255 (very dark) – whole numbers only. There are 785 columns in total as one column is dedicated to the label.\n \n\n(Figure 4)\n\nThere are 10 labels in total:\n0 T-shirt/top 1 Trouser 2 Pullover 3 Dress 4 Coat 5 Sandal 6 Shirt 7 Sneaker 8 Bag 9 Ankle boot\nBelow we load the dataset.\n\n\nClick to Show Code and Output\n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nimport torch\nimport torchvision.datasets\nimport torchvision.models\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\n\n\n\ntrain_data = torchvision.datasets.FashionMNIST(\n    root=\"./data\", \n    train=True, \n    download=True, \n    transform=transforms.ToTensor()  # Converts to tensor but does NOT normalize\n)\n\ntest_data = torchvision.datasets.FashionMNIST(\n    root=\"./data\", \n    train=False, \n    download=True, \n    transform=transforms.ToTensor()  \n)\n\n\nGet the seventh image to show a sample\n\n\nClick to Show Code and Output\n\n\n# Extract the first image (or choose any index)\nimage_tensor, label = train_data[6]  # shape: [1, 28, 28]\n\n# Convert tensor to NumPy array\nimage_array = image_tensor.numpy().squeeze()  \n\n# Plot the image\nplt.figure(figsize=(5,5))\nplt.imshow(image_array, cmap=\"gray\")\nplt.title(f\"FashionMNIST Image (Label: {label})\")\nplt.axis(\"off\")  # Hide axes\nplt.show()\n\n\n\n\n\n\n\n\n\n\n(Figure 5)\n\n\n\nClick to Show Code and Output\n\n\ntrain_images = train_data.data.numpy()  # Raw pixel values (0-255)\ntrain_labels = train_data.targets.numpy()\nX = train_images.reshape(-1, 784)  # Flatten 28x28 images into 1D (60000, 784)\n\n\nDisplay head of the data\n\n\nClick to Show Code and Output\n\n\n#print(train_images[:5])\nflattened = train_images[:5].reshape(5, -1) \n\n# Create a DataFrame\ndf_flat = pd.DataFrame(flattened)\nprint(df_flat.head())\n#train_df.info() #datatypes are integers\n\n   0    1    2    3    4    5    6    7    8    9    ...  774  775  776  777  \\\n0    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n1    0    0    0    0    0    1    0    0    0    0  ...  119  114  130   76   \n2    0    0    0    0    0    0    0    0    0   22  ...    0    0    1    0   \n3    0    0    0    0    0    0    0    0   33   96  ...    0    0    0    0   \n4    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n\n   778  779  780  781  782  783  \n0    0    0    0    0    0    0  \n1    0    0    0    0    0    0  \n2    0    0    0    0    0    0  \n3    0    0    0    0    0    0  \n4    0    0    0    0    0    0  \n\n[5 rows x 784 columns]\n\n\n\nThere are no missing values in the data.\n\n\nClick to Show Code and Output\n\n\nprint(np.isnan(train_images).any()) \n\nFalse\n\n\n\nThere appears to be no class imbalance\n\n\nClick to Show Code and Output\n\n\nunique_labels, counts = np.unique(train_labels, return_counts=True)\n\n# Print the counts sorted by label\nfor label, count in zip(unique_labels, counts):\n    print(f\"Label {label}: {count}\")\n\nLabel 0: 6000\nLabel 1: 6000\nLabel 2: 6000\nLabel 3: 6000\nLabel 4: 6000\nLabel 5: 6000\nLabel 6: 6000\nLabel 7: 6000\nLabel 8: 6000\nLabel 9: 6000\n\n\n\n\n\nClick to Show Code and Output\n\n\nprint(f\"X shape: {X.shape}\")\n\nX shape: (60000, 784)"
  },
  {
    "objectID": "slides.html#visualization-of-the-neural-connections-in-a-rbm",
    "href": "slides.html#visualization-of-the-neural-connections-in-a-rbm",
    "title": "Restricted Boltzmann Machines",
    "section": "Visualization of the Neural Connections in a RBM",
    "text": "Visualization of the Neural Connections in a RBM"
  },
  {
    "objectID": "slides.html#more-about-restricted-boltzmann-machines",
    "href": "slides.html#more-about-restricted-boltzmann-machines",
    "title": "Restricted Boltzmann Machines",
    "section": "More about Restricted Boltzmann Machines",
    "text": "More about Restricted Boltzmann Machines\nGoodfellow, et al. discuss the expense in drawing samples for most undirected graphical models; however, the RBM allows for block Gibbs sampling (p. 578) where the network alternates between sampling all hidden units simultaneously (etc. for visible). Derivatives are also simplified by the fact that the energy function of the RBM is a linear function of it’s parameters, which will be seen further in Methods.\nRBMs are trained using a process called Contrastive Divergence (CD) (Hinton 2002) where the weights are updated to minimize the difference between samples from the data and samples from the model. Learning rate, batch size, and number of hidden units are all hyperparameters that can affect the ability of the training to converge successfully and learn the underlying structure of the data."
  },
  {
    "objectID": "slides.html#our-modeling-process",
    "href": "slides.html#our-modeling-process",
    "title": "Restricted Boltzmann Machines",
    "section": "Our Modeling Process",
    "text": "Our Modeling Process\nWe train Logistic Regression (with and without RBM features as input), Feed Forward Network (with and without RBM features as input), and Convolutional Neural Network. Below is a brief reminder of the basics of each model.\nFor the models incoroporating the RBM, we take the Fashion MNIST features/pixels and train the RBM (unsupervised learning) to extract hidden features from the visible layer and then feed these features into either logistic regression or feed forward network. We then use the trained model to predict labels for the test data, evaluating how well the RBM-derived features perform in a supervised classification task."
  },
  {
    "objectID": "slides.html#background-on-logistic-regression",
    "href": "slides.html#background-on-logistic-regression",
    "title": "Restricted Boltzmann Machines",
    "section": "Background on Logistic Regression",
    "text": "Background on Logistic Regression\nMathematically, the concept behind binary logistic regression is the logit (the natural logarithm of an odds ratio)(Peng, Lee, and Ingersoll 2002). However, since we have 10 labels, our classification task falls into “Multinomial Logistic Regression.”\n\\[\nP(Y = k | X) = \\frac{e^{\\beta_{0k} + \\beta_k^T X}}{\\sum_{l=1}^{K} e^{\\beta_{0l} + \\beta_l^T X}}\n\\qquad(2)\\]"
  },
  {
    "objectID": "slides.html#background-on-simple-feed-forward-neural-network",
    "href": "slides.html#background-on-simple-feed-forward-neural-network",
    "title": "Restricted Boltzmann Machines",
    "section": "Background on Simple Feed Forward Neural Network",
    "text": "Background on Simple Feed Forward Neural Network\nThe feed forward network (FNN) is one where information flows in one direction from input to output with no loops or feedback. There can be zero hidden layers in between (called single FNN) or one or more hidden layers (multilayer FNN).  (Sazlı 2006)"
  },
  {
    "objectID": "slides.html#background-on-convolutional-neural-network",
    "href": "slides.html#background-on-convolutional-neural-network",
    "title": "Restricted Boltzmann Machines",
    "section": "Background on Convolutional Neural Network",
    "text": "Background on Convolutional Neural Network\nThe convolutional neural network (CNN) is a type of feed forward network except that unlike the traditional ANN, CNNs are primarily used for pattern recognition with images (O’Shea and Nash 2015). The CNN has 3 layers which are stacked to form the full CNN: convolutional, pooling, and fully-connected layers."
  },
  {
    "objectID": "slides.html#below-is-our-process-for-creating-the-rbm",
    "href": "slides.html#below-is-our-process-for-creating-the-rbm",
    "title": "Restricted Boltzmann Machines",
    "section": "Below is our Process for creating the RBM",
    "text": "Below is our Process for creating the RBM\nStep 1: We first initialize the RBM with random weights and biases and set visible units to 784 and hidden units to 256. We also set the number of contrastive divergence steps (k) to 1.  Step 2: Sample hidden units from visible. The math behind computing the hidden unit activations from the given input can be seen in Equation 3 (Fischer and Igel 2012) where the probability is used to sample from the Bernoulli distribution. \n\\[\np(H_i = 1 | \\mathbf{v}) = \\sigma \\left( \\sum_{j=1}^{m} w_{ij} v_j + c_i \\right)\n\\qquad(3)\\]\nwhere p(.) is the probability of the ith hidden state being activated (=1) given the visible input vector. σ is the sigmoid activation function (below) which maps the weighted sum to a probability between 0 and 1. m is the number of visible units. wij is the weight connecting visible unit j to hidden unit i. vj is the value of the jth visible unit. and ci is the bias term for the hidden unit.\n\\[\n\\sigma(x) = \\frac{1}{1 + e^{-x}}\n\\]\nStep 3: Sample visible units from hidden. The math behind computing visible unit activations from the hidden layer can be seen in Equation 4 (Fischer and Igel 2012) Visible states are sampled using the Bernoulli distribution. This way we can see how well the RBM learned from the inputs. \n\\[\np(V_j = 1 | \\mathbf{h}) = \\sigma \\left( \\sum_{i=1}^{n} w_{ij} h_i + b_j \\right)\n\\qquad(4)\\]\nwhere p(.) is the probability of the ith visible unit being activated (=1) given the hidden vector h. σ is same as above. n is the number of hidden units. wij is the weight connecting hidden unit i to visible unit j. bj is the bias term for the jth visible unit.\nStep 4: K=1 steps of Contrastive Divergence (Feed Forward, Feed Backward) which executes steps 2 and 3. Contrastive Divergence updates the RBM’s weights by minimizing the difference between the original input and the reconstructed input created by the RBM.  Step 5: Free energy is computed. The free energy F is given by the logarithm of the partition function Z (Oh, Baggag, and Nha 2020) where the partition function is \n\\[\nZ(\\theta) \\equiv \\sum_{v,h} e^{-E(v,h; \\theta)}\n\\qquad(5)\\]\nand the free energy function is \n\\[\nF(\\theta) = -\\ln Z(\\theta)\n\\qquad(6)\\]\nwhere lower free energy means the RBM learned the visible state well.\nStep 6: Train the RBM. Model weights updated via gradient descent. Step 7: Feature extraction for classification with LR. The hidden layer activations of the RBM are used as features for Logistic Regression and Feed Forward Network."
  },
  {
    "objectID": "slides.html#hyperparameter-tuning",
    "href": "slides.html#hyperparameter-tuning",
    "title": "Restricted Boltzmann Machines",
    "section": "Hyperparameter Tuning",
    "text": "Hyperparameter Tuning\nWe use the Tree-structured Parzen Estimator algorithm from Optuna (Akiba et al. 2019) to tune the hyperparameters of the RBM and the classifier models, and we use MLFlow (Zaharia et al. 2018) to record and visualize the results of the hyperparameter tuning process. The hyperparameters we tune include the learning rate, batch size, number of hidden units, and number of epochs."
  },
  {
    "objectID": "slides.html#metrics-used",
    "href": "slides.html#metrics-used",
    "title": "Restricted Boltzmann Machines",
    "section": "Metrics Used",
    "text": "Metrics Used\n1. Accuracy Accuracy is defined as the number of correct classifications divided by the total number of classifications \\[\n\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}\n\\qquad(8)\\]\n2. Macro F1 Score Macro F1 score is the unweighted average of the individual F1 scores of each class. It takes no regard for class imbalance; however, we saw earlier the classes are all balanced in Fashion MNIST. The F1 score for each individual class is as follows \\[\n\\text{F1} = \\frac{2 \\cdot \\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n\\qquad(9)\\] where precision for each class is  \\[\n\\text{Precision} = \\frac{TP}{TP + FP}\n\\qquad(10)\\] and recall for each class is  \\[\n\\text{Recall} = \\frac{TP}{TP + FN}\n\\qquad(11)\\] The definitions of these terms for multiclass problems are more complicated than binary and are best displayed as examples. \n\n\n\n\n\n\n\n\nAcronymn\nExample for a trouser image\n\n\n\n\nTP = True Positives\nthe image is a trouser and the model predicts a trouser\n\n\nTN = True Negatives\nthe image is not a trouser and the model predicts anything but trouser\n\n\nFP = False Positives\nthe image is anything but trouser but the model predicts trouser\n\n\nFN = False Negatives\nthe image is a trouser and the model predicts another class (like shirt)\n\n\n\n\n(Table 1)\n\nAs stated earlier, the individual F1 scores for each class are taken and averaged to compute the Macro F1 score in a multiclass problem like Fashion MNIST."
  },
  {
    "objectID": "slides.html#model-1-logistic-regression-on-fashion-mnist-data",
    "href": "slides.html#model-1-logistic-regression-on-fashion-mnist-data",
    "title": "Restricted Boltzmann Machines",
    "section": "Model 1: Logistic Regression on Fashion MNIST Data",
    "text": "Model 1: Logistic Regression on Fashion MNIST Data\nModel 1: Logistic Regression on Fashion MNIST Data\n\n\nClick to Show Code and Output\n\n\nfrom sklearn.metrics import f1_score\n\nCLASSIFIER = \"LogisticRegression\"  # Change for FNN, LogisticRegression, or CNN\n\n\n\n# Define CNN model\nclass FashionCNN(nn.Module):\n    def __init__(self, filters1, filters2, kernel1, kernel2):\n        super(FashionCNN, self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=filters1, kernel_size=kernel1, padding=1),\n            nn.BatchNorm2d(filters1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(in_channels=filters1, out_channels=filters2, kernel_size=kernel2),\n            nn.BatchNorm2d(filters2),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        self.fc1 = None #initialize first fully connected layer as none, defined later in fwd\n        self.drop = nn.Dropout2d(0.25)\n        self.fc2 = nn.Linear(in_features=600, out_features=120)\n        self.fc3 = nn.Linear(in_features=120, out_features=10)\n        \n\n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        #Flatten tensor dynamically, preserve batch size\n        out = out.view(out.size(0), -1) \n        if self.fc1 is None:\n            self.fc1 = nn.Linear(out.shape[1], 600).to(x.device)\n        out = self.fc1(out)\n        out = self.drop(out)\n        out = self.fc2(out)\n        out = self.fc3(out)\n        return out\n\n\n# Define Optuna objective function\ndef objective(trial):\n      # Set MLflow experiment name\n    if CLASSIFIER == \"LogisticRegression\":\n        experiment = mlflow.set_experiment(\"new-pytorch-fmnist-lr-noRBM\")\n    elif CLASSIFIER == \"FNN\":\n        experiment = mlflow.set_experiment(\"new-pytorch-fmnist-fnn-noRBM\")\n    elif CLASSIFIER == \"CNN\":\n        experiment = mlflow.set_experiment(\"new-pytorch-fmnist-cnn-noRBM\")\n    batch_size = trial.suggest_int(\"batch_size\", 64, 256, step=32)\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n    mlflow.start_run(experiment_id=experiment.experiment_id)\n    num_classifier_epochs = trial.suggest_int(\"num_classifier_epochs\", 5, 5) \n    mlflow.log_param(\"num_classifier_epochs\", num_classifier_epochs)\n\n    if CLASSIFIER == \"FNN\":\n        hidden_size = trial.suggest_int(\"fnn_hidden\", 192, 384)\n        learning_rate = trial.suggest_float(\"learning_rate\", 0.0001, 0.0025)\n\n        mlflow.log_param(\"classifier\", \"FNN\")\n        mlflow.log_param(\"fnn_hidden\", hidden_size)\n        mlflow.log_param(\"learning_rate\", learning_rate)\n\n        model = nn.Sequential(\n            nn.Linear(784, hidden_size), \n            nn.ReLU(),\n            nn.Linear(hidden_size, 10)\n        ).to(device)\n\n\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n    elif CLASSIFIER == \"CNN\":\n        filters1 = trial.suggest_int(\"filters1\", 16, 64, step=16)\n        filters2 = trial.suggest_int(\"filters2\", 32, 128, step=32)\n        kernel1 = trial.suggest_int(\"kernel1\", 3, 5)\n        kernel2 = trial.suggest_int(\"kernel2\", 3, 5)\n        learning_rate = trial.suggest_float(\"learning_rate\", 0.0001, 0.0025)\n\n        mlflow.log_param(\"classifier\", \"CNN\")\n        mlflow.log_param(\"filters1\", filters1)\n        mlflow.log_param(\"filters2\", filters2)\n        mlflow.log_param(\"kernel1\", kernel1)\n        mlflow.log_param(\"kernel2\", kernel2)\n        mlflow.log_param(\"learning_rate\", learning_rate)\n\n        model = FashionCNN(filters1, filters2, kernel1, kernel2).to(device)\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n      \n    elif CLASSIFIER == \"LogisticRegression\":\n        mlflow.log_param(\"classifier\", \"LogisticRegression\")\n    \n        # Prepare data for Logistic Regression (Flatten 28x28 images to 784 features)\n        train_features = train_dataset.data.view(-1, 784).numpy()\n        train_labels = train_dataset.targets.numpy()\n        test_features = test_dataset.data.view(-1, 784).numpy()\n        test_labels = test_dataset.targets.numpy()\n    \n        # Normalize the pixel values to [0,1] for better convergence\n        train_features = train_features / 255.0\n        test_features = test_features / 255.0\n    \n    \n        C = trial.suggest_float(\"C\", 0.01, 10.0, log=True)  \n        solver = \"saga\" \n    \n        model = LogisticRegression(C=C, max_iter=num_classifier_epochs, solver=solver)\n        model.fit(train_features, train_labels)\n    \n    \n        predictions = model.predict(test_features)\n        accuracy = accuracy_score(test_labels, predictions) * 100\n        \n        macro_f1 = f1_score(test_labels, predictions, average=\"macro\") #for f1\n        print(f\"Logistic Regression Test Accuracy: {accuracy:.2f}%\")\n        print(f\"Macro F1 Score: {macro_f1:.4f}\") #for f1\n    \n        mlflow.log_param(\"C\", C)\n        mlflow.log_metric(\"test_accuracy\", accuracy)\n        mlflow.log_metric(\"macro_f1\", macro_f1) #for f1\n        mlflow.end_run()\n        return accuracy\n\n    # Training Loop for FNN and CNN\n    criterion = nn.CrossEntropyLoss()\n\n\n    model.train()\n    for epoch in range(num_classifier_epochs):\n        running_loss = 0.0\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images) if CLASSIFIER == \"CNN\" else model(images.view(images.size(0), -1))\n\n            optimizer.zero_grad()\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n\n        print(f\"{CLASSIFIER} Epoch {epoch+1}: loss = {running_loss / len(train_loader):.4f}\")\n\n    # Model Evaluation\n    model.eval()\n    correct, total = 0, 0\n    all_preds = []   # for f1\n    all_labels = [] \n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images) if CLASSIFIER == \"CNN\" else model(images.view(images.size(0), -1))\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            all_preds.extend(predicted.cpu().numpy())   #for f1\n            all_labels.extend(labels.cpu().numpy()) #for f1\n\n    accuracy = 100 * correct / total\n    macro_f1 = f1_score(all_labels, all_preds, average=\"macro\") #for f1\n    print(f\"Test Accuracy: {accuracy:.2f}%\")\n    print(f\"Macro F1 Score: {macro_f1:.4f}\") #for f1\n\n    mlflow.log_metric(\"test_accuracy\", accuracy)\n    mlflow.log_metric(\"macro_f1\", macro_f1) #for f1\n    mlflow.end_run()\n    return accuracy\n\nif __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=1) # n_trials set to 1 for quick rendering\n    print(f\"Best Parameters for {CLASSIFIER}:\", study.best_params)\n    print(\"Best Accuracy:\", study.best_value)\n\nLogistic Regression Test Accuracy: 84.26%\nMacro F1 Score: 0.8417\nBest Parameters for LogisticRegression: {'batch_size': 96, 'num_classifier_epochs': 5, 'C': 0.021590868530144244}\nBest Accuracy: 84.26"
  },
  {
    "objectID": "slides.html#model-2-feed-forward-network-on-fashion-mnist-data",
    "href": "slides.html#model-2-feed-forward-network-on-fashion-mnist-data",
    "title": "Restricted Boltzmann Machines",
    "section": "Model 2: Feed Forward Network on Fashion MNIST Data",
    "text": "Model 2: Feed Forward Network on Fashion MNIST Data\nModel 2: Feed Forward Network on Fashion MNIST Data\n\n\nClick to Show Code and Output\n\n\nfrom sklearn.metrics import f1_score\n\nCLASSIFIER = \"FNN\"  # Change for FNN, LogisticRegression, or CNN\n\n# Define CNN model\nclass FashionCNN(nn.Module):\n    def __init__(self, filters1, filters2, kernel1, kernel2):\n        super(FashionCNN, self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=filters1, kernel_size=kernel1, padding=1),\n            nn.BatchNorm2d(filters1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(in_channels=filters1, out_channels=filters2, kernel_size=kernel2),\n            nn.BatchNorm2d(filters2),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        self.fc1 = None #initialize first fully connected layer as none, defined later in fwd\n        self.drop = nn.Dropout2d(0.25)\n        self.fc2 = nn.Linear(in_features=600, out_features=120)\n        self.fc3 = nn.Linear(in_features=120, out_features=10)\n        \n\n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        #Flatten tensor dynamically\n        out = out.view(out.size(0), -1)\n        if self.fc1 is None:\n            self.fc1 = nn.Linear(out.shape[1], 600).to(x.device)\n        out = self.fc1(out)\n        out = self.drop(out)\n        out = self.fc2(out)\n        out = self.fc3(out)\n        return out\n\n\n\n# Define Optuna objective function\ndef objective(trial):\n      # Set MLflow experiment name\n    if CLASSIFIER == \"LogisticRegression\":\n        experiment = mlflow.set_experiment(\"new-pytorch-fmnist-lr-noRBM\")\n    elif CLASSIFIER == \"FNN\":\n        experiment = mlflow.set_experiment(\"new-pytorch-fmnist-fnn-noRBM\")\n    elif CLASSIFIER == \"CNN\":\n        experiment = mlflow.set_experiment(\"new-pytorch-fmnist-cnn-noRBM\")\n    batch_size = trial.suggest_int(\"batch_size\", 64, 256, step=32)\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n    mlflow.start_run(experiment_id=experiment.experiment_id)\n    num_classifier_epochs = trial.suggest_int(\"num_classifier_epochs\", 5, 5) \n    mlflow.log_param(\"num_classifier_epochs\", num_classifier_epochs)\n\n    if CLASSIFIER == \"FNN\":\n        hidden_size = trial.suggest_int(\"fnn_hidden\", 192, 384)\n        learning_rate = trial.suggest_float(\"learning_rate\", 0.0001, 0.0025)\n\n        mlflow.log_param(\"classifier\", \"FNN\")\n        mlflow.log_param(\"fnn_hidden\", hidden_size)\n        mlflow.log_param(\"learning_rate\", learning_rate)\n\n        model = nn.Sequential(\n            nn.Linear(784, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, 10)\n        ).to(device)\n\n\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n    elif CLASSIFIER == \"CNN\":\n        filters1 = trial.suggest_int(\"filters1\", 16, 64, step=16)\n        filters2 = trial.suggest_int(\"filters2\", 32, 128, step=32)\n        kernel1 = trial.suggest_int(\"kernel1\", 3, 5)\n        kernel2 = trial.suggest_int(\"kernel2\", 3, 5)\n        learning_rate = trial.suggest_float(\"learning_rate\", 0.0001, 0.0025)\n\n        mlflow.log_param(\"classifier\", \"CNN\")\n        mlflow.log_param(\"filters1\", filters1)\n        mlflow.log_param(\"filters2\", filters2)\n        mlflow.log_param(\"kernel1\", kernel1)\n        mlflow.log_param(\"kernel2\", kernel2)\n        mlflow.log_param(\"learning_rate\", learning_rate)\n\n        model = FashionCNN(filters1, filters2, kernel1, kernel2).to(device)\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n      \n    elif CLASSIFIER == \"LogisticRegression\":\n        mlflow.log_param(\"classifier\", \"LogisticRegression\")\n    \n        # Prepare data for Logistic Regression (Flatten 28x28 images to 784 features)\n        train_features = train_dataset.data.view(-1, 784).numpy()\n        train_labels = train_dataset.targets.numpy()\n        test_features = test_dataset.data.view(-1, 784).numpy()\n        test_labels = test_dataset.targets.numpy()\n    \n        # Normalize the pixel values to [0,1] for better convergence\n        train_features = train_features / 255.0\n        test_features = test_features / 255.0\n    \n    \n        C = trial.suggest_float(\"C\", 0.01, 10.0, log=True)  \n        solver = \"saga\" \n    \n        model = LogisticRegression(C=C, max_iter=num_classifier_epochs, solver=solver)\n        model.fit(train_features, train_labels)\n    \n    \n        predictions = model.predict(test_features)\n        accuracy = accuracy_score(test_labels, predictions) * 100\n        \n        macro_f1 = f1_score(test_labels, predictions, average=\"macro\") #for f1\n        print(f\"Logistic Regression Test Accuracy: {accuracy:.2f}%\")\n        print(f\"Macro F1 Score: {macro_f1:.4f}\") #for f1\n    \n        mlflow.log_param(\"C\", C)\n        mlflow.log_metric(\"test_accuracy\", accuracy)\n        mlflow.log_metric(\"macro_f1\", macro_f1) #for f1\n        mlflow.end_run()\n        return accuracy\n\n    # Training Loop for FNN and CNN\n    criterion = nn.CrossEntropyLoss()\n\n\n    model.train()\n    for epoch in range(num_classifier_epochs):\n        running_loss = 0.0\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images) if CLASSIFIER == \"CNN\" else model(images.view(images.size(0), -1))\n\n            optimizer.zero_grad()\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n\n        print(f\"{CLASSIFIER} Epoch {epoch+1}: loss = {running_loss / len(train_loader):.4f}\")\n\n    # Model Evaluation\n    model.eval()\n    correct, total = 0, 0\n    all_preds = []   # for f1\n    all_labels = [] \n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images) if CLASSIFIER == \"CNN\" else model(images.view(images.size(0), -1))\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            all_preds.extend(predicted.cpu().numpy())   #for f1\n            all_labels.extend(labels.cpu().numpy()) #for f1\n\n    accuracy = 100 * correct / total\n    macro_f1 = f1_score(all_labels, all_preds, average=\"macro\") #for f1\n    print(f\"Test Accuracy: {accuracy:.2f}%\")\n    print(f\"Macro F1 Score: {macro_f1:.4f}\") #for f1\n\n    mlflow.log_metric(\"test_accuracy\", accuracy)\n    mlflow.log_metric(\"macro_f1\", macro_f1) #for f1\n    mlflow.end_run()\n    return accuracy\n\nif __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=1) # n_trials set to 1 for quick rendering\n    print(f\"Best Parameters for {CLASSIFIER}:\", study.best_params)\n    print(\"Best Accuracy:\", study.best_value)\n\nFNN Epoch 1: loss = 0.6057\nFNN Epoch 2: loss = 0.4210\nFNN Epoch 3: loss = 0.3781\nFNN Epoch 4: loss = 0.3508\nFNN Epoch 5: loss = 0.3311\n\n\nTest Accuracy: 86.62%\nMacro F1 Score: 0.8638\nBest Parameters for FNN: {'batch_size': 128, 'num_classifier_epochs': 5, 'fnn_hidden': 341, 'learning_rate': 0.0005327829115089294}\nBest Accuracy: 86.62"
  },
  {
    "objectID": "slides.html#model-3-convolutional-neural-network-on-fashion-mnist-data",
    "href": "slides.html#model-3-convolutional-neural-network-on-fashion-mnist-data",
    "title": "Restricted Boltzmann Machines",
    "section": "Model 3: Convolutional Neural Network on Fashion MNIST Data",
    "text": "Model 3: Convolutional Neural Network on Fashion MNIST Data\nModel 3: Convolutional Neural Network on Fashion MNIST Data Base code for CNN structure borrowed from Kaggle\n\n\nClick to Show Code and Output\n\n\nfrom sklearn.metrics import f1_score\n\nCLASSIFIER = \"CNN\"  # Change for FNN, LogisticRegression, or CNN\n\n# Define CNN model\nclass FashionCNN(nn.Module):\n    def __init__(self, filters1, filters2, kernel1, kernel2):\n        super(FashionCNN, self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=filters1, kernel_size=kernel1, padding=1),\n            nn.BatchNorm2d(filters1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(in_channels=filters1, out_channels=filters2, kernel_size=kernel2),\n            nn.BatchNorm2d(filters2),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        self.fc1 = None #initialize first fully connected layer as none, defined later in fwd\n        self.drop = nn.Dropout2d(0.25)\n        self.fc2 = nn.Linear(in_features=600, out_features=120)\n        self.fc3 = nn.Linear(in_features=120, out_features=10)\n        \n\n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        #Flatten tensor dynamically\n        out = out.view(out.size(0), -1)\n        if self.fc1 is None:\n            self.fc1 = nn.Linear(out.shape[1], 600).to(x.device)\n        out = self.fc1(out)\n        out = self.drop(out)\n        out = self.fc2(out)\n        out = self.fc3(out)\n        return out\n\n\n\n# Define Optuna objective function\ndef objective(trial):\n        # Set MLflow experiment name\n    if CLASSIFIER == \"LogisticRegression\":\n        experiment = mlflow.set_experiment(\"new-pytorch-fmnist-lr-noRBM\")\n    elif CLASSIFIER == \"FNN\":\n        experiment = mlflow.set_experiment(\"new-pytorch-fmnist-fnn-noRBM\")\n    elif CLASSIFIER == \"CNN\":\n        experiment = mlflow.set_experiment(\"new-pytorch-fmnist-cnn-noRBM\")\n    batch_size = trial.suggest_int(\"batch_size\", 64, 256, step=32)\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n    mlflow.start_run(experiment_id=experiment.experiment_id)\n    num_classifier_epochs = trial.suggest_int(\"num_classifier_epochs\", 5, 5) \n    mlflow.log_param(\"num_classifier_epochs\", num_classifier_epochs)\n\n    if CLASSIFIER == \"FNN\":\n        hidden_size = trial.suggest_int(\"fnn_hidden\", 192, 384)\n        learning_rate = trial.suggest_float(\"learning_rate\", 0.0001, 0.0025)\n\n        mlflow.log_param(\"classifier\", \"FNN\")\n        mlflow.log_param(\"fnn_hidden\", hidden_size)\n        mlflow.log_param(\"learning_rate\", learning_rate)\n\n        model = nn.Sequential(\n            nn.Linear(784, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, 10)\n        ).to(device)\n\n\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n    elif CLASSIFIER == \"CNN\":\n        filters1 = trial.suggest_int(\"filters1\", 16, 64, step=16)\n        filters2 = trial.suggest_int(\"filters2\", 32, 128, step=32)\n        kernel1 = trial.suggest_int(\"kernel1\", 3, 5)\n        kernel2 = trial.suggest_int(\"kernel2\", 3, 5)\n        learning_rate = trial.suggest_float(\"learning_rate\", 0.0001, 0.0025)\n\n        mlflow.log_param(\"classifier\", \"CNN\")\n        mlflow.log_param(\"filters1\", filters1)\n        mlflow.log_param(\"filters2\", filters2)\n        mlflow.log_param(\"kernel1\", kernel1)\n        mlflow.log_param(\"kernel2\", kernel2)\n        mlflow.log_param(\"learning_rate\", learning_rate)\n\n        model = FashionCNN(filters1, filters2, kernel1, kernel2).to(device)\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n      \n    elif CLASSIFIER == \"LogisticRegression\":\n        mlflow.log_param(\"classifier\", \"LogisticRegression\")\n    \n        # Prepare data for Logistic Regression (Flatten 28x28 images to 784 features)\n        train_features = train_dataset.data.view(-1, 784).numpy()\n        train_labels = train_dataset.targets.numpy()\n        test_features = test_dataset.data.view(-1, 784).numpy()\n        test_labels = test_dataset.targets.numpy()\n    \n        # Normalize the pixel values to [0,1] for better convergence\n        train_features = train_features / 255.0\n        test_features = test_features / 255.0\n    \n    \n        C = trial.suggest_float(\"C\", 0.01, 10.0, log=True)  \n        solver = \"saga\" \n    \n        model = LogisticRegression(C=C, max_iter=num_classifier_epochs, solver=solver)\n        model.fit(train_features, train_labels)\n    \n    \n        predictions = model.predict(test_features)\n        accuracy = accuracy_score(test_labels, predictions) * 100\n        \n        macro_f1 = f1_score(test_labels, predictions, average=\"macro\") #for f1\n        print(f\"Logistic Regression Test Accuracy: {accuracy:.2f}%\")\n        print(f\"Macro F1 Score: {macro_f1:.4f}\") #for f1\n    \n        mlflow.log_param(\"C\", C)\n        mlflow.log_metric(\"test_accuracy\", accuracy)\n        mlflow.log_metric(\"macro_f1\", macro_f1) #for f1\n        mlflow.end_run()\n        return accuracy\n\n    # Training Loop for FNN and CNN\n    criterion = nn.CrossEntropyLoss()\n\n\n    model.train()\n    for epoch in range(num_classifier_epochs):\n        running_loss = 0.0\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images) if CLASSIFIER == \"CNN\" else model(images.view(images.size(0), -1))\n\n            optimizer.zero_grad()\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n\n        print(f\"{CLASSIFIER} Epoch {epoch+1}: loss = {running_loss / len(train_loader):.4f}\")\n\n    # Model Evaluation\n    model.eval()\n    correct, total = 0, 0\n    all_preds = []   # for f1\n    all_labels = [] \n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images) if CLASSIFIER == \"CNN\" else model(images.view(images.size(0), -1))\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            all_preds.extend(predicted.cpu().numpy())   #for f1\n            all_labels.extend(labels.cpu().numpy()) #for f1\n\n    accuracy = 100 * correct / total\n    macro_f1 = f1_score(all_labels, all_preds, average=\"macro\") #for f1\n    print(f\"Test Accuracy: {accuracy:.2f}%\")\n    print(f\"Macro F1 Score: {macro_f1:.4f}\") #for f1\n\n    mlflow.log_metric(\"test_accuracy\", accuracy)\n    mlflow.log_metric(\"macro_f1\", macro_f1) #for f1\n    mlflow.end_run()\n    return accuracy\n\nif __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=1) # n_trials set to 1 for quick rendering\n    print(f\"Best Parameters for {CLASSIFIER}:\", study.best_params)\n    print(\"Best Accuracy:\", study.best_value)\n\nCNN Epoch 1: loss = 0.5825\nCNN Epoch 2: loss = 0.3798\nCNN Epoch 3: loss = 0.3430\nCNN Epoch 4: loss = 0.3238\nCNN Epoch 5: loss = 0.3070\n\n\nTest Accuracy: 88.55%\nMacro F1 Score: 0.8852\nBest Parameters for CNN: {'batch_size': 64, 'num_classifier_epochs': 5, 'filters1': 16, 'filters2': 128, 'kernel1': 4, 'kernel2': 3, 'learning_rate': 0.000206307373969411}\nBest Accuracy: 88.55"
  },
  {
    "objectID": "slides.html#model-4-logistic-regression-on-rbm-hidden-features-of-fashion-mnist",
    "href": "slides.html#model-4-logistic-regression-on-rbm-hidden-features-of-fashion-mnist",
    "title": "Restricted Boltzmann Machines",
    "section": "Model 4: Logistic Regression on RBM Hidden Features (of Fashion MNIST",
    "text": "Model 4: Logistic Regression on RBM Hidden Features (of Fashion MNIST\nData) Model 4: Logistic Regression on RBM Hidden Features (of Fashion MNIST Data)\n\n\nClick to Show Code and Output\n\n\nfrom sklearn.metrics import accuracy_score, f1_score\nCLASSIFIER = 'LogisticRegression'\n\nif CLASSIFIER == 'LogisticRegression':\n    experiment = mlflow.set_experiment(\"new-pytorch-fmnist-lr-withrbm\")\nelse:\n    experiment = mlflow.set_experiment(\"new-pytorch-fmnist-fnn-withrbm\")\n\n\nclass RBM(nn.Module):\n    def __init__(self, n_visible=784, n_hidden=256, k=1):\n        super(RBM, self).__init__()\n        self.n_visible = n_visible\n        self.n_hidden = n_hidden\n        # Initialize weights and biases\n        self.W = nn.Parameter(torch.randn(n_hidden, n_visible) * 0.1)\n        self.v_bias = nn.Parameter(torch.zeros(n_visible))\n        self.h_bias = nn.Parameter(torch.zeros(n_hidden))\n        self.k = k  # CD-k steps\n\n    def sample_h(self, v):\n        # Given visible v, sample hidden h\n        p_h = torch.sigmoid(F.linear(v, self.W, self.h_bias))  # p(h=1|v)\n        h_sample = torch.bernoulli(p_h)                        # sample Bernoulli\n        return p_h, h_sample\n\n    def sample_v(self, h):\n        # Given hidden h, sample visible v\n        p_v = torch.sigmoid(F.linear(h, self.W.t(), self.v_bias))  # p(v=1|h)\n        v_sample = torch.bernoulli(p_v)\n        return p_v, v_sample\n\n    def forward(self, v):\n        # Perform k steps of contrastive divergence starting from v\n        v_k = v.clone()\n        for _ in range(self.k):\n            _, h_k = self.sample_h(v_k)    # sample hidden from current visible\n            _, v_k = self.sample_v(h_k)    # sample visible from hidden\n        return v_k  # k-step reconstructed visible\n\n    def free_energy(self, v):\n        # Compute the visible bias term for each sample in the batch\n        vbias_term = (v * self.v_bias).sum(dim=1)  # shape: [batch_size]\n        # Compute the activation of the hidden units\n        wx_b = F.linear(v, self.W, self.h_bias)     # shape: [batch_size, n_hidden]\n        # Compute the hidden term\n        hidden_term = torch.sum(torch.log1p(torch.exp(wx_b)), dim=1)  # shape: [batch_size]\n        # Return the mean free energy over the batch\n        return - (vbias_term + hidden_term).mean()\n    \ntransform = transforms.Compose([transforms.ToTensor()])\ntrain_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\ntest_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n\ndef objective(trial):\n    num_rbm_epochs = trial.suggest_int(\"num_rbm_epochs\", 5, 5)# 24, 33)\n    batch_size = trial.suggest_int(\"batch_size\", 192, 1024)\n    rbm_lr = trial.suggest_float(\"rbm_lr\", 0.05, 0.1)\n    rbm_hidden = trial.suggest_int(\"rbm_hidden\", 384, 8192)\n\n    mlflow.start_run(experiment_id=experiment.experiment_id)\n    if CLASSIFIER != 'LogisticRegression':\n        fnn_hidden = trial.suggest_int(\"fnn_hidden\", 192, 384)\n        fnn_lr = trial.suggest_float(\"fnn_lr\", 0.0001, 0.0025)\n        mlflow.log_param(\"fnn_hidden\", fnn_hidden)\n        mlflow.log_param(\"fnn_lr\", fnn_lr)\n\n    num_classifier_epochs = trial.suggest_int(\"num_classifier_epochs\", 5, 5)# 40, 60)\n\n    mlflow.log_param(\"num_rbm_epochs\", num_rbm_epochs)\n    mlflow.log_param(\"batch_size\", batch_size)\n    mlflow.log_param(\"rbm_lr\", rbm_lr)\n    mlflow.log_param(\"rbm_hidden\", rbm_hidden)\n    mlflow.log_param(\"num_classifier_epochs\", num_classifier_epochs)\n\n    # Instantiate RBM and optimizer\n    device = torch.device(\"mps\")\n    rbm = RBM(n_visible=784, n_hidden=rbm_hidden, k=1).to(device)\n    optimizer = torch.optim.SGD(rbm.parameters(), lr=rbm_lr)\n\n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n    rbm_training_failed = False\n    # Training loop (assuming train_loader yields batches of images and labels)\n    for epoch in range(num_rbm_epochs):\n        total_loss = 0.0\n        for images, _ in train_loader:\n            # Flatten images and binarize\n            v0 = images.view(-1, 784).to(rbm.W.device)      # shape [batch_size, 784]\n            v0 = torch.bernoulli(v0)                        # sample binary input\n            vk = rbm(v0)                                    # k-step CD reconstruction\n            # Compute contrastive divergence loss (free energy difference)\n            loss = rbm.free_energy(v0) - rbm.free_energy(vk)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n        print(f\"Epoch {epoch+1}: avg free-energy loss = {total_loss/len(train_loader):.4f}\")\n        if np.isnan(total_loss):\n            rbm_training_failed = True\n            break\n\n        if rbm_training_failed:\n            accuracy = 0.0\n            macro_f1 = 0.0 \n            print(\"RBM training failed — returning 0.0 for accuracy and macro F1\")  \n            mlflow.log_metric(\"test_accuracy\", accuracy)\n            mlflow.log_metric(\"macro_f1\", macro_f1)\n            mlflow.set_tag(\"status\", \"rbm_failed\")  # Optional tag\n            mlflow.end_run()\n            return float(accuracy)\n    else:\n        rbm.eval()  # set in evaluation mode if using any layers that behave differently in training\n        features_list = []\n        labels_list = []\n        for images, labels in train_loader:\n            v = images.view(-1, 784).to(rbm.W.device)\n            v = v  # (optionally binarize or use raw normalized pixels)\n            h_prob, h_sample = rbm.sample_h(v)  # get hidden activations\n            features_list.append(h_prob.cpu().detach().numpy())\n            labels_list.append(labels.numpy())\n        train_features = np.concatenate(features_list)  # shape: [N_train, n_hidden]\n        train_labels = np.concatenate(labels_list)\n\n        # Convert pre-extracted training features and labels to tensors and create a DataLoader\n        train_features_tensor = torch.tensor(train_features, dtype=torch.float32)\n        train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n        train_feature_dataset = torch.utils.data.TensorDataset(train_features_tensor, train_labels_tensor)\n        train_feature_loader = torch.utils.data.DataLoader(train_feature_dataset, batch_size=batch_size, shuffle=True)\n\n            \n        if CLASSIFIER == 'LogisticRegression':\n            # add optuna tuning same as log reg without RBM features...\n            lr_C = trial.suggest_float(\"lr_C\", 0.01, 10.0, log=True)  \n            mlflow.log_param(\"lr_C\", lr_C)  # Log the chosen C value\n\n\n            classifier = LogisticRegression(max_iter=num_classifier_epochs, C=lr_C, solver=\"saga\") \n            classifier.fit(train_features, train_labels)            \n            \n        else:\n            classifier = nn.Sequential(\n                nn.Linear(rbm.n_hidden, fnn_hidden),\n                nn.ReLU(),\n                nn.Linear(fnn_hidden, 10)\n            )\n\n            # Move classifier to the same device as the RBM\n            classifier = classifier.to(device)\n            criterion = nn.CrossEntropyLoss()\n            classifier_optimizer = torch.optim.Adam(classifier.parameters(), lr=fnn_lr)\n\n            classifier.train()\n            for epoch in range(num_classifier_epochs):\n                running_loss = 0.0\n                for features, labels in train_feature_loader:\n                    features = features.to(device)\n                    labels = labels.to(device)\n                    \n                    # Forward pass through classifier\n                    outputs = classifier(features)\n                    loss = criterion(outputs, labels)\n                    \n                    # Backpropagation and optimization\n                    classifier_optimizer.zero_grad()\n                    loss.backward()\n                    classifier_optimizer.step()\n                    \n                    running_loss += loss.item()\n                avg_loss = running_loss / len(train_feature_loader)\n                print(f\"Classifier Epoch {epoch+1}: loss = {avg_loss:.4f}\")\n\n        # Evaluate the classifier on test data.\n        # Here we extract features from the RBM for each test image.\n        if CLASSIFIER != 'LogisticRegression':\n            classifier.eval()\n            correct = 0\n            total = 0\n        features_list = []\n        labels_list = []\n        with torch.no_grad():\n            for images, labels in test_loader:\n                v = images.view(-1, 784).to(device)\n                # Extract hidden activations; you can use either h_prob or h_sample.\n                h_prob, _ = rbm.sample_h(v)\n                if CLASSIFIER == 'LogisticRegression':\n                    features_list.append(h_prob.cpu().detach().numpy())\n                    labels_list.append(labels.numpy())\n                else:\n                    outputs = classifier(h_prob)\n                    _, predicted = torch.max(outputs.data, 1)\n                    total += labels.size(0)\n                    correct += (predicted.cpu() == labels).sum().item()\n\n        if CLASSIFIER == 'LogisticRegression':\n            test_features = np.concatenate(features_list)\n            test_labels = np.concatenate(labels_list)\n            predictions = classifier.predict(test_features)\n            accuracy = accuracy_score(test_labels, predictions) * 100\n        \n            macro_f1 = f1_score(test_labels, predictions, average=\"macro\") \n        \n        else:\n            accuracy = 100 * correct / total\n        \n            all_preds = [] \n            all_labels = [] \n            classifier.eval()\n            with torch.no_grad():\n                for images, labels in test_loader:\n                    v = images.view(-1, 784).to(device)\n                    h_prob, _ = rbm.sample_h(v)\n                    outputs = classifier(h_prob)\n                    _, predicted = torch.max(outputs.data, 1)\n                    all_preds.extend(predicted.cpu().numpy()) \n                    all_labels.extend(labels.numpy()) \n        \n            macro_f1 = f1_score(all_labels, all_preds, average=\"macro\") \n        \n        print(f\"Test Accuracy: {accuracy:.2f}%\")\n        print(f\"Macro F1 Score: {macro_f1:.4f}\") \n        \n        mlflow.log_metric(\"test_accuracy\", accuracy)\n        mlflow.log_metric(\"macro_f1\", macro_f1) \n        mlflow.end_run()\n        return float(accuracy if accuracy is not None else 0.0)\n\nif __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=1) # n_trials set to 1 for quick rendering\n    print(study.best_params)\n    print(study.best_value)\n    print(study.best_trial)\n\nEpoch 1: avg free-energy loss = 31.9494\nEpoch 2: avg free-energy loss = 17.7599\nEpoch 3: avg free-energy loss = 13.7968\nEpoch 4: avg free-energy loss = 12.1223\nEpoch 5: avg free-energy loss = 8.9156\n\n\nTest Accuracy: 86.72%\nMacro F1 Score: 0.8663\n{'num_rbm_epochs': 5, 'batch_size': 458, 'rbm_lr': 0.05226083584049011, 'rbm_hidden': 2290, 'num_classifier_epochs': 5, 'lr_C': 2.748682090445784}\n86.72\nFrozenTrial(number=0, state=1, values=[86.72], datetime_start=datetime.datetime(2025, 4, 5, 19, 6, 6, 485920), datetime_complete=datetime.datetime(2025, 4, 5, 19, 6, 41, 750946), params={'num_rbm_epochs': 5, 'batch_size': 458, 'rbm_lr': 0.05226083584049011, 'rbm_hidden': 2290, 'num_classifier_epochs': 5, 'lr_C': 2.748682090445784}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'num_rbm_epochs': IntDistribution(high=5, log=False, low=5, step=1), 'batch_size': IntDistribution(high=1024, log=False, low=192, step=1), 'rbm_lr': FloatDistribution(high=0.1, log=False, low=0.05, step=None), 'rbm_hidden': IntDistribution(high=8192, log=False, low=384, step=1), 'num_classifier_epochs': IntDistribution(high=5, log=False, low=5, step=1), 'lr_C': FloatDistribution(high=10.0, log=True, low=0.01, step=None)}, trial_id=0, value=None)"
  },
  {
    "objectID": "slides.html#model-5-feed-forward-network-on-rbm-hidden-features-of-fashion",
    "href": "slides.html#model-5-feed-forward-network-on-rbm-hidden-features-of-fashion",
    "title": "Restricted Boltzmann Machines",
    "section": "Model 5: Feed Forward Network on RBM Hidden Features (of Fashion",
    "text": "Model 5: Feed Forward Network on RBM Hidden Features (of Fashion\nMNIST Data) Model 5: Feed Forward Network on RBM Hidden Features (of Fashion MNIST Data)\n\n\nClick to Show Code and Output\n\n\nfrom sklearn.metrics import accuracy_score, f1_score\nCLASSIFIER = 'FNN'\n\nif CLASSIFIER == 'LogisticRegression':\n    experiment = mlflow.set_experiment(\"new-pytorch-fmnist-lr-withrbm\")\nelse:\n    experiment = mlflow.set_experiment(\"new-pytorch-fmnist-fnn-withrbm\")\n\n\nclass RBM(nn.Module):\n    def __init__(self, n_visible=784, n_hidden=256, k=1):\n        super(RBM, self).__init__()\n        self.n_visible = n_visible\n        self.n_hidden = n_hidden\n        # Initialize weights and biases\n        self.W = nn.Parameter(torch.randn(n_hidden, n_visible) * 0.1)\n        self.v_bias = nn.Parameter(torch.zeros(n_visible))\n        self.h_bias = nn.Parameter(torch.zeros(n_hidden))\n        self.k = k  # CD-k steps\n\n    def sample_h(self, v):\n        # Given visible v, sample hidden h\n        p_h = torch.sigmoid(F.linear(v, self.W, self.h_bias))  # p(h=1|v)\n        h_sample = torch.bernoulli(p_h)                        # sample Bernoulli\n        return p_h, h_sample\n\n    def sample_v(self, h):\n        # Given hidden h, sample visible v\n        p_v = torch.sigmoid(F.linear(h, self.W.t(), self.v_bias))  # p(v=1|h)\n        v_sample = torch.bernoulli(p_v)\n        return p_v, v_sample\n\n    def forward(self, v):\n        # Perform k steps of contrastive divergence starting from v\n        v_k = v.clone()\n        for _ in range(self.k):\n            _, h_k = self.sample_h(v_k)    # sample hidden from current visible\n            _, v_k = self.sample_v(h_k)    # sample visible from hidden\n        return v_k  # k-step reconstructed visible\n\n    def free_energy(self, v):\n        # Compute the visible bias term for each sample in the batch\n        vbias_term = (v * self.v_bias).sum(dim=1)  # shape: [batch_size]\n        # Compute the activation of the hidden units\n        wx_b = F.linear(v, self.W, self.h_bias)     # shape: [batch_size, n_hidden]\n        # Compute the hidden term\n        hidden_term = torch.sum(torch.log1p(torch.exp(wx_b)), dim=1)  # shape: [batch_size]\n        # Return the mean free energy over the batch\n        return - (vbias_term + hidden_term).mean()\n    \ntransform = transforms.Compose([transforms.ToTensor()])\ntrain_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\ntest_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n\ndef objective(trial):\n    num_rbm_epochs = trial.suggest_int(\"num_rbm_epochs\", 5, 5)# 24, 33)\n    batch_size = trial.suggest_int(\"batch_size\", 192, 1024)\n    rbm_lr = trial.suggest_float(\"rbm_lr\", 0.05, 0.1)\n    rbm_hidden = trial.suggest_int(\"rbm_hidden\", 384, 8192)\n\n    mlflow.start_run(experiment_id=experiment.experiment_id)\n    if CLASSIFIER != 'LogisticRegression':\n        fnn_hidden = trial.suggest_int(\"fnn_hidden\", 192, 384)\n        fnn_lr = trial.suggest_float(\"fnn_lr\", 0.0001, 0.0025)\n        mlflow.log_param(\"fnn_hidden\", fnn_hidden)\n        mlflow.log_param(\"fnn_lr\", fnn_lr)\n\n    num_classifier_epochs = trial.suggest_int(\"num_classifier_epochs\", 5, 5)# 40, 60)\n\n    mlflow.log_param(\"num_rbm_epochs\", num_rbm_epochs)\n    mlflow.log_param(\"batch_size\", batch_size)\n    mlflow.log_param(\"rbm_lr\", rbm_lr)\n    mlflow.log_param(\"rbm_hidden\", rbm_hidden)\n    mlflow.log_param(\"num_classifier_epochs\", num_classifier_epochs)\n\n    # Instantiate RBM and optimizer\n    device = torch.device(\"mps\")\n    rbm = RBM(n_visible=784, n_hidden=rbm_hidden, k=1).to(device)\n    optimizer = torch.optim.SGD(rbm.parameters(), lr=rbm_lr)\n\n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n    rbm_training_failed = False\n    # Training loop (assuming train_loader yields batches of images and labels)\n    for epoch in range(num_rbm_epochs):\n        total_loss = 0.0\n        for images, _ in train_loader:\n            # Flatten images and binarize\n            v0 = images.view(-1, 784).to(rbm.W.device)      # shape [batch_size, 784]\n            v0 = torch.bernoulli(v0)                        # sample binary input\n            vk = rbm(v0)                                    # k-step CD reconstruction\n            # Compute contrastive divergence loss (free energy difference)\n            loss = rbm.free_energy(v0) - rbm.free_energy(vk)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n        print(f\"Epoch {epoch+1}: avg free-energy loss = {total_loss/len(train_loader):.4f}\")\n        if np.isnan(total_loss):\n            rbm_training_failed = True\n            break\n\n        if rbm_training_failed:\n            accuracy = 0.0\n            macro_f1 = 0.0 \n            print(\"RBM training failed — returning 0.0 for accuracy and macro F1\")  \n            mlflow.log_metric(\"test_accuracy\", accuracy)\n            mlflow.log_metric(\"macro_f1\", macro_f1)\n            mlflow.set_tag(\"status\", \"rbm_failed\")  # Optional tag\n            mlflow.end_run()\n            return float(accuracy)\n    else:\n        rbm.eval()  # set in evaluation mode if using any layers that behave differently in training\n        features_list = []\n        labels_list = []\n        for images, labels in train_loader:\n            v = images.view(-1, 784).to(rbm.W.device)\n            v = v  # (optionally binarize or use raw normalized pixels)\n            h_prob, h_sample = rbm.sample_h(v)  # get hidden activations\n            features_list.append(h_prob.cpu().detach().numpy())\n            labels_list.append(labels.numpy())\n        train_features = np.concatenate(features_list)  # shape: [N_train, n_hidden]\n        train_labels = np.concatenate(labels_list)\n\n        # Convert pre-extracted training features and labels to tensors and create a DataLoader\n        train_features_tensor = torch.tensor(train_features, dtype=torch.float32)\n        train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n        train_feature_dataset = torch.utils.data.TensorDataset(train_features_tensor, train_labels_tensor)\n        train_feature_loader = torch.utils.data.DataLoader(train_feature_dataset, batch_size=batch_size, shuffle=True)\n\n            \n        if CLASSIFIER == 'LogisticRegression':\n            # add optuna tuning same as log reg without RBM features...\n            lr_C = trial.suggest_float(\"lr_C\", 0.01, 10.0, log=True)  \n            mlflow.log_param(\"lr_C\", lr_C)  # Log the chosen C value\n\n\n            classifier = LogisticRegression(max_iter=num_classifier_epochs, C=lr_C, solver=\"saga\") \n            classifier.fit(train_features, train_labels)            \n            \n        else:\n            classifier = nn.Sequential(\n                nn.Linear(rbm.n_hidden, fnn_hidden),\n                nn.ReLU(),\n                nn.Linear(fnn_hidden, 10)\n            )\n\n            # Move classifier to the same device as the RBM\n            classifier = classifier.to(device)\n            criterion = nn.CrossEntropyLoss()\n            classifier_optimizer = torch.optim.Adam(classifier.parameters(), lr=fnn_lr)\n\n            classifier.train()\n            for epoch in range(num_classifier_epochs):\n                running_loss = 0.0\n                for features, labels in train_feature_loader:\n                    features = features.to(device)\n                    labels = labels.to(device)\n                    \n                    # Forward pass through classifier\n                    outputs = classifier(features)\n                    loss = criterion(outputs, labels)\n                    \n                    # Backpropagation and optimization\n                    classifier_optimizer.zero_grad()\n                    loss.backward()\n                    classifier_optimizer.step()\n                    \n                    running_loss += loss.item()\n                avg_loss = running_loss / len(train_feature_loader)\n                print(f\"Classifier Epoch {epoch+1}: loss = {avg_loss:.4f}\")\n\n        # Evaluate the classifier on test data.\n        # Here we extract features from the RBM for each test image.\n        if CLASSIFIER != 'LogisticRegression':\n            classifier.eval()\n            correct = 0\n            total = 0\n        features_list = []\n        labels_list = []\n        with torch.no_grad():\n            for images, labels in test_loader:\n                v = images.view(-1, 784).to(device)\n                # Extract hidden activations; you can use either h_prob or h_sample.\n                h_prob, _ = rbm.sample_h(v)\n                if CLASSIFIER == 'LogisticRegression':\n                    features_list.append(h_prob.cpu().detach().numpy())\n                    labels_list.append(labels.numpy())\n                else:\n                    outputs = classifier(h_prob)\n                    _, predicted = torch.max(outputs.data, 1)\n                    total += labels.size(0)\n                    correct += (predicted.cpu() == labels).sum().item()\n\n        if CLASSIFIER == 'LogisticRegression':\n            test_features = np.concatenate(features_list)\n            test_labels = np.concatenate(labels_list)\n            predictions = classifier.predict(test_features)\n            accuracy = accuracy_score(test_labels, predictions) * 100\n        \n            macro_f1 = f1_score(test_labels, predictions, average=\"macro\") \n        \n        else:\n            accuracy = 100 * correct / total\n        \n            all_preds = [] \n            all_labels = [] \n            classifier.eval()\n            with torch.no_grad():\n                for images, labels in test_loader:\n                    v = images.view(-1, 784).to(device)\n                    h_prob, _ = rbm.sample_h(v)\n                    outputs = classifier(h_prob)\n                    _, predicted = torch.max(outputs.data, 1)\n                    all_preds.extend(predicted.cpu().numpy()) \n                    all_labels.extend(labels.numpy()) \n        \n            macro_f1 = f1_score(all_labels, all_preds, average=\"macro\") \n        \n        print(f\"Test Accuracy: {accuracy:.2f}%\")\n        print(f\"Macro F1 Score: {macro_f1:.4f}\") \n        \n        mlflow.log_metric(\"test_accuracy\", accuracy)\n        mlflow.log_metric(\"macro_f1\", macro_f1) \n        mlflow.end_run()\n        return float(accuracy if accuracy is not None else 0.0)\n\nif __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=1) # n_trials set to 1 for quick rendering\n    print(study.best_params)\n    print(study.best_value)\n    print(study.best_trial)\n\nEpoch 1: avg free-energy loss = 364.7098\nEpoch 2: avg free-energy loss = 113.8737\nEpoch 3: avg free-energy loss = 73.4600\nEpoch 4: avg free-energy loss = 59.0966\nEpoch 5: avg free-energy loss = 46.6551\nClassifier Epoch 1: loss = 0.7301\nClassifier Epoch 2: loss = 0.4593\nClassifier Epoch 3: loss = 0.4186\nClassifier Epoch 4: loss = 0.3938\nClassifier Epoch 5: loss = 0.3770\n\n\nTest Accuracy: 84.88%\nMacro F1 Score: 0.8406\n{'num_rbm_epochs': 5, 'batch_size': 854, 'rbm_lr': 0.0767551211280469, 'rbm_hidden': 7136, 'fnn_hidden': 226, 'fnn_lr': 0.0020724163661187595, 'num_classifier_epochs': 5}\n84.88\nFrozenTrial(number=0, state=1, values=[84.88], datetime_start=datetime.datetime(2025, 4, 5, 19, 6, 41, 823558), datetime_complete=datetime.datetime(2025, 4, 5, 19, 7, 16, 741349), params={'num_rbm_epochs': 5, 'batch_size': 854, 'rbm_lr': 0.0767551211280469, 'rbm_hidden': 7136, 'fnn_hidden': 226, 'fnn_lr': 0.0020724163661187595, 'num_classifier_epochs': 5}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'num_rbm_epochs': IntDistribution(high=5, log=False, low=5, step=1), 'batch_size': IntDistribution(high=1024, log=False, low=192, step=1), 'rbm_lr': FloatDistribution(high=0.1, log=False, low=0.05, step=None), 'rbm_hidden': IntDistribution(high=8192, log=False, low=384, step=1), 'fnn_hidden': IntDistribution(high=384, log=False, low=192, step=1), 'fnn_lr': FloatDistribution(high=0.0025, log=False, low=0.0001, step=None), 'num_classifier_epochs': IntDistribution(high=5, log=False, low=5, step=1)}, trial_id=0, value=None)"
  },
  {
    "objectID": "slides.html#summary-table-of-results",
    "href": "slides.html#summary-table-of-results",
    "title": "Restricted Boltzmann Machines",
    "section": "Summary Table of Results",
    "text": "Summary Table of Results\n\n\n\n\n\n\n\n\n\nModel\nOptuna Best Trial\nMLflow Test Accuracy(%)\nMacro F1 Score\n\n\n\n\nLogistic Regression\n84.71\n0.846\n\n\nFeed Forward Network\n88.06\n0.879\n\n\nConvolutional Neural Network\n91.29\n0.913\n\n\nLogistic Regression (on RBM Hidden Features)\n87.14\n0.871\n\n\nFeed Forward Network (on RBM Hidden Features)\n86.95\n0.869"
  },
  {
    "objectID": "slides.html#t-sne-visualization-of-class-separations",
    "href": "slides.html#t-sne-visualization-of-class-separations",
    "title": "Restricted Boltzmann Machines",
    "section": "t-SNE Visualization of Class Separations",
    "text": "t-SNE Visualization of Class Separations\nt-SNE Visualization t-distributed Stochastic Neighbor Embedding (t-SNE) is used here to visualize the separation between classes in a high-dimensional dataset. Each point represents a single fashion item (e.g., T-shirt, Trouser, etc.), and the color corresponds to its true label across the 10 categories listed above.\n\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\n# Run t-SNE to reduce dimensionality\n#embeddings = TSNE(n_jobs=2).fit_transform(X)\n\ntsne = TSNE(n_jobs=-1, random_state=42)  # Use -1 to use all available cores\nembeddings = tsne.fit_transform(X) #use scikitlearn instead\n\n\n# Create scatter plot\nfigure = plt.figure(figsize=(15,7))\nplt.scatter(embeddings[:, 0], embeddings[:, 1], c=train_labels,\n            cmap=plt.cm.get_cmap(\"jet\", 10), marker='.')\nplt.colorbar(ticks=range(10))\nplt.clim(-0.5, 9.5)\nplt.title(\"t-SNE Visualization of Fashion MNIST\")\nplt.show()"
  },
  {
    "objectID": "slides.html#test-accuracy-of-logistic-regression-by-c-inverse-regularization",
    "href": "slides.html#test-accuracy-of-logistic-regression-by-c-inverse-regularization",
    "title": "Restricted Boltzmann Machines",
    "section": "Test Accuracy of Logistic Regression by C (inverse regularization",
    "text": "Test Accuracy of Logistic Regression by C (inverse regularization\nstrength)\n\n\\[\nC = \\frac{1}{\\lambda} \\quad \\text{(inverse regularization strength)}\n\\]\nLower values of C mean more regularization (higher penalties for larger weight coefficients)\nWhat the plot shows: Most optuna trials were lower values of C, so optimization favors stronger regularization. This is further evidenced by the clustering of higher accuracies for lower values of C. A possibly anomaly is seen at C=10 with fairly high accuracy; however, it’s still not higher than lower values of C."
  },
  {
    "objectID": "slides.html#test-accuracy-by-fnn-hidden-units",
    "href": "slides.html#test-accuracy-by-fnn-hidden-units",
    "title": "Restricted Boltzmann Machines",
    "section": "Test Accuracy by FNN Hidden Units",
    "text": "Test Accuracy by FNN Hidden Units\n\nWhat the plot shows: Higher values of hidden units in the feedforward network were sampled more frequently by Optuna, suggesting a preference for more complex models. However, test accuracy appears to level off between 300 and 375 hidden units, suggesting complexity reached its optimal range. Further increases in hidden units would likely not yield higher accuracy."
  },
  {
    "objectID": "slides.html#plots-for-cnn-model",
    "href": "slides.html#plots-for-cnn-model",
    "title": "Restricted Boltzmann Machines",
    "section": "Plots for CNN Model",
    "text": "Plots for CNN Model\n\nTest Accuracy Based on the Number of Filters in the First Conv2D LayerTest Accuracy Based on the Number of Filters in the Second Conv2D LayerTest Accuracy Based on Kernel Size in the First Conv2D LayerTest Accuracy Based on Kernel Size in the Second Conv2D Layer\n\n\n\nWhat the plot shows: Although the highest test accuracy was achieved with 64 filters in the first convolutional 2D layer, the number of filters alone isn’t a strong predictor of model performance. Each filter size shows high variance (accuracies are spread out for each value vertically). This, combined with the fact that accuracies are well distributed across the different filter counts, suggests other factors or hyperparameters may play a bigger role in predicting accuracy. \n\n\n\nWhat the plot shows: Like the first Conv2D layer, the number of filters doesn’t seem to be a extremely strong predictor in accuracy. However, Optuna has sampled more frequently from higher number of filters, even 128 for this second layer, suggesting higher filters performed better. However, like before, there is still high variance in accuracy for each number of filters. \n\n\n\nWhat the plot shows: Kernel size of 3 was sampled more frequently by Optuna and yielded higher accuracies than kernel sizes of 4 or 5.\n\n\n\nWhat the plot shows: Like with the first convolutional 2D layer, kernel size of 3 is highly favored by Optuna and consistently led to higher test accuracies."
  },
  {
    "objectID": "slides.html#plots-for-log-reg-on-rbm-hidden-features",
    "href": "slides.html#plots-for-log-reg-on-rbm-hidden-features",
    "title": "Restricted Boltzmann Machines",
    "section": "Plots for Log Reg on RBM Hidden Features",
    "text": "Plots for Log Reg on RBM Hidden Features\n\nTest Accuracy of Log Reg on RBM Hidden Features by CTest Accuracy By Number of RBM Hidden Units\n\n\n\nWhat the plot shows: When using RBM-extracted hidden features as input to logistic regression, the inverse regularization strength does not appear to be a strong predictor of test accuracy.\n\n\n\nWhat the plot shows: Optuna slightly favors higher number of hidden units in the rbm with a peak at 5340 (and similar peaks 5358, 5341, etc.). However, after 7000 units, accuracy appears to decline suggesting the optimum number of units was reached around that 5300 mark."
  },
  {
    "objectID": "slides.html#plots-for-fnn-using-rbm-hidden-units",
    "href": "slides.html#plots-for-fnn-using-rbm-hidden-units",
    "title": "Restricted Boltzmann Machines",
    "section": "Plots for FNN using RBM Hidden Units",
    "text": "Plots for FNN using RBM Hidden Units\n\nTest Accuracy by RBM Hidden UnitsTest Accuracy by FNN Hidden Units\n\n\n\nWhat the plot shows: Highest accuracies cluster between 2000 and 4000 hidden units in the RBM with an outlier at 3764 hidden units. This possibly suggests too few hidden units lacks the complexity needed to explain the data; however, too many hidden units is perhaps causing some overfitting–resulting in poor generalization of the FNN classifier that receives the RBM hidden features.\n\n\n\nWhat the plot shows: Surprisingly, the number of hidden units in the FNN does not show a strong correlation with test accuracy. All hidden units tested seem to result in similar performance. This suggests the FNN is able to learn from the RBM features sufficently, and additional neurons do not significantly improve generalization."
  },
  {
    "objectID": "slides.html#applications",
    "href": "slides.html#applications",
    "title": "Restricted Boltzmann Machines",
    "section": "Applications",
    "text": "Applications\n\nCollaborative Filtering: The RBM model was used in the Netflix Prize competition to predict user ratings for movies, outperforming the Singular Value Decomposition (SVD) method that was state-of-the-art at the time (Salakhutdinov, Mnih, and Hinton 2007)\nImage Recognition: They have been trained to recognize handwritten digits, such as the MNIST dataset (Hinton 2002)\nNetwork Traffic Anomaly Detection: In a real-life dataset where one host had normal traffic and one was infected by a bot, discriminative RBM (DRBM) was able to successfully distinguish the normal from anomalous traffic (Fiore et al. 2013)\nBrain Disorders: The RBM-GAN uses RBM features from real MRI images as inputs to the generator. Features from the discriminator are then used as inputs to a classifier (Aslan, Dogan, and Koca 2023)\nQuantum Computing: RBMs have been used to approximate the many-body quantum wavefunction, which describes the quantum state of a system of particles using variational Monte Carlo methods. (Melko et al. 2019)"
  },
  {
    "objectID": "slides.html#there-are-10-labels-in-total",
    "href": "slides.html#there-are-10-labels-in-total",
    "title": "Restricted Boltzmann Machines",
    "section": "There are 10 labels in total",
    "text": "There are 10 labels in total\n0 T-shirt/top 1 Trouser 2 Pullover 3 Dress 4 Coat 5 Sandal 6 Shirt 7 Sneaker 8 Bag 9 Ankle boot\nLoad Dataset\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nimport torch\nimport torchvision.datasets\nimport torchvision.models\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\n\n\n\ntrain_data = torchvision.datasets.FashionMNIST(\n    root=\"./data\", \n    train=True, \n    download=True, \n    transform=transforms.ToTensor()  # Converts to tensor but does NOT normalize\n)\n\ntest_data = torchvision.datasets.FashionMNIST(\n    root=\"./data\", \n    train=False, \n    download=True, \n    transform=transforms.ToTensor()  \n)"
  },
  {
    "objectID": "slides.html#load-dataset",
    "href": "slides.html#load-dataset",
    "title": "Restricted Boltzmann Machines",
    "section": "Load Dataset",
    "text": "Load Dataset\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nimport torch\nimport torchvision.datasets\nimport torchvision.models\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\n\n\n\ntrain_data = torchvision.datasets.FashionMNIST(\n    root=\"./data\", \n    train=True, \n    download=True, \n    transform=transforms.ToTensor()  # Converts to tensor but does NOT normalize\n)\n\ntest_data = torchvision.datasets.FashionMNIST(\n    root=\"./data\", \n    train=False, \n    download=True, \n    transform=transforms.ToTensor()  \n)"
  },
  {
    "objectID": "slides.html#get-the-seventh-image-to-show-a-sample",
    "href": "slides.html#get-the-seventh-image-to-show-a-sample",
    "title": "Restricted Boltzmann Machines",
    "section": "Get the seventh image to show a sample",
    "text": "Get the seventh image to show a sample\nCode below\n\n# Extract the first image (or choose any index)\nimage_tensor, label = train_data[6]  # shape: [1, 28, 28]\n\n# Convert tensor to NumPy array\nimage_array = image_tensor.numpy().squeeze()  \n\n# Plot the image\nplt.figure(figsize=(5,5))\nplt.imshow(image_array, cmap=\"gray\")\nplt.title(f\"FashionMNIST Image (Label: {label})\")\nplt.axis(\"off\")  # Hide axes\nplt.show()\n\n\n\n\n\n\n\n\n\ntrain_images = train_data.data.numpy()  # Raw pixel values (0-255)\ntrain_labels = train_data.targets.numpy()\nX = train_images.reshape(-1, 784)  # Flatten 28x28 images into 1D (60000, 784)\n\n\n#print(train_images[:5])\nflattened = train_images[:5].reshape(5, -1) \n\n# Create a DataFrame\ndf_flat = pd.DataFrame(flattened)\nprint(df_flat.head())\n#train_df.info() #datatypes are integers\n\n   0    1    2    3    4    5    6    7    8    9    ...  774  775  776  777  \\\n0    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n1    0    0    0    0    0    1    0    0    0    0  ...  119  114  130   76   \n2    0    0    0    0    0    0    0    0    0   22  ...    0    0    1    0   \n3    0    0    0    0    0    0    0    0   33   96  ...    0    0    0    0   \n4    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n\n   778  779  780  781  782  783  \n0    0    0    0    0    0    0  \n1    0    0    0    0    0    0  \n2    0    0    0    0    0    0  \n3    0    0    0    0    0    0  \n4    0    0    0    0    0    0  \n\n[5 rows x 784 columns]"
  },
  {
    "objectID": "slides.html#check-for-missing-values",
    "href": "slides.html#check-for-missing-values",
    "title": "Restricted Boltzmann Machines",
    "section": "Check for Missing Values",
    "text": "Check for Missing Values\nThere are no missing values in the data.\n\nprint(np.isnan(train_images).any()) \n\nFalse"
  },
  {
    "objectID": "slides.html#check-for-class-imbalance",
    "href": "slides.html#check-for-class-imbalance",
    "title": "Restricted Boltzmann Machines",
    "section": "Check for Class Imbalance",
    "text": "Check for Class Imbalance\nThere appears to be no class imbalance\n\nunique_labels, counts = np.unique(train_labels, return_counts=True)\n\n# Print the counts sorted by label\nfor label, count in zip(unique_labels, counts):\n    print(f\"Label {label}: {count}\")\n\nLabel 0: 6000\nLabel 1: 6000\nLabel 2: 6000\nLabel 3: 6000\nLabel 4: 6000\nLabel 5: 6000\nLabel 6: 6000\nLabel 7: 6000\nLabel 8: 6000\nLabel 9: 6000\n\n\n\nprint(f\"X shape: {X.shape}\")\n\nX shape: (60000, 784)"
  },
  {
    "objectID": "slides.html#what-the-visualization-shows",
    "href": "slides.html#what-the-visualization-shows",
    "title": "Restricted Boltzmann Machines",
    "section": "What the visualization shows",
    "text": "What the visualization shows\nClass 1 (blue / Trousers) forms a clearly distinct and tightly packed cluster, indicating that the pixel patterns for trousers are less similar to those of other classes. In contrast, Classes 4 (Coat), 6 (Shirt), and 2 (Pullover) show significant overlap, suggesting that these clothing items are harder to distinguish visually and may lead to more confusion during classification."
  },
  {
    "objectID": "slides.html#library-importing-and-re-loading-fresh-dataset",
    "href": "slides.html#library-importing-and-re-loading-fresh-dataset",
    "title": "Restricted Boltzmann Machines",
    "section": "Library Importing and Re-loading Fresh Dataset",
    "text": "Library Importing and Re-loading Fresh Dataset\nImport Libraries and Re-load data for first 3 models\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nimport numpy as np\nimport mlflow\nimport optuna\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom torch.utils.data import DataLoader\n\n# Set device\ndevice = torch.device(\"mps\")\n\n# Load Fashion-MNIST dataset again for the first 3 models\ntransform = transforms.Compose([transforms.ToTensor()])\ntrain_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\ntest_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n\n\n#mlflow.end_run()\n#run this in terminal when need to fully clean out expierment after you delete it in the ui\n#rm -rf mlruns/.trash/*"
  },
  {
    "objectID": "slides.html#test-accuracy-of-logistic-regression-by-c-inverse-regularization-strength",
    "href": "slides.html#test-accuracy-of-logistic-regression-by-c-inverse-regularization-strength",
    "title": "Restricted Boltzmann Machines",
    "section": "Test Accuracy of Logistic Regression by C (inverse regularization strength)",
    "text": "Test Accuracy of Logistic Regression by C (inverse regularization strength)\n\n\\[\nC = \\frac{1}{\\lambda} \\quad \\text{(inverse regularization strength)}\n\\]\nLower values of C mean more regularization (higher penalties for larger weight coefficients)\nWhat the plot shows: Most optuna trials were lower values of C, so optimization favors stronger regularization. This is further evidenced by the clustering of higher accuracies for lower values of C. A possibly anomaly is seen at C=10 with fairly high accuracy; however, it’s still not higher than lower values of C."
  },
  {
    "objectID": "slides.html#test-accuracy-of-logistic-regression-by-c",
    "href": "slides.html#test-accuracy-of-logistic-regression-by-c",
    "title": "Restricted Boltzmann Machines",
    "section": "Test Accuracy of Logistic Regression by C",
    "text": "Test Accuracy of Logistic Regression by C\n\n\\[\nC = \\frac{1}{\\lambda} \\quad \\text{(inverse regularization strength)}\n\\]\nLower values of C mean more regularization (higher penalties for larger weight coefficients)\nWhat the plot shows: Most optuna trials were lower values of C, so optimization favors stronger regularization. This is further evidenced by the clustering of higher accuracies for lower values of C. A possibly anomaly is seen at C=10 with fairly high accuracy; however, it’s still not higher than lower values of C."
  },
  {
    "objectID": "slides.html#accuracy",
    "href": "slides.html#accuracy",
    "title": "Restricted Boltzmann Machines",
    "section": "Accuracy",
    "text": "Accuracy\n1. Accuracy Accuracy is defined as the number of correct classifications divided by the total number of classifications\n\\[\n\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}\n\\]"
  },
  {
    "objectID": "slides.html#f1",
    "href": "slides.html#f1",
    "title": "Restricted Boltzmann Machines",
    "section": "F1",
    "text": "F1\n2. Macro F1 Score Macro F1 score is the unweighted average of the individual F1 scores of each class. It takes no regard for class imbalance; however, we saw earlier the classes are all balanced in Fashion MNIST. The F1 score for each individual class is as follows\n\\[\n\\text{F1} = \\frac{2 \\cdot \\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n\\]\nwhere precision for each class is\n\\[\n\\text{Precision} = \\frac{TP}{TP + FP}\n\\]\nand recall for each class is\n\\[\n\\text{Recall} = \\frac{TP}{TP + FN}\n\\]\nThe definitions of these terms for multiclass problems are more complicated than binary and are best displayed as examples. \n\n\n\n\n\n\n\n\nAcronymn\nExample for a trouser image\n\n\n\n\nTP = True Positives\nthe image is a trouser and the model predicts a trouser\n\n\nTN = True Negatives\nthe image is not a trouser and the model predicts anything but trouser\n\n\nFP = False Positives\nthe image is anything but trouser but the model predicts trouser\n\n\nFN = False Negatives\nthe image is a trouser and the model predicts another class (like shirt)\n\n\n\nAs stated earlier, the individual F1 scores for each class are taken and averaged to compute the Macro F1 score in a multiclass problem like Fashion MNIST."
  },
  {
    "objectID": "slides.html#our-goal",
    "href": "slides.html#our-goal",
    "title": "Restricted Boltzmann Machines",
    "section": "Our Goal",
    "text": "Our Goal\nWe are classifying Fashion MNIST images into one of 10 categories. To evaluate performance, we’re comparing five different models — some trained on raw pixel values and others using features extracted by a Restricted Boltzmann Machine (RBM). Our objective is to assess whether incorporating RBM into the workflow improves classification accuracy compared to using raw image data alone.\nOur Models 1. Logistic Regression on Fashion MNIST Data 2. Feed Forward Network on Fashion MNIST Data 3. Convolutional Neural Network on Fashion MNIST Data 4. Logistic Regression on RBM Hidden Features (of Fashion MNIST Data) 5. Feed Forward Network on RBM Hidden Features (of Fashion MNIST Data)\nNote: Outputs (50 trials) and Code follow for each model and the code and output can be toggled by the reader. • Clicking “Show Code and Output” will show the code. • Clicking again will switch from output to the actual output. • Clicking “Show Code and Output” again will collapse the chunk.\nImport Libraries and Re-load data for first 3 models\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nimport numpy as np\nimport mlflow\nimport optuna\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom torch.utils.data import DataLoader\n\n# Set device\ndevice = torch.device(\"mps\")\n\n# Load Fashion-MNIST dataset again for the first 3 models\ntransform = transforms.Compose([transforms.ToTensor()])\ntrain_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\ntest_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n\n\n#mlflow.end_run()\n#run this in terminal when need to fully clean out expierment after you delete it in the ui\n#rm -rf mlruns/.trash/*"
  },
  {
    "objectID": "slides.html#import-libraries-and-re-load-data-for-first-3-models",
    "href": "slides.html#import-libraries-and-re-load-data-for-first-3-models",
    "title": "Restricted Boltzmann Machines",
    "section": "Import Libraries and Re-load data for first 3 models",
    "text": "Import Libraries and Re-load data for first 3 models\nCode below\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nimport numpy as np\nimport mlflow\nimport optuna\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom torch.utils.data import DataLoader\n\n# Set device\ndevice = torch.device(\"mps\")\n\n# Load Fashion-MNIST dataset again for the first 3 models\ntransform = transforms.Compose([transforms.ToTensor()])\ntrain_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\ntest_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n\n\n#mlflow.end_run()\n#run this in terminal when need to fully clean out expierment after you delete it in the ui\n#rm -rf mlruns/.trash/*"
  },
  {
    "objectID": "slides.html#more-on-restricted-boltzmann-machines",
    "href": "slides.html#more-on-restricted-boltzmann-machines",
    "title": "Restricted Boltzmann Machines",
    "section": "More on Restricted Boltzmann Machines",
    "text": "More on Restricted Boltzmann Machines\nGoodfellow, et al. discuss the expense in drawing samples for most undirected graphical models; however, the RBM allows for block Gibbs sampling (p. 578) where the network alternates between sampling all hidden units simultaneously (etc. for visible). Derivatives are also simplified by the fact that the energy function of the RBM is a linear function of it’s parameters, which will be seen further in Methods.\nRBMs are trained using a process called Contrastive Divergence (CD) (Hinton 2002) where the weights are updated to minimize the difference between samples from the data and samples from the model. Learning rate, batch size, and number of hidden units are all hyperparameters that can affect the ability of the training to converge successfully and learn the underlying structure of the data."
  },
  {
    "objectID": "slides.html#background-on-models-for-classification-task",
    "href": "slides.html#background-on-models-for-classification-task",
    "title": "Restricted Boltzmann Machines",
    "section": "Background on Models for Classification Task",
    "text": "Background on Models for Classification Task\nWe train Logistic Regression (with and without RBM features as input), Feed Forward Network (with and without RBM features as input), and Convolutional Neural Network.\nFor the models incoroporating the RBM, we take the Fashion MNIST features/pixels and train the RBM (unsupervised learning) to extract hidden features from the visible layer and then feed these features into either logistic regression or feed forward network. We then use the trained model to predict labels for the test data, evaluating how well the RBM-derived features perform in a supervised classification task.\n1. Logistic Regression\nMathematically, the concept behind binary logistic regression is the logit (the natural logarithm of an odds ratio)(Peng, Lee, and Ingersoll 2002). However, since we have 10 labels, our classification task falls into “Multinomial Logistic Regression.”\n\\[\nP(Y = k | X) = \\frac{e^{\\beta_{0k} + \\beta_k^T X}}{\\sum_{l=1}^{K} e^{\\beta_{0l} + \\beta_l^T X}}\n\\qquad(2)\\]\n2. Simple Feed Forward Neural Network\nThe feed forward network (FNN) is one where information flows in one direction from input to output with no loops or feedback. There can be zero hidden layers in between (called single FNN) or one or more hidden layers (multilayer FNN).  (Sazlı 2006) \n\n(Figure 2)\n\n3. Convolutional Neural Network\nThe convolutional neural network (CNN) is a type of feed forward network except that unlike the traditional ANN, CNNs are primarily used for pattern recognition with images (O’Shea and Nash 2015). The CNN has 3 layers which are stacked to form the full CNN: convolutional, pooling, and fully-connected layers. \n\n(Figure 3)"
  },
  {
    "objectID": "slides.html#creating-the-rbm",
    "href": "slides.html#creating-the-rbm",
    "title": "Restricted Boltzmann Machines",
    "section": "Creating the RBM",
    "text": "Creating the RBM\nBelow is our Process for creating the RBM:\nStep 1: We first initialize the RBM with random weights and biases and set visible units to 784 and hidden units to 256. We also set the number of contrastive divergence steps (k) to 1.  Step 2: Sample hidden units from visible. The math behind computing the hidden unit activations from the given input can be seen in Equation 3 (Fischer and Igel 2012) where the probability is used to sample from the Bernoulli distribution.  \\[\np(H_i = 1 | \\mathbf{v}) = \\sigma \\left( \\sum_{j=1}^{m} w_{ij} v_j + c_i \\right)\n\\qquad(3)\\]\nwhere p(.) is the probability of the ith hidden state being activated (=1) given the visible input vector. σ is the sigmoid activation function (below) which maps the weighted sum to a probability between 0 and 1. m is the number of visible units. wij is the weight connecting visible unit j to hidden unit i. vj is the value of the jth visible unit. and ci is the bias term for the hidden unit. \\[\n\\sigma(x) = \\frac{1}{1 + e^{-x}}\n\\qquad(4)\\]\nStep 3: Sample visible units from hidden. The math behind computing visible unit activations from the hidden layer can be seen in Equation 5 (Fischer and Igel 2012) Visible states are sampled using the Bernoulli distribution. This way we can see how well the RBM learned from the inputs.  \\[\np(V_j = 1 | \\mathbf{h}) = \\sigma \\left( \\sum_{i=1}^{n} w_{ij} h_i + b_j \\right)\n\\qquad(5)\\]\nwhere p(.) is the probability of the ith visible unit being activated (=1) given the hidden vector h. σ is same as above. n is the number of hidden units. wij is the weight connecting hidden unit i to visible unit j. bj is the bias term for the jth visible unit.\nStep 4: K=1 steps of Contrastive Divergence (Feed Forward, Feed Backward) which executes steps 2 and 3. Contrastive Divergence updates the RBM’s weights by minimizing the difference between the original input and the reconstructed input created by the RBM.  Step 5: Free energy is computed. The free energy F is given by the logarithm of the partition function Z (Oh, Baggag, and Nha 2020) where the partition function is  \\[\nZ(\\theta) \\equiv \\sum_{v,h} e^{-E(v,h; \\theta)}\n\\qquad(6)\\] and the free energy function is  \\[\nF(\\theta) = -\\ln Z(\\theta)\n\\qquad(7)\\] where lower free energy means the RBM learned the visible state well.\nStep 6: Train the RBM. Model weights updated via gradient descent. Step 7: Feature extraction for classification with LR. The hidden layer activations of the RBM are used as features for Logistic Regression and Feed Forward Network."
  },
  {
    "objectID": "slides.html#t-sne-visualization-of-classes",
    "href": "slides.html#t-sne-visualization-of-classes",
    "title": "Restricted Boltzmann Machines",
    "section": "t-SNE Visualization of Classes",
    "text": "t-SNE Visualization of Classes\nt-SNE Visualization t-distributed Stochastic Neighbor Embedding (t-SNE) is used here to visualize the separation between classes in a high-dimensional dataset. Each point represents a single fashion item (e.g., T-shirt, Trouser, etc.), and the color corresponds to its true label across the 10 categories listed above.\n\n\nClick to Show Code and Output\n\n\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\n# Run t-SNE to reduce dimensionality\n#embeddings = TSNE(n_jobs=2).fit_transform(X)\n\ntsne = TSNE(n_jobs=-1, random_state=42)  # Use -1 to use all available cores\nembeddings = tsne.fit_transform(X) #use scikitlearn instead\n\n\n# Create scatter plot\nfigure = plt.figure(figsize=(15,7))\nplt.scatter(embeddings[:, 0], embeddings[:, 1], c=train_labels,\n            cmap=plt.cm.get_cmap(\"jet\", 10), marker='.')\nplt.colorbar(ticks=range(10))\nplt.clim(-0.5, 9.5)\nplt.title(\"t-SNE Visualization of Fashion MNIST\")\nplt.show()\n\n\n\n\n\n\n(Figure 6)\n\nWhat the visualization shows:  Class 1 (blue / Trousers) forms a clearly distinct and tightly packed cluster, indicating that the pixel patterns for trousers are less similar to those of other classes. In contrast, Classes 4 (Coat), 6 (Shirt), and 2 (Pullover) show significant overlap, suggesting that these clothing items are harder to distinguish visually and may lead to more confusion during classification."
  },
  {
    "objectID": "slides.html#model-1",
    "href": "slides.html#model-1",
    "title": "Restricted Boltzmann Machines",
    "section": "Model 1",
    "text": "Model 1\nModel 1: Logistic Regression on Fashion MNIST Data\n\n\nClick to Show Code and Output\n\n\nfrom sklearn.metrics import f1_score\n\nCLASSIFIER = \"LogisticRegression\"  # Change for FNN, LogisticRegression, or CNN\n\n\n\n# Define CNN model\nclass FashionCNN(nn.Module):\n    def __init__(self, filters1, filters2, kernel1, kernel2):\n        super(FashionCNN, self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=filters1, kernel_size=kernel1, padding=1),\n            nn.BatchNorm2d(filters1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(in_channels=filters1, out_channels=filters2, kernel_size=kernel2),\n            nn.BatchNorm2d(filters2),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        self.fc1 = None #initialize first fully connected layer as none, defined later in fwd\n        self.drop = nn.Dropout2d(0.25)\n        self.fc2 = nn.Linear(in_features=600, out_features=120)\n        self.fc3 = nn.Linear(in_features=120, out_features=10)\n        \n\n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        #Flatten tensor dynamically, preserve batch size\n        out = out.view(out.size(0), -1) \n        if self.fc1 is None:\n            self.fc1 = nn.Linear(out.shape[1], 600).to(x.device)\n        out = self.fc1(out)\n        out = self.drop(out)\n        out = self.fc2(out)\n        out = self.fc3(out)\n        return out\n\n\n# Define Optuna objective function\ndef objective(trial):\n      # Set MLflow experiment name\n    if CLASSIFIER == \"LogisticRegression\":\n        experiment = mlflow.set_experiment(\"new-pytorch-fmnist-lr-noRBM\")\n    elif CLASSIFIER == \"FNN\":\n        experiment = mlflow.set_experiment(\"new-pytorch-fmnist-fnn-noRBM\")\n    elif CLASSIFIER == \"CNN\":\n        experiment = mlflow.set_experiment(\"new-pytorch-fmnist-cnn-noRBM\")\n    batch_size = trial.suggest_int(\"batch_size\", 64, 256, step=32)\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n    mlflow.start_run(experiment_id=experiment.experiment_id)\n    num_classifier_epochs = trial.suggest_int(\"num_classifier_epochs\", 5, 5) \n    mlflow.log_param(\"num_classifier_epochs\", num_classifier_epochs)\n\n    if CLASSIFIER == \"FNN\":\n        hidden_size = trial.suggest_int(\"fnn_hidden\", 192, 384)\n        learning_rate = trial.suggest_float(\"learning_rate\", 0.0001, 0.0025)\n\n        mlflow.log_param(\"classifier\", \"FNN\")\n        mlflow.log_param(\"fnn_hidden\", hidden_size)\n        mlflow.log_param(\"learning_rate\", learning_rate)\n\n        model = nn.Sequential(\n            nn.Linear(784, hidden_size), \n            nn.ReLU(),\n            nn.Linear(hidden_size, 10)\n        ).to(device)\n\n\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n    elif CLASSIFIER == \"CNN\":\n        filters1 = trial.suggest_int(\"filters1\", 16, 64, step=16)\n        filters2 = trial.suggest_int(\"filters2\", 32, 128, step=32)\n        kernel1 = trial.suggest_int(\"kernel1\", 3, 5)\n        kernel2 = trial.suggest_int(\"kernel2\", 3, 5)\n        learning_rate = trial.suggest_float(\"learning_rate\", 0.0001, 0.0025)\n\n        mlflow.log_param(\"classifier\", \"CNN\")\n        mlflow.log_param(\"filters1\", filters1)\n        mlflow.log_param(\"filters2\", filters2)\n        mlflow.log_param(\"kernel1\", kernel1)\n        mlflow.log_param(\"kernel2\", kernel2)\n        mlflow.log_param(\"learning_rate\", learning_rate)\n\n        model = FashionCNN(filters1, filters2, kernel1, kernel2).to(device)\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n      \n    elif CLASSIFIER == \"LogisticRegression\":\n        mlflow.log_param(\"classifier\", \"LogisticRegression\")\n    \n        # Prepare data for Logistic Regression (Flatten 28x28 images to 784 features)\n        train_features = train_dataset.data.view(-1, 784).numpy()\n        train_labels = train_dataset.targets.numpy()\n        test_features = test_dataset.data.view(-1, 784).numpy()\n        test_labels = test_dataset.targets.numpy()\n    \n        # Normalize the pixel values to [0,1] for better convergence\n        train_features = train_features / 255.0\n        test_features = test_features / 255.0\n    \n    \n        C = trial.suggest_float(\"C\", 0.01, 10.0, log=True)  \n        solver = \"saga\" \n    \n        model = LogisticRegression(C=C, max_iter=num_classifier_epochs, solver=solver)\n        model.fit(train_features, train_labels)\n    \n    \n        predictions = model.predict(test_features)\n        accuracy = accuracy_score(test_labels, predictions) * 100\n        \n        macro_f1 = f1_score(test_labels, predictions, average=\"macro\") #for f1\n        print(f\"Logistic Regression Test Accuracy: {accuracy:.2f}%\")\n        print(f\"Macro F1 Score: {macro_f1:.4f}\") #for f1\n    \n        mlflow.log_param(\"C\", C)\n        mlflow.log_metric(\"test_accuracy\", accuracy)\n        mlflow.log_metric(\"macro_f1\", macro_f1) #for f1\n        mlflow.end_run()\n        return accuracy\n\n    # Training Loop for FNN and CNN\n    criterion = nn.CrossEntropyLoss()\n\n\n    model.train()\n    for epoch in range(num_classifier_epochs):\n        running_loss = 0.0\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images) if CLASSIFIER == \"CNN\" else model(images.view(images.size(0), -1))\n\n            optimizer.zero_grad()\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n\n        print(f\"{CLASSIFIER} Epoch {epoch+1}: loss = {running_loss / len(train_loader):.4f}\")\n\n    # Model Evaluation\n    model.eval()\n    correct, total = 0, 0\n    all_preds = []   # for f1\n    all_labels = [] \n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images) if CLASSIFIER == \"CNN\" else model(images.view(images.size(0), -1))\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            all_preds.extend(predicted.cpu().numpy())   #for f1\n            all_labels.extend(labels.cpu().numpy()) #for f1\n\n    accuracy = 100 * correct / total\n    macro_f1 = f1_score(all_labels, all_preds, average=\"macro\") #for f1\n    print(f\"Test Accuracy: {accuracy:.2f}%\")\n    print(f\"Macro F1 Score: {macro_f1:.4f}\") #for f1\n\n    mlflow.log_metric(\"test_accuracy\", accuracy)\n    mlflow.log_metric(\"macro_f1\", macro_f1) #for f1\n    mlflow.end_run()\n    return accuracy\n\nif __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=1) # n_trials set to 1 for quick rendering\n    print(f\"Best Parameters for {CLASSIFIER}:\", study.best_params)\n    print(\"Best Accuracy:\", study.best_value)\n\n\nTest Accuracy of Logistic Regression by C (inverse regularization strength) \n\n(Figure 7)\n\n\n\\[\nC = \\frac{1}{\\lambda} \\quad \\text{(inverse regularization strength)}\n\\]\nLower values of C mean more regularization (higher penalties for larger weight coefficients)\nWhat the plot shows: Most optuna trials were lower values of C, so optimization favors stronger regularization. This is further evidenced by the clustering of higher accuracies for lower values of C. A possibly anomaly is seen at C=10 with fairly high accuracy; however, it’s still not higher than lower values of C."
  },
  {
    "objectID": "slides.html#model-2",
    "href": "slides.html#model-2",
    "title": "Restricted Boltzmann Machines",
    "section": "Model 2",
    "text": "Model 2\nModel 2: Feed Forward Network on Fashion MNIST Data\n\n\nClick to Show Code and Output\n\n\nfrom sklearn.metrics import f1_score\n\nCLASSIFIER = \"FNN\"  # Change for FNN, LogisticRegression, or CNN\n\n# Define CNN model\nclass FashionCNN(nn.Module):\n    def __init__(self, filters1, filters2, kernel1, kernel2):\n        super(FashionCNN, self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=filters1, kernel_size=kernel1, padding=1),\n            nn.BatchNorm2d(filters1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(in_channels=filters1, out_channels=filters2, kernel_size=kernel2),\n            nn.BatchNorm2d(filters2),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        self.fc1 = None #initialize first fully connected layer as none, defined later in fwd\n        self.drop = nn.Dropout2d(0.25)\n        self.fc2 = nn.Linear(in_features=600, out_features=120)\n        self.fc3 = nn.Linear(in_features=120, out_features=10)\n        \n\n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        #Flatten tensor dynamically\n        out = out.view(out.size(0), -1)\n        if self.fc1 is None:\n            self.fc1 = nn.Linear(out.shape[1], 600).to(x.device)\n        out = self.fc1(out)\n        out = self.drop(out)\n        out = self.fc2(out)\n        out = self.fc3(out)\n        return out\n\n\n\n# Define Optuna objective function\ndef objective(trial):\n      # Set MLflow experiment name\n    if CLASSIFIER == \"LogisticRegression\":\n        experiment = mlflow.set_experiment(\"new-pytorch-fmnist-lr-noRBM\")\n    elif CLASSIFIER == \"FNN\":\n        experiment = mlflow.set_experiment(\"new-pytorch-fmnist-fnn-noRBM\")\n    elif CLASSIFIER == \"CNN\":\n        experiment = mlflow.set_experiment(\"new-pytorch-fmnist-cnn-noRBM\")\n    batch_size = trial.suggest_int(\"batch_size\", 64, 256, step=32)\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n    mlflow.start_run(experiment_id=experiment.experiment_id)\n    num_classifier_epochs = trial.suggest_int(\"num_classifier_epochs\", 5, 5) \n    mlflow.log_param(\"num_classifier_epochs\", num_classifier_epochs)\n\n    if CLASSIFIER == \"FNN\":\n        hidden_size = trial.suggest_int(\"fnn_hidden\", 192, 384)\n        learning_rate = trial.suggest_float(\"learning_rate\", 0.0001, 0.0025)\n\n        mlflow.log_param(\"classifier\", \"FNN\")\n        mlflow.log_param(\"fnn_hidden\", hidden_size)\n        mlflow.log_param(\"learning_rate\", learning_rate)\n\n        model = nn.Sequential(\n            nn.Linear(784, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, 10)\n        ).to(device)\n\n\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n    elif CLASSIFIER == \"CNN\":\n        filters1 = trial.suggest_int(\"filters1\", 16, 64, step=16)\n        filters2 = trial.suggest_int(\"filters2\", 32, 128, step=32)\n        kernel1 = trial.suggest_int(\"kernel1\", 3, 5)\n        kernel2 = trial.suggest_int(\"kernel2\", 3, 5)\n        learning_rate = trial.suggest_float(\"learning_rate\", 0.0001, 0.0025)\n\n        mlflow.log_param(\"classifier\", \"CNN\")\n        mlflow.log_param(\"filters1\", filters1)\n        mlflow.log_param(\"filters2\", filters2)\n        mlflow.log_param(\"kernel1\", kernel1)\n        mlflow.log_param(\"kernel2\", kernel2)\n        mlflow.log_param(\"learning_rate\", learning_rate)\n\n        model = FashionCNN(filters1, filters2, kernel1, kernel2).to(device)\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n      \n    elif CLASSIFIER == \"LogisticRegression\":\n        mlflow.log_param(\"classifier\", \"LogisticRegression\")\n    \n        # Prepare data for Logistic Regression (Flatten 28x28 images to 784 features)\n        train_features = train_dataset.data.view(-1, 784).numpy()\n        train_labels = train_dataset.targets.numpy()\n        test_features = test_dataset.data.view(-1, 784).numpy()\n        test_labels = test_dataset.targets.numpy()\n    \n        # Normalize the pixel values to [0,1] for better convergence\n        train_features = train_features / 255.0\n        test_features = test_features / 255.0\n    \n    \n        C = trial.suggest_float(\"C\", 0.01, 10.0, log=True)  \n        solver = \"saga\" \n    \n        model = LogisticRegression(C=C, max_iter=num_classifier_epochs, solver=solver)\n        model.fit(train_features, train_labels)\n    \n    \n        predictions = model.predict(test_features)\n        accuracy = accuracy_score(test_labels, predictions) * 100\n        \n        macro_f1 = f1_score(test_labels, predictions, average=\"macro\") #for f1\n        print(f\"Logistic Regression Test Accuracy: {accuracy:.2f}%\")\n        print(f\"Macro F1 Score: {macro_f1:.4f}\") #for f1\n    \n        mlflow.log_param(\"C\", C)\n        mlflow.log_metric(\"test_accuracy\", accuracy)\n        mlflow.log_metric(\"macro_f1\", macro_f1) #for f1\n        mlflow.end_run()\n        return accuracy\n\n    # Training Loop for FNN and CNN\n    criterion = nn.CrossEntropyLoss()\n\n\n    model.train()\n    for epoch in range(num_classifier_epochs):\n        running_loss = 0.0\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images) if CLASSIFIER == \"CNN\" else model(images.view(images.size(0), -1))\n\n            optimizer.zero_grad()\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n\n        print(f\"{CLASSIFIER} Epoch {epoch+1}: loss = {running_loss / len(train_loader):.4f}\")\n\n    # Model Evaluation\n    model.eval()\n    correct, total = 0, 0\n    all_preds = []   # for f1\n    all_labels = [] \n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images) if CLASSIFIER == \"CNN\" else model(images.view(images.size(0), -1))\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            all_preds.extend(predicted.cpu().numpy())   #for f1\n            all_labels.extend(labels.cpu().numpy()) #for f1\n\n    accuracy = 100 * correct / total\n    macro_f1 = f1_score(all_labels, all_preds, average=\"macro\") #for f1\n    print(f\"Test Accuracy: {accuracy:.2f}%\")\n    print(f\"Macro F1 Score: {macro_f1:.4f}\") #for f1\n\n    mlflow.log_metric(\"test_accuracy\", accuracy)\n    mlflow.log_metric(\"macro_f1\", macro_f1) #for f1\n    mlflow.end_run()\n    return accuracy\n\nif __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=1) # n_trials set to 1 for quick rendering\n    print(f\"Best Parameters for {CLASSIFIER}:\", study.best_params)\n    print(\"Best Accuracy:\", study.best_value)\n\n\nTest Accuracy by FNN Hidden Units \n\n(Figure 8)\n\n\nWhat the plot shows: Higher values of hidden units in the feedforward network were sampled more frequently by Optuna, suggesting a preference for more complex models. However, test accuracy appears to level off between 300 and 375 hidden units, suggesting complexity reached its optimal range. Further increases in hidden units would likely not yield higher accuracy."
  },
  {
    "objectID": "slides.html#model-3",
    "href": "slides.html#model-3",
    "title": "Restricted Boltzmann Machines",
    "section": "Model 3",
    "text": "Model 3\nModel 3: Convolutional Neural Network on Fashion MNIST Data Base code for CNN structure borrowed from Kaggle\n\n\nClick to Show Code and Output\n\n\nfrom sklearn.metrics import f1_score\n\nCLASSIFIER = \"CNN\"  # Change for FNN, LogisticRegression, or CNN\n\n# Define CNN model\nclass FashionCNN(nn.Module):\n    def __init__(self, filters1, filters2, kernel1, kernel2):\n        super(FashionCNN, self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=filters1, kernel_size=kernel1, padding=1),\n            nn.BatchNorm2d(filters1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(in_channels=filters1, out_channels=filters2, kernel_size=kernel2),\n            nn.BatchNorm2d(filters2),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        self.fc1 = None #initialize first fully connected layer as none, defined later in fwd\n        self.drop = nn.Dropout2d(0.25)\n        self.fc2 = nn.Linear(in_features=600, out_features=120)\n        self.fc3 = nn.Linear(in_features=120, out_features=10)\n        \n\n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        #Flatten tensor dynamically\n        out = out.view(out.size(0), -1)\n        if self.fc1 is None:\n            self.fc1 = nn.Linear(out.shape[1], 600).to(x.device)\n        out = self.fc1(out)\n        out = self.drop(out)\n        out = self.fc2(out)\n        out = self.fc3(out)\n        return out\n\n\n\n# Define Optuna objective function\ndef objective(trial):\n        # Set MLflow experiment name\n    if CLASSIFIER == \"LogisticRegression\":\n        experiment = mlflow.set_experiment(\"new-pytorch-fmnist-lr-noRBM\")\n    elif CLASSIFIER == \"FNN\":\n        experiment = mlflow.set_experiment(\"new-pytorch-fmnist-fnn-noRBM\")\n    elif CLASSIFIER == \"CNN\":\n        experiment = mlflow.set_experiment(\"new-pytorch-fmnist-cnn-noRBM\")\n    batch_size = trial.suggest_int(\"batch_size\", 64, 256, step=32)\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n    mlflow.start_run(experiment_id=experiment.experiment_id)\n    num_classifier_epochs = trial.suggest_int(\"num_classifier_epochs\", 5, 5) \n    mlflow.log_param(\"num_classifier_epochs\", num_classifier_epochs)\n\n    if CLASSIFIER == \"FNN\":\n        hidden_size = trial.suggest_int(\"fnn_hidden\", 192, 384)\n        learning_rate = trial.suggest_float(\"learning_rate\", 0.0001, 0.0025)\n\n        mlflow.log_param(\"classifier\", \"FNN\")\n        mlflow.log_param(\"fnn_hidden\", hidden_size)\n        mlflow.log_param(\"learning_rate\", learning_rate)\n\n        model = nn.Sequential(\n            nn.Linear(784, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, 10)\n        ).to(device)\n\n\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n    elif CLASSIFIER == \"CNN\":\n        filters1 = trial.suggest_int(\"filters1\", 16, 64, step=16)\n        filters2 = trial.suggest_int(\"filters2\", 32, 128, step=32)\n        kernel1 = trial.suggest_int(\"kernel1\", 3, 5)\n        kernel2 = trial.suggest_int(\"kernel2\", 3, 5)\n        learning_rate = trial.suggest_float(\"learning_rate\", 0.0001, 0.0025)\n\n        mlflow.log_param(\"classifier\", \"CNN\")\n        mlflow.log_param(\"filters1\", filters1)\n        mlflow.log_param(\"filters2\", filters2)\n        mlflow.log_param(\"kernel1\", kernel1)\n        mlflow.log_param(\"kernel2\", kernel2)\n        mlflow.log_param(\"learning_rate\", learning_rate)\n\n        model = FashionCNN(filters1, filters2, kernel1, kernel2).to(device)\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n      \n    elif CLASSIFIER == \"LogisticRegression\":\n        mlflow.log_param(\"classifier\", \"LogisticRegression\")\n    \n        # Prepare data for Logistic Regression (Flatten 28x28 images to 784 features)\n        train_features = train_dataset.data.view(-1, 784).numpy()\n        train_labels = train_dataset.targets.numpy()\n        test_features = test_dataset.data.view(-1, 784).numpy()\n        test_labels = test_dataset.targets.numpy()\n    \n        # Normalize the pixel values to [0,1] for better convergence\n        train_features = train_features / 255.0\n        test_features = test_features / 255.0\n    \n    \n        C = trial.suggest_float(\"C\", 0.01, 10.0, log=True)  \n        solver = \"saga\" \n    \n        model = LogisticRegression(C=C, max_iter=num_classifier_epochs, solver=solver)\n        model.fit(train_features, train_labels)\n    \n    \n        predictions = model.predict(test_features)\n        accuracy = accuracy_score(test_labels, predictions) * 100\n        \n        macro_f1 = f1_score(test_labels, predictions, average=\"macro\") #for f1\n        print(f\"Logistic Regression Test Accuracy: {accuracy:.2f}%\")\n        print(f\"Macro F1 Score: {macro_f1:.4f}\") #for f1\n    \n        mlflow.log_param(\"C\", C)\n        mlflow.log_metric(\"test_accuracy\", accuracy)\n        mlflow.log_metric(\"macro_f1\", macro_f1) #for f1\n        mlflow.end_run()\n        return accuracy\n\n    # Training Loop for FNN and CNN\n    criterion = nn.CrossEntropyLoss()\n\n\n    model.train()\n    for epoch in range(num_classifier_epochs):\n        running_loss = 0.0\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images) if CLASSIFIER == \"CNN\" else model(images.view(images.size(0), -1))\n\n            optimizer.zero_grad()\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n\n        print(f\"{CLASSIFIER} Epoch {epoch+1}: loss = {running_loss / len(train_loader):.4f}\")\n\n    # Model Evaluation\n    model.eval()\n    correct, total = 0, 0\n    all_preds = []   # for f1\n    all_labels = [] \n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images) if CLASSIFIER == \"CNN\" else model(images.view(images.size(0), -1))\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            all_preds.extend(predicted.cpu().numpy())   #for f1\n            all_labels.extend(labels.cpu().numpy()) #for f1\n\n    accuracy = 100 * correct / total\n    macro_f1 = f1_score(all_labels, all_preds, average=\"macro\") #for f1\n    print(f\"Test Accuracy: {accuracy:.2f}%\")\n    print(f\"Macro F1 Score: {macro_f1:.4f}\") #for f1\n\n    mlflow.log_metric(\"test_accuracy\", accuracy)\n    mlflow.log_metric(\"macro_f1\", macro_f1) #for f1\n    mlflow.end_run()\n    return accuracy\n\nif __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=1) # n_trials set to 1 for quick rendering\n    print(f\"Best Parameters for {CLASSIFIER}:\", study.best_params)\n    print(\"Best Accuracy:\", study.best_value)\n\n\nTest Accuracy Based on the Number of Filters in the First Conv2D Layer \n\n(Figure 9)\n\n\nWhat the plot shows: Although the highest test accuracy was achieved with 64 filters in the first convolutional 2D layer, the number of filters alone isn’t a strong predictor of model performance. Each filter size shows high variance (accuracies are spread out for each value vertically). This, combined with the fact that accuracies are well distributed across the different filter counts, suggests other factors or hyperparameters may play a bigger role in predicting accuracy. \nTest Accuracy Based on the Number of Filters in the Second Conv2D Layer \n\n(Figure 10)\n\n\nWhat the plot shows: Like the first Conv2D layer, the number of filters doesn’t seem to be a extremely strong predictor in accuracy. However, Optuna has sampled more frequently from higher number of filters, even 128 for this second layer, suggesting higher filters performed better. However, like before, there is still high variance in accuracy for each number of filters. \nTest Accuracy Based on Kernel Size in the First Conv2D Layer \n\n(Figure 11)\n\n\nWhat the plot shows: Kernel size of 3 was sampled more frequently by Optuna and yielded higher accuracies than kernel sizes of 4 or 5.\nTest Accuracy Based on Kernel Size in the Second Conv2D Layer \n\n(Figure 12)\n\n\nWhat the plot shows: Like with the first convolutional 2D layer, kernel size of 3 is highly favored by Optuna and consistently led to higher test accuracies."
  },
  {
    "objectID": "slides.html#model-4",
    "href": "slides.html#model-4",
    "title": "Restricted Boltzmann Machines",
    "section": "Model 4",
    "text": "Model 4\nModel 4: Logistic Regression on RBM Hidden Features (of Fashion MNIST Data)\n\n\nClick to Show Code and Output\n\n\nfrom sklearn.metrics import accuracy_score, f1_score\nCLASSIFIER = 'LogisticRegression'\n\nif CLASSIFIER == 'LogisticRegression':\n    experiment = mlflow.set_experiment(\"new-pytorch-fmnist-lr-withrbm\")\nelse:\n    experiment = mlflow.set_experiment(\"new-pytorch-fmnist-fnn-withrbm\")\n\n\nclass RBM(nn.Module):\n    def __init__(self, n_visible=784, n_hidden=256, k=1):\n        super(RBM, self).__init__()\n        self.n_visible = n_visible\n        self.n_hidden = n_hidden\n        # Initialize weights and biases\n        self.W = nn.Parameter(torch.randn(n_hidden, n_visible) * 0.1)\n        self.v_bias = nn.Parameter(torch.zeros(n_visible))\n        self.h_bias = nn.Parameter(torch.zeros(n_hidden))\n        self.k = k  # CD-k steps\n\n    def sample_h(self, v):\n        # Given visible v, sample hidden h\n        p_h = torch.sigmoid(F.linear(v, self.W, self.h_bias))  # p(h=1|v)\n        h_sample = torch.bernoulli(p_h)                        # sample Bernoulli\n        return p_h, h_sample\n\n    def sample_v(self, h):\n        # Given hidden h, sample visible v\n        p_v = torch.sigmoid(F.linear(h, self.W.t(), self.v_bias))  # p(v=1|h)\n        v_sample = torch.bernoulli(p_v)\n        return p_v, v_sample\n\n    def forward(self, v):\n        # Perform k steps of contrastive divergence starting from v\n        v_k = v.clone()\n        for _ in range(self.k):\n            _, h_k = self.sample_h(v_k)    # sample hidden from current visible\n            _, v_k = self.sample_v(h_k)    # sample visible from hidden\n        return v_k  # k-step reconstructed visible\n\n    def free_energy(self, v):\n        # Compute the visible bias term for each sample in the batch\n        vbias_term = (v * self.v_bias).sum(dim=1)  # shape: [batch_size]\n        # Compute the activation of the hidden units\n        wx_b = F.linear(v, self.W, self.h_bias)     # shape: [batch_size, n_hidden]\n        # Compute the hidden term\n        hidden_term = torch.sum(torch.log1p(torch.exp(wx_b)), dim=1)  # shape: [batch_size]\n        # Return the mean free energy over the batch\n        return - (vbias_term + hidden_term).mean()\n    \ntransform = transforms.Compose([transforms.ToTensor()])\ntrain_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\ntest_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n\ndef objective(trial):\n    num_rbm_epochs = trial.suggest_int(\"num_rbm_epochs\", 5, 5)# 24, 33)\n    batch_size = trial.suggest_int(\"batch_size\", 192, 1024)\n    rbm_lr = trial.suggest_float(\"rbm_lr\", 0.05, 0.1)\n    rbm_hidden = trial.suggest_int(\"rbm_hidden\", 384, 8192)\n\n    mlflow.start_run(experiment_id=experiment.experiment_id)\n    if CLASSIFIER != 'LogisticRegression':\n        fnn_hidden = trial.suggest_int(\"fnn_hidden\", 192, 384)\n        fnn_lr = trial.suggest_float(\"fnn_lr\", 0.0001, 0.0025)\n        mlflow.log_param(\"fnn_hidden\", fnn_hidden)\n        mlflow.log_param(\"fnn_lr\", fnn_lr)\n\n    num_classifier_epochs = trial.suggest_int(\"num_classifier_epochs\", 5, 5)# 40, 60)\n\n    mlflow.log_param(\"num_rbm_epochs\", num_rbm_epochs)\n    mlflow.log_param(\"batch_size\", batch_size)\n    mlflow.log_param(\"rbm_lr\", rbm_lr)\n    mlflow.log_param(\"rbm_hidden\", rbm_hidden)\n    mlflow.log_param(\"num_classifier_epochs\", num_classifier_epochs)\n\n    # Instantiate RBM and optimizer\n    device = torch.device(\"mps\")\n    rbm = RBM(n_visible=784, n_hidden=rbm_hidden, k=1).to(device)\n    optimizer = torch.optim.SGD(rbm.parameters(), lr=rbm_lr)\n\n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n    rbm_training_failed = False\n    # Training loop (assuming train_loader yields batches of images and labels)\n    for epoch in range(num_rbm_epochs):\n        total_loss = 0.0\n        for images, _ in train_loader:\n            # Flatten images and binarize\n            v0 = images.view(-1, 784).to(rbm.W.device)      # shape [batch_size, 784]\n            v0 = torch.bernoulli(v0)                        # sample binary input\n            vk = rbm(v0)                                    # k-step CD reconstruction\n            # Compute contrastive divergence loss (free energy difference)\n            loss = rbm.free_energy(v0) - rbm.free_energy(vk)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n        print(f\"Epoch {epoch+1}: avg free-energy loss = {total_loss/len(train_loader):.4f}\")\n        if np.isnan(total_loss):\n            rbm_training_failed = True\n            break\n\n        if rbm_training_failed:\n            accuracy = 0.0\n            macro_f1 = 0.0 \n            print(\"RBM training failed — returning 0.0 for accuracy and macro F1\")  \n            mlflow.log_metric(\"test_accuracy\", accuracy)\n            mlflow.log_metric(\"macro_f1\", macro_f1)\n            mlflow.set_tag(\"status\", \"rbm_failed\")  # Optional tag\n            mlflow.end_run()\n            return float(accuracy)\n    else:\n        rbm.eval()  # set in evaluation mode if using any layers that behave differently in training\n        features_list = []\n        labels_list = []\n        for images, labels in train_loader:\n            v = images.view(-1, 784).to(rbm.W.device)\n            v = v  # (optionally binarize or use raw normalized pixels)\n            h_prob, h_sample = rbm.sample_h(v)  # get hidden activations\n            features_list.append(h_prob.cpu().detach().numpy())\n            labels_list.append(labels.numpy())\n        train_features = np.concatenate(features_list)  # shape: [N_train, n_hidden]\n        train_labels = np.concatenate(labels_list)\n\n        # Convert pre-extracted training features and labels to tensors and create a DataLoader\n        train_features_tensor = torch.tensor(train_features, dtype=torch.float32)\n        train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n        train_feature_dataset = torch.utils.data.TensorDataset(train_features_tensor, train_labels_tensor)\n        train_feature_loader = torch.utils.data.DataLoader(train_feature_dataset, batch_size=batch_size, shuffle=True)\n\n            \n        if CLASSIFIER == 'LogisticRegression':\n            # add optuna tuning same as log reg without RBM features...\n            lr_C = trial.suggest_float(\"lr_C\", 0.01, 10.0, log=True)  \n            mlflow.log_param(\"lr_C\", lr_C)  # Log the chosen C value\n\n\n            classifier = LogisticRegression(max_iter=num_classifier_epochs, C=lr_C, solver=\"saga\") \n            classifier.fit(train_features, train_labels)            \n            \n        else:\n            classifier = nn.Sequential(\n                nn.Linear(rbm.n_hidden, fnn_hidden),\n                nn.ReLU(),\n                nn.Linear(fnn_hidden, 10)\n            )\n\n            # Move classifier to the same device as the RBM\n            classifier = classifier.to(device)\n            criterion = nn.CrossEntropyLoss()\n            classifier_optimizer = torch.optim.Adam(classifier.parameters(), lr=fnn_lr)\n\n            classifier.train()\n            for epoch in range(num_classifier_epochs):\n                running_loss = 0.0\n                for features, labels in train_feature_loader:\n                    features = features.to(device)\n                    labels = labels.to(device)\n                    \n                    # Forward pass through classifier\n                    outputs = classifier(features)\n                    loss = criterion(outputs, labels)\n                    \n                    # Backpropagation and optimization\n                    classifier_optimizer.zero_grad()\n                    loss.backward()\n                    classifier_optimizer.step()\n                    \n                    running_loss += loss.item()\n                avg_loss = running_loss / len(train_feature_loader)\n                print(f\"Classifier Epoch {epoch+1}: loss = {avg_loss:.4f}\")\n\n        # Evaluate the classifier on test data.\n        # Here we extract features from the RBM for each test image.\n        if CLASSIFIER != 'LogisticRegression':\n            classifier.eval()\n            correct = 0\n            total = 0\n        features_list = []\n        labels_list = []\n        with torch.no_grad():\n            for images, labels in test_loader:\n                v = images.view(-1, 784).to(device)\n                # Extract hidden activations; you can use either h_prob or h_sample.\n                h_prob, _ = rbm.sample_h(v)\n                if CLASSIFIER == 'LogisticRegression':\n                    features_list.append(h_prob.cpu().detach().numpy())\n                    labels_list.append(labels.numpy())\n                else:\n                    outputs = classifier(h_prob)\n                    _, predicted = torch.max(outputs.data, 1)\n                    total += labels.size(0)\n                    correct += (predicted.cpu() == labels).sum().item()\n\n        if CLASSIFIER == 'LogisticRegression':\n            test_features = np.concatenate(features_list)\n            test_labels = np.concatenate(labels_list)\n            predictions = classifier.predict(test_features)\n            accuracy = accuracy_score(test_labels, predictions) * 100\n        \n            macro_f1 = f1_score(test_labels, predictions, average=\"macro\") \n        \n        else:\n            accuracy = 100 * correct / total\n        \n            all_preds = [] \n            all_labels = [] \n            classifier.eval()\n            with torch.no_grad():\n                for images, labels in test_loader:\n                    v = images.view(-1, 784).to(device)\n                    h_prob, _ = rbm.sample_h(v)\n                    outputs = classifier(h_prob)\n                    _, predicted = torch.max(outputs.data, 1)\n                    all_preds.extend(predicted.cpu().numpy()) \n                    all_labels.extend(labels.numpy()) \n        \n            macro_f1 = f1_score(all_labels, all_preds, average=\"macro\") \n        \n        print(f\"Test Accuracy: {accuracy:.2f}%\")\n        print(f\"Macro F1 Score: {macro_f1:.4f}\") \n        \n        mlflow.log_metric(\"test_accuracy\", accuracy)\n        mlflow.log_metric(\"macro_f1\", macro_f1) \n        mlflow.end_run()\n        return float(accuracy if accuracy is not None else 0.0)\n\nif __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=1) # n_trials set to 1 for quick rendering\n    print(study.best_params)\n    print(study.best_value)\n    print(study.best_trial)\n\n\nTest Accuracy of Logistic Regression on RBM Hidden Features by Inverse Regularization Strength \n\n(Figure 13)\n\n\nWhat the plot shows: When using RBM-extracted hidden features as input to logistic regression, the inverse regularization strength does not appear to be a strong predictor of test accuracy.\nTest Accuracy By Number of RBM Hidden Units \n\n(Figure 14)\n\n\nWhat the plot shows: Optuna slightly favors higher number of hidden units in the rbm with a peak at 5340 (and similar peaks 5358, 5341, etc.). However, after 7000 units, accuracy appears to decline suggesting the optimum number of units was reached around that 5300 mark."
  },
  {
    "objectID": "slides.html#model-5",
    "href": "slides.html#model-5",
    "title": "Restricted Boltzmann Machines",
    "section": "Model 5",
    "text": "Model 5\nModel 5: Feed Forward Network on RBM Hidden Features (of Fashion MNIST Data)\n\n\nClick to Show Code and Output\n\n\nfrom sklearn.metrics import accuracy_score, f1_score\nCLASSIFIER = 'FNN'\n\nif CLASSIFIER == 'LogisticRegression':\n    experiment = mlflow.set_experiment(\"new-pytorch-fmnist-lr-withrbm\")\nelse:\n    experiment = mlflow.set_experiment(\"new-pytorch-fmnist-fnn-withrbm\")\n\n\nclass RBM(nn.Module):\n    def __init__(self, n_visible=784, n_hidden=256, k=1):\n        super(RBM, self).__init__()\n        self.n_visible = n_visible\n        self.n_hidden = n_hidden\n        # Initialize weights and biases\n        self.W = nn.Parameter(torch.randn(n_hidden, n_visible) * 0.1)\n        self.v_bias = nn.Parameter(torch.zeros(n_visible))\n        self.h_bias = nn.Parameter(torch.zeros(n_hidden))\n        self.k = k  # CD-k steps\n\n    def sample_h(self, v):\n        # Given visible v, sample hidden h\n        p_h = torch.sigmoid(F.linear(v, self.W, self.h_bias))  # p(h=1|v)\n        h_sample = torch.bernoulli(p_h)                        # sample Bernoulli\n        return p_h, h_sample\n\n    def sample_v(self, h):\n        # Given hidden h, sample visible v\n        p_v = torch.sigmoid(F.linear(h, self.W.t(), self.v_bias))  # p(v=1|h)\n        v_sample = torch.bernoulli(p_v)\n        return p_v, v_sample\n\n    def forward(self, v):\n        # Perform k steps of contrastive divergence starting from v\n        v_k = v.clone()\n        for _ in range(self.k):\n            _, h_k = self.sample_h(v_k)    # sample hidden from current visible\n            _, v_k = self.sample_v(h_k)    # sample visible from hidden\n        return v_k  # k-step reconstructed visible\n\n    def free_energy(self, v):\n        # Compute the visible bias term for each sample in the batch\n        vbias_term = (v * self.v_bias).sum(dim=1)  # shape: [batch_size]\n        # Compute the activation of the hidden units\n        wx_b = F.linear(v, self.W, self.h_bias)     # shape: [batch_size, n_hidden]\n        # Compute the hidden term\n        hidden_term = torch.sum(torch.log1p(torch.exp(wx_b)), dim=1)  # shape: [batch_size]\n        # Return the mean free energy over the batch\n        return - (vbias_term + hidden_term).mean()\n    \ntransform = transforms.Compose([transforms.ToTensor()])\ntrain_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\ntest_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n\ndef objective(trial):\n    num_rbm_epochs = trial.suggest_int(\"num_rbm_epochs\", 5, 5)# 24, 33)\n    batch_size = trial.suggest_int(\"batch_size\", 192, 1024)\n    rbm_lr = trial.suggest_float(\"rbm_lr\", 0.05, 0.1)\n    rbm_hidden = trial.suggest_int(\"rbm_hidden\", 384, 8192)\n\n    mlflow.start_run(experiment_id=experiment.experiment_id)\n    if CLASSIFIER != 'LogisticRegression':\n        fnn_hidden = trial.suggest_int(\"fnn_hidden\", 192, 384)\n        fnn_lr = trial.suggest_float(\"fnn_lr\", 0.0001, 0.0025)\n        mlflow.log_param(\"fnn_hidden\", fnn_hidden)\n        mlflow.log_param(\"fnn_lr\", fnn_lr)\n\n    num_classifier_epochs = trial.suggest_int(\"num_classifier_epochs\", 5, 5)# 40, 60)\n\n    mlflow.log_param(\"num_rbm_epochs\", num_rbm_epochs)\n    mlflow.log_param(\"batch_size\", batch_size)\n    mlflow.log_param(\"rbm_lr\", rbm_lr)\n    mlflow.log_param(\"rbm_hidden\", rbm_hidden)\n    mlflow.log_param(\"num_classifier_epochs\", num_classifier_epochs)\n\n    # Instantiate RBM and optimizer\n    device = torch.device(\"mps\")\n    rbm = RBM(n_visible=784, n_hidden=rbm_hidden, k=1).to(device)\n    optimizer = torch.optim.SGD(rbm.parameters(), lr=rbm_lr)\n\n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n    rbm_training_failed = False\n    # Training loop (assuming train_loader yields batches of images and labels)\n    for epoch in range(num_rbm_epochs):\n        total_loss = 0.0\n        for images, _ in train_loader:\n            # Flatten images and binarize\n            v0 = images.view(-1, 784).to(rbm.W.device)      # shape [batch_size, 784]\n            v0 = torch.bernoulli(v0)                        # sample binary input\n            vk = rbm(v0)                                    # k-step CD reconstruction\n            # Compute contrastive divergence loss (free energy difference)\n            loss = rbm.free_energy(v0) - rbm.free_energy(vk)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n        print(f\"Epoch {epoch+1}: avg free-energy loss = {total_loss/len(train_loader):.4f}\")\n        if np.isnan(total_loss):\n            rbm_training_failed = True\n            break\n\n        if rbm_training_failed:\n            accuracy = 0.0\n            macro_f1 = 0.0 \n            print(\"RBM training failed — returning 0.0 for accuracy and macro F1\")  \n            mlflow.log_metric(\"test_accuracy\", accuracy)\n            mlflow.log_metric(\"macro_f1\", macro_f1)\n            mlflow.set_tag(\"status\", \"rbm_failed\")  # Optional tag\n            mlflow.end_run()\n            return float(accuracy)\n    else:\n        rbm.eval()  # set in evaluation mode if using any layers that behave differently in training\n        features_list = []\n        labels_list = []\n        for images, labels in train_loader:\n            v = images.view(-1, 784).to(rbm.W.device)\n            v = v  # (optionally binarize or use raw normalized pixels)\n            h_prob, h_sample = rbm.sample_h(v)  # get hidden activations\n            features_list.append(h_prob.cpu().detach().numpy())\n            labels_list.append(labels.numpy())\n        train_features = np.concatenate(features_list)  # shape: [N_train, n_hidden]\n        train_labels = np.concatenate(labels_list)\n\n        # Convert pre-extracted training features and labels to tensors and create a DataLoader\n        train_features_tensor = torch.tensor(train_features, dtype=torch.float32)\n        train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n        train_feature_dataset = torch.utils.data.TensorDataset(train_features_tensor, train_labels_tensor)\n        train_feature_loader = torch.utils.data.DataLoader(train_feature_dataset, batch_size=batch_size, shuffle=True)\n\n            \n        if CLASSIFIER == 'LogisticRegression':\n            # add optuna tuning same as log reg without RBM features...\n            lr_C = trial.suggest_float(\"lr_C\", 0.01, 10.0, log=True)  \n            mlflow.log_param(\"lr_C\", lr_C)  # Log the chosen C value\n\n\n            classifier = LogisticRegression(max_iter=num_classifier_epochs, C=lr_C, solver=\"saga\") \n            classifier.fit(train_features, train_labels)            \n            \n        else:\n            classifier = nn.Sequential(\n                nn.Linear(rbm.n_hidden, fnn_hidden),\n                nn.ReLU(),\n                nn.Linear(fnn_hidden, 10)\n            )\n\n            # Move classifier to the same device as the RBM\n            classifier = classifier.to(device)\n            criterion = nn.CrossEntropyLoss()\n            classifier_optimizer = torch.optim.Adam(classifier.parameters(), lr=fnn_lr)\n\n            classifier.train()\n            for epoch in range(num_classifier_epochs):\n                running_loss = 0.0\n                for features, labels in train_feature_loader:\n                    features = features.to(device)\n                    labels = labels.to(device)\n                    \n                    # Forward pass through classifier\n                    outputs = classifier(features)\n                    loss = criterion(outputs, labels)\n                    \n                    # Backpropagation and optimization\n                    classifier_optimizer.zero_grad()\n                    loss.backward()\n                    classifier_optimizer.step()\n                    \n                    running_loss += loss.item()\n                avg_loss = running_loss / len(train_feature_loader)\n                print(f\"Classifier Epoch {epoch+1}: loss = {avg_loss:.4f}\")\n\n        # Evaluate the classifier on test data.\n        # Here we extract features from the RBM for each test image.\n        if CLASSIFIER != 'LogisticRegression':\n            classifier.eval()\n            correct = 0\n            total = 0\n        features_list = []\n        labels_list = []\n        with torch.no_grad():\n            for images, labels in test_loader:\n                v = images.view(-1, 784).to(device)\n                # Extract hidden activations; you can use either h_prob or h_sample.\n                h_prob, _ = rbm.sample_h(v)\n                if CLASSIFIER == 'LogisticRegression':\n                    features_list.append(h_prob.cpu().detach().numpy())\n                    labels_list.append(labels.numpy())\n                else:\n                    outputs = classifier(h_prob)\n                    _, predicted = torch.max(outputs.data, 1)\n                    total += labels.size(0)\n                    correct += (predicted.cpu() == labels).sum().item()\n\n        if CLASSIFIER == 'LogisticRegression':\n            test_features = np.concatenate(features_list)\n            test_labels = np.concatenate(labels_list)\n            predictions = classifier.predict(test_features)\n            accuracy = accuracy_score(test_labels, predictions) * 100\n        \n            macro_f1 = f1_score(test_labels, predictions, average=\"macro\") \n        \n        else:\n            accuracy = 100 * correct / total\n        \n            all_preds = [] \n            all_labels = [] \n            classifier.eval()\n            with torch.no_grad():\n                for images, labels in test_loader:\n                    v = images.view(-1, 784).to(device)\n                    h_prob, _ = rbm.sample_h(v)\n                    outputs = classifier(h_prob)\n                    _, predicted = torch.max(outputs.data, 1)\n                    all_preds.extend(predicted.cpu().numpy()) \n                    all_labels.extend(labels.numpy()) \n        \n            macro_f1 = f1_score(all_labels, all_preds, average=\"macro\") \n        \n        print(f\"Test Accuracy: {accuracy:.2f}%\")\n        print(f\"Macro F1 Score: {macro_f1:.4f}\") \n        \n        mlflow.log_metric(\"test_accuracy\", accuracy)\n        mlflow.log_metric(\"macro_f1\", macro_f1) \n        mlflow.end_run()\n        return float(accuracy if accuracy is not None else 0.0)\n\nif __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=1) # n_trials set to 1 for quick rendering\n    print(study.best_params)\n    print(study.best_value)\n    print(study.best_trial)\n\n\nTest Accuracy by RBM Hidden Units \n\n(Figure 15)\n\n\nWhat the plot shows: Highest accuracies cluster between 2000 and 4000 hidden units in the RBM with an outlier at 3764 hidden units. This possibly suggests too few hidden units lacks the complexity needed to explain the data; however, too many hidden units is perhaps causing some overfitting–resulting in poor generalization of the FNN classifier that receives the RBM hidden features.\nTest Accuracy by FNN Hidden Units \n\n(Figure 16)\n\n\nWhat the plot shows: Surprisingly, the number of hidden units in the FNN does not show a strong correlation with test accuracy. All hidden units tested seem to result in similar performance. This suggests the FNN is able to learn from the RBM features sufficently, and additional neurons do not significantly improve generalization.\n\n\n\n\n\n\n\n\n\nModel\nOptuna Best Trial\nMLflow Test Accuracy(%)\nMacro F1 Score\n\n\n\n\nLogistic Regression\n84.71\n0.846\n\n\nFeed Forward Network\n88.06\n0.879\n\n\nConvolutional Neural Network\n91.29\n0.913\n\n\nLogistic Regression (on RBM Hidden Features)\n87.14\n0.871\n\n\nFeed Forward Network (on RBM Hidden Features)\n86.95\n0.869\n\n\n\n\n(Table 2)"
  },
  {
    "objectID": "slides.html#our-models",
    "href": "slides.html#our-models",
    "title": "Restricted Boltzmann Machines",
    "section": "Our Models",
    "text": "Our Models\n\nLogistic Regression on Fashion MNIST Data\nFeed Forward Network on Fashion MNIST Data 3. Convolutional Neural Network on Fashion MNIST Data 4. Logistic Regression on RBM Hidden Features (of Fashion MNIST Data) 5. Feed Forward Network on RBM Hidden Features (of Fashion MNIST Data)\n\nNote: Outputs (50 trials) and Code are below for each model. Both the code and output can be toggled by the reader. • The first click reveals a toggle labeled “Code”. • Clicking “Code” will show the output. • Clicking again will switch from output to the actual code. • Clicking “Show Code and Output” again will collapse both views.\nImport Libraries and Re-load data for first 3 models\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nimport numpy as np\nimport mlflow\nimport optuna\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom torch.utils.data import DataLoader\n\n# Set device\ndevice = torch.device(\"mps\")\n\n# Load Fashion-MNIST dataset again for the first 3 models\ntransform = transforms.Compose([transforms.ToTensor()])\ntrain_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\ntest_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n\n\n#mlflow.end_run()\n#run this in terminal when need to fully clean out expierment after you delete it in the ui\n#rm -rf mlruns/.trash/*"
  },
  {
    "objectID": "slides.html#introduction-1",
    "href": "slides.html#introduction-1",
    "title": "Restricted Boltzmann Machines",
    "section": "Introduction",
    "text": "Introduction\nBackground\nRestricted Boltzmann Machines (RBM) are a type of neural network that has been around since the 1980s. RBMs are primarily used for unsupervised learning tasks like dimensionality reduction and feature extraction, which help prepare datasets for machine learning models that may later be trained using supervised learning.\nLike Hopfield networks, Boltzmann machines are undirected graphical models, but they are different in that they are stochastic and can have hidden units. Both models are energy-based, meaning they learn by minimizing an energy function for each model (Smolensky et al. 1986). Boltzmann machines use a sigmoid activation function, which allows for the model to be probabilistic.\nIn the “Restricted” Boltzmann Machine, there are no interactions between neurons in the visible layer or between neurons in the hidden layer, creating a bipartite graph of neurons. Below is a diagram taken from Goodfellow, et al. (Goodfellow, Bengio, and Courville 2016) (p. 577) for visualization of the connections."
  },
  {
    "objectID": "slides.html#introduction-2",
    "href": "slides.html#introduction-2",
    "title": "Restricted Boltzmann Machines",
    "section": "Introduction",
    "text": "Introduction\nBackground\nRestricted Boltzmann Machines (RBM) are a type of neural network that has been around since the 1980s. RBMs are primarily used for unsupervised learning tasks like dimensionality reduction and feature extraction, which help prepare datasets for machine learning models that may later be trained using supervised learning.\nLike Hopfield networks, Boltzmann machines are undirected graphical models, but they are different in that they are stochastic and can have hidden units. Both models are energy-based, meaning they learn by minimizing an energy function for each model (Smolensky et al. 1986). Boltzmann machines use a sigmoid activation function, which allows for the model to be probabilistic.\nIn the “Restricted” Boltzmann Machine, there are no interactions between neurons in the visible layer or between neurons in the hidden layer, creating a bipartite graph of neurons. Below is a diagram taken from Goodfellow, et al. (Goodfellow, Bengio, and Courville 2016) (p. 577) for visualization of the connections."
  },
  {
    "objectID": "slides.html#boltzmann-machines",
    "href": "slides.html#boltzmann-machines",
    "title": "Restricted Boltzmann Machines",
    "section": "Boltzmann Machines",
    "text": "Boltzmann Machines\n\nBoltzmann Machines are undirected graphical models where nodes are connected with symmetric weights\nTotal input is calculated for a node by \\[z_i = b_i + \\sum_j s_j w_{ij}\\] where \\(b_i\\) is the bias, \\(s_j\\) is the state of the connected node, and \\(w_{ij}\\) is the weight of the connection\nNodes are updated stochastically by sampling from the bernoulli distribution given by \\[p(s_i = 1) = \\frac{1}{1 + e^{-z_i}}\\] where \\(s_i\\) is the state of the node"
  },
  {
    "objectID": "slides.html#boltzmann-machine-energy-function",
    "href": "slides.html#boltzmann-machine-energy-function",
    "title": "Restricted Boltzmann Machines",
    "section": "Boltzmann Machine Energy Function",
    "text": "Boltzmann Machine Energy Function\n\nThe energy of a given state vector \\(\\mathbf{v}\\) is given by \\[E(\\mathbf{v}) = -\\sum_i s_i^{\\mathbf{v}} b_i - \\sum_{i&lt;j} s_i^{\\mathbf{v}} s_j^{\\mathbf{v}} w_{ij}\\] where \\(s_i^{\\mathbf{v}}\\) is the state of the node in the vector \\(\\mathbf{v}\\), \\(b_i\\) is the bias, and \\(w_{ij}\\) is the weight of the connection\nThe Boltzmann distribution (or equilibrium or stationary distribution) is given by \\[p(\\mathbf{v}) = \\frac{e^{-E(\\mathbf{v})}}{\\sum_{\\mathbf{u}} e^{-E(\\mathbf{u})}}\\] where \\(\\mathbf{u}\\) is the set of all possible states"
  },
  {
    "objectID": "slides2.html#boltzmann-machines",
    "href": "slides2.html#boltzmann-machines",
    "title": "Restricted Boltzmann Machines",
    "section": "Boltzmann Machines",
    "text": "Boltzmann Machines\n\nBoltzmann Machines are undirected graphical models where nodes are connected with symmetric weights\nTotal input is calculated for a node by \\[z_i = b_i + \\sum_j s_j w_{ij}\\] where \\(b_i\\) is the bias, \\(s_j\\) is the state of the connected node, and \\(w_{ij}\\) is the weight of the connection\nNodes are updated stochastically by sampling from the bernoulli distribution given by \\[p(s_i = 1) = \\frac{1}{1 + e^{-z_i}}\\] where \\(s_i\\) is the state of the node"
  },
  {
    "objectID": "slides2.html#boltzmann-machine-energy-function",
    "href": "slides2.html#boltzmann-machine-energy-function",
    "title": "Restricted Boltzmann Machines",
    "section": "Boltzmann Machine Energy Function",
    "text": "Boltzmann Machine Energy Function\n\nThe energy of a given state vector \\(\\mathbf{v}\\) is given by \\[E(\\mathbf{v}) = -\\sum_i s_i^{\\mathbf{v}} b_i - \\sum_{i&lt;j} s_i^{\\mathbf{v}} s_j^{\\mathbf{v}} w_{ij}\\] where \\(s_i^{\\mathbf{v}}\\) is the state of the node in the vector \\(\\mathbf{v}\\), \\(b_i\\) is the bias, and \\(w_{ij}\\) is the weight of the connection\nThe Boltzmann distribution (or equilibrium or stationary distribution) is given by \\[p(\\mathbf{v}) = \\frac{e^{-E(\\mathbf{v})}}{\\sum_{\\mathbf{u}} e^{-E(\\mathbf{u})}}\\] where \\(\\mathbf{u}\\) is the set of all possible states"
  },
  {
    "objectID": "slides2.html#introduction",
    "href": "slides2.html#introduction",
    "title": "Restricted Boltzmann Machines",
    "section": "Introduction",
    "text": "Introduction\nBackground\nRestricted Boltzmann Machines (RBM) are a type of neural network that has been around since the 1980s. RBMs are primarily used for unsupervised learning tasks like dimensionality reduction and feature extraction, which help prepare datasets for machine learning models that may later be trained using supervised learning.\nLike Hopfield networks, Boltzmann machines are undirected graphical models, but they are different in that they are stochastic and can have hidden units. Both models are energy-based, meaning they learn by minimizing an energy function for each model (Smolensky et al. 1986). Boltzmann machines use a sigmoid activation function, which allows for the model to be probabilistic.\nIn the “Restricted” Boltzmann Machine, there are no interactions between neurons in the visible layer or between neurons in the hidden layer, creating a bipartite graph of neurons. Below is a diagram taken from Goodfellow, et al. (Goodfellow, Bengio, and Courville 2016) (p. 577) for visualization of the connections."
  },
  {
    "objectID": "slides2.html#more-on-restricted-boltzmann-machines",
    "href": "slides2.html#more-on-restricted-boltzmann-machines",
    "title": "Restricted Boltzmann Machines",
    "section": "More on Restricted Boltzmann Machines",
    "text": "More on Restricted Boltzmann Machines\nGoodfellow, et al. discuss the expense in drawing samples for most undirected graphical models; however, the RBM allows for block Gibbs sampling (p. 578) where the network alternates between sampling all hidden units simultaneously (etc. for visible). Derivatives are also simplified by the fact that the energy function of the RBM is a linear function of it’s parameters, which will be seen further in Methods.\nRBMs are trained using a process called Contrastive Divergence (CD) (Hinton 2002) where the weights are updated to minimize the difference between samples from the data and samples from the model. Learning rate, batch size, and number of hidden units are all hyperparameters that can affect the ability of the training to converge successfully and learn the underlying structure of the data."
  },
  {
    "objectID": "slides2.html#applications",
    "href": "slides2.html#applications",
    "title": "Restricted Boltzmann Machines",
    "section": "Applications",
    "text": "Applications\nRBMs are probably best known for their success in collaborative filtering. The RBM model was used in the Netflix Prize competition to predict user ratings for movies, with the result that it outperformed the Singular Value Decomposition (SVD) method that was state-of-the-art at the time (Salakhutdinov, Mnih, and Hinton 2007). They have also been trained to recognize handwritten digits, such as the MNIST dataset (Hinton 2002).\nRBMs have been successfully used to distinguish normal and anomalous network traffic. Their potential use in improving network security for companies in the future is promising. There is slow progress in network anomaly detection due to the difficulty of obtaining datasets for training and testing networks. Clients are often reluctant to divulge information that could potentially harm their networks. In a real-life dataset where one host had normal traffic and one was infected by a bot, discriminative RBM (DRBM) was able to successfully distinguish the normal from anomalous traffic. DRBM doesn’t rely on knowing the data distribution ahead of time, which is useful, except that it also causes the DRBM to overfit. As a result, when trying to use the same trained RBM on the KDD ’99 training dataset performance declined. (Fiore et al. 2013)\nRBMs can provide greatly improved classification of brain disorders in MRI images. Generative Adversarial Networks (GANs) use two neural networks: a generator which generates fake data, and a discriminator which tries to distinguish between real and fake data. Loss from the discriminator is backpropagated through the generator so that both part are trained simultaneously. The RBM-GAN uses RBM features from real MRI images as inputs to the generator. Features from the discriminator are then used as inputs to a classifier. (Aslan, Dogan, and Koca 2023)\nThe many-body quantum wavefunction, which describes the quantum state of a system of particles is difficult to compute with classical computers. RBMs have been used to approximate it using variational Monte Carlo methods. (Melko et al. 2019)\nRBMs are notoriously slow to train. The process of computing the activation probability requires the calculation of vector dot products. Lean Constrastive Divergence (LCD) is a method which adds two techniques to speed up the process of training RBMs. The first is bounds-based filtering where upper and lower bounds of the probability select only a range of dot products to perform. Second, the delta product involves only recalculating the changed portions of the vector dot product. (Ning, Pittman, and Shen 2018)\n\n\n\n\n\n\nAslan, Narin, Sengul Dogan, and Gonca Ozmen Koca. 2023. “Automated Classification of Brain Diseases Using the Restricted Boltzmann Machine and the Generative Adversarial Network.” Engineering Applications of Artificial Intelligence 126: 106794.\n\n\nFiore, Ugo, Francesco Palmieri, Aniello Castiglione, and Alfredo De Santis. 2013. “Network Anomaly Detection with the Restricted Boltzmann Machine.” Neurocomputing 122: 13–23.\n\n\nHinton, Geoffrey E. 2002. “Training Products of Experts by Minimizing Contrastive Divergence.” Neural Computation 14 (8): 1771–1800.\n\n\nMelko, Roger G, Giuseppe Carleo, Juan Carrasquilla, and J Ignacio Cirac. 2019. “Restricted Boltzmann Machines in Quantum Physics.” Nature Physics 15 (9): 887–92.\n\n\nNing, Lin, Randall Pittman, and Xipeng Shen. 2018. “LCD: A Fast Contrastive Divergence Based Algorithm for Restricted Boltzmann Machine.” Neural Networks 108: 399–410.\n\n\nSalakhutdinov, Ruslan, Andriy Mnih, and Geoffrey Hinton. 2007. “Restricted Boltzmann Machines for Collaborative Filtering.” In Proceedings of the 24th International Conference on Machine Learning, 791–98."
  },
  {
    "objectID": "slides2.html#methods",
    "href": "slides2.html#methods",
    "title": "Restricted Boltzmann Machines",
    "section": "Methods",
    "text": "Methods\nBelow is the energy function of the RBM.\n\\[\nE(v,h) = - \\sum_{i} a_i v_i - \\sum_{j} b_j h_j - \\sum_{i} \\sum_{j} v_i w_{i,j} h_j\n\\qquad(1)\\] where vi and hj represent visible and hidden units; ai and bj are the bias terms of the visible and hidden units; and each w{i,j} (weight) element represents the interaction between the visible and hidden units. (Fischer and Igel 2012)"
  },
  {
    "objectID": "slides2.html#background-on-models-for-classification-task",
    "href": "slides2.html#background-on-models-for-classification-task",
    "title": "Restricted Boltzmann Machines",
    "section": "Background on Models for Classification Task",
    "text": "Background on Models for Classification Task\nWe train Logistic Regression (with and without RBM features as input), Feed Forward Network (with and without RBM features as input), and Convolutional Neural Network. Below is a brief reminder of the basics of each model.\nFor the models incoroporating the RBM, we take the Fashion MNIST features/pixels and train the RBM (unsupervised learning) to extract hidden features from the visible layer and then feed these features into either logistic regression or feed forward network. We then use the trained model to predict labels for the test data, evaluating how well the RBM-derived features perform in a supervised classification task.\n1. Logistic Regression\nMathematically, the concept behind binary logistic regression is the logit (the natural logarithm of an odds ratio)(Peng, Lee, and Ingersoll 2002). However, since we have 10 labels, our classification task falls into “Multinomial Logistic Regression.”\n\\[\nP(Y = k | X) = \\frac{e^{\\beta_{0k} + \\beta_k^T X}}{\\sum_{l=1}^{K} e^{\\beta_{0l} + \\beta_l^T X}}\n\\qquad(2)\\]\n2. Simple Feed Forward Neural Network\nThe feed forward network (FNN) is one where information flows in one direction from input to output with no loops or feedback. There can be zero hidden layers in between (called single FNN) or one or more hidden layers (multilayer FNN).  (Sazlı 2006) \n3. Convolutional Neural Network\nThe convolutional neural network (CNN) is a type of feed forward network except that unlike the traditional ANN, CNNs are primarily used for pattern recognition with images (O’Shea and Nash 2015). The CNN has 3 layers which are stacked to form the full CNN: convolutional, pooling, and fully-connected layers."
  },
  {
    "objectID": "slides2.html#creating-the-rbm",
    "href": "slides2.html#creating-the-rbm",
    "title": "Restricted Boltzmann Machines",
    "section": "Creating the RBM",
    "text": "Creating the RBM\nBelow is our Process for creating the RBM:\nStep 1: We first initialize the RBM with random weights and biases and set visible units to 784 and hidden units to 256. We also set the number of contrastive divergence steps (k) to 1.  Step 2: Sample hidden units from visible. The math behind computing the hidden unit activations from the given input can be seen in Equation 3 (Fischer and Igel 2012) where the probability is used to sample from the Bernoulli distribution.  \\[\np(H_i = 1 | \\mathbf{v}) = \\sigma \\left( \\sum_{j=1}^{m} w_{ij} v_j + c_i \\right)\n\\qquad(3)\\]\nwhere p(.) is the probability of the ith hidden state being activated (=1) given the visible input vector. σ is the sigmoid activation function (below) which maps the weighted sum to a probability between 0 and 1. m is the number of visible units. wij is the weight connecting visible unit j to hidden unit i. vj is the value of the jth visible unit. and ci is the bias term for the hidden unit. \\[\n\\sigma(x) = \\frac{1}{1 + e^{-x}}\n\\]\nStep 3: Sample visible units from hidden. The math behind computing visible unit activations from the hidden layer can be seen in Equation 4 (Fischer and Igel 2012) Visible states are sampled using the Bernoulli distribution. This way we can see how well the RBM learned from the inputs.  \\[\np(V_j = 1 | \\mathbf{h}) = \\sigma \\left( \\sum_{i=1}^{n} w_{ij} h_i + b_j \\right)\n\\qquad(4)\\]\nwhere p(.) is the probability of the ith visible unit being activated (=1) given the hidden vector h. σ is same as above. n is the number of hidden units. wij is the weight connecting hidden unit i to visible unit j. bj is the bias term for the jth visible unit.\nStep 4: K=1 steps of Contrastive Divergence (Feed Forward, Feed Backward) which executes steps 2 and 3. Contrastive Divergence updates the RBM’s weights by minimizing the difference between the original input and the reconstructed input created by the RBM.  Step 5: Free energy is computed. The free energy F is given by the logarithm of the partition function Z (Oh, Baggag, and Nha 2020) where the partition function is  \\[\nZ(\\theta) \\equiv \\sum_{v,h} e^{-E(v,h; \\theta)}\n\\qquad(5)\\] and the free energy function is  \\[\nF(\\theta) = -\\ln Z(\\theta)\n\\qquad(6)\\] where lower free energy means the RBM learned the visible state well.\nStep 6: Train the RBM. Model weights updated via gradient descent. Step 7: Feature extraction for classification with LR. The hidden layer activations of the RBM are used as features for Logistic Regression and Feed Forward Network."
  },
  {
    "objectID": "slides2.html#hyperparameter-tuning",
    "href": "slides2.html#hyperparameter-tuning",
    "title": "Restricted Boltzmann Machines",
    "section": "Hyperparameter Tuning",
    "text": "Hyperparameter Tuning\nWe use the Tree-structured Parzen Estimator algorithm from Optuna (Akiba et al. 2019) to tune the hyperparameters of the RBM and the classifier models, and we use MLFlow (Zaharia et al. 2018) to record and visualize the results of the hyperparameter tuning process. The hyperparameters we tune include the learning rate, batch size, number of hidden units, and number of epochs."
  },
  {
    "objectID": "slides2.html#metrics-used",
    "href": "slides2.html#metrics-used",
    "title": "Restricted Boltzmann Machines",
    "section": "Metrics Used",
    "text": "Metrics Used\n1. Accuracy Accuracy is defined as the number of correct classifications divided by the total number of classifications \\[\n\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}\n\\]\n2. Macro F1 Score Macro F1 score is the unweighted average of the individual F1 scores of each class. It takes no regard for class imbalance; however, we saw earlier the classes are all balanced in Fashion MNIST. The F1 score for each individual class is as follows \\[\n\\text{F1} = \\frac{2 \\cdot \\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n\\] where precision for each class is \\[\n\\text{Precision} = \\frac{TP}{TP + FP}\n\\] and recall for each class is \\[\n\\text{Recall} = \\frac{TP}{TP + FN}\n\\] The definitions of these terms for multiclass problems are more complicated than binary and are best displayed as examples. \n\n\n\n\n\n\n\n\nAcronymn\nExample for a trouser image\n\n\n\n\nTP = True Positives\nthe image is a trouser and the model predicts a trouser\n\n\nTN = True Negatives\nthe image is not a trouser and the model predicts anything but trouser\n\n\nFP = False Positives\nthe image is anything but trouser but the model predicts trouser\n\n\nFN = False Negatives\nthe image is a trouser and the model predicts another class (like shirt)\n\n\n\nAs stated earlier, the individual F1 scores for each class are taken and averaged to compute the Macro F1 score in a multiclass problem like Fashion MNIST."
  },
  {
    "objectID": "slides2.html#analysis-and-results",
    "href": "slides2.html#analysis-and-results",
    "title": "Restricted Boltzmann Machines",
    "section": "Analysis and Results",
    "text": "Analysis and Results\nData Exploration and Visualization\nWe use the Fashion MNIST dataset from Zalando Research (Xiao, Rasul, and Vollgraf 2017). The set includes 70,000 grayscale images of clothing items, 60,000 for training and 10,000 for testing. Each image is 28x28 pixels (784 pixels total). Each pixel has a value associated with it ranging from 0 (white) to 255 (very dark) – whole numbers only. There are 785 columns in total as one column is dedicated to the label.\n \nThere are 10 labels in total:\n0 T-shirt/top 1 Trouser 2 Pullover 3 Dress 4 Coat 5 Sandal 6 Shirt 7 Sneaker 8 Bag 9 Ankle boot\nBelow we load the dataset.\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nimport torch\nimport torchvision.datasets\nimport torchvision.models\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\n\n\n\ntrain_data = torchvision.datasets.FashionMNIST(\n    root=\"./data\", \n    train=True, \n    download=True, \n    transform=transforms.ToTensor()  # Converts to tensor but does NOT normalize\n)\n\ntest_data = torchvision.datasets.FashionMNIST(\n    root=\"./data\", \n    train=False, \n    download=True, \n    transform=transforms.ToTensor()  \n)\n\nGet the seventh image to show a sample\n\n# Extract the first image (or choose any index)\nimage_tensor, label = train_data[6]  # shape: [1, 28, 28]\n\n# Convert tensor to NumPy array\nimage_array = image_tensor.numpy().squeeze()  \n\n# Plot the image\nplt.figure(figsize=(5,5))\nplt.imshow(image_array, cmap=\"gray\")\nplt.title(f\"FashionMNIST Image (Label: {label})\")\nplt.axis(\"off\")  # Hide axes\nplt.show()\n\n\n\n\n\ntrain_images = train_data.data.numpy()  # Raw pixel values (0-255)\ntrain_labels = train_data.targets.numpy()\nX = train_images.reshape(-1, 784)  # Flatten 28x28 images into 1D (60000, 784)\n\n\n#print(train_images[:5])\nflattened = train_images[:5].reshape(5, -1) \n\n# Create a DataFrame\ndf_flat = pd.DataFrame(flattened)\nprint(df_flat.head())\n#train_df.info() #datatypes are integers\n\n   0    1    2    3    4    5    6    7    8    9    ...  774  775  776  777  \\\n0    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n1    0    0    0    0    0    1    0    0    0    0  ...  119  114  130   76   \n2    0    0    0    0    0    0    0    0    0   22  ...    0    0    1    0   \n3    0    0    0    0    0    0    0    0   33   96  ...    0    0    0    0   \n4    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n\n   778  779  780  781  782  783  \n0    0    0    0    0    0    0  \n1    0    0    0    0    0    0  \n2    0    0    0    0    0    0  \n3    0    0    0    0    0    0  \n4    0    0    0    0    0    0  \n\n[5 rows x 784 columns]\n\n\nThere are no missing values in the data.\n\nprint(np.isnan(train_images).any()) \n\nFalse\n\n\nThere appears to be no class imbalance\n\nunique_labels, counts = np.unique(train_labels, return_counts=True)\n\n# Print the counts sorted by label\nfor label, count in zip(unique_labels, counts):\n    print(f\"Label {label}: {count}\")\n\nLabel 0: 6000\nLabel 1: 6000\nLabel 2: 6000\nLabel 3: 6000\nLabel 4: 6000\nLabel 5: 6000\nLabel 6: 6000\nLabel 7: 6000\nLabel 8: 6000\nLabel 9: 6000\n\n\n\nprint(f\"X shape: {X.shape}\")\n\nX shape: (60000, 784)"
  },
  {
    "objectID": "slides2.html#t-sne-visualization-of-classes",
    "href": "slides2.html#t-sne-visualization-of-classes",
    "title": "Restricted Boltzmann Machines",
    "section": "t-SNE Visualization of Classes",
    "text": "t-SNE Visualization of Classes\nt-SNE Visualization t-distributed Stochastic Neighbor Embedding (t-SNE) is used here to visualize the separation between classes in a high-dimensional dataset. Each point represents a single fashion item (e.g., T-shirt, Trouser, etc.), and the color corresponds to its true label across the 10 categories listed above.\n\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\n# Run t-SNE to reduce dimensionality\n#embeddings = TSNE(n_jobs=2).fit_transform(X)\n\ntsne = TSNE(n_jobs=-1, random_state=42)  # Use -1 to use all available cores\nembeddings = tsne.fit_transform(X) #use scikitlearn instead\n\n\n# Create scatter plot\nfigure = plt.figure(figsize=(15,7))\nplt.scatter(embeddings[:, 0], embeddings[:, 1], c=train_labels,\n            cmap=plt.cm.get_cmap(\"jet\", 10), marker='.')\nplt.colorbar(ticks=range(10))\nplt.clim(-0.5, 9.5)\nplt.title(\"t-SNE Visualization of Fashion MNIST\")\nplt.show()\n\nWhat the visualization shows:  Class 1 (blue / Trousers) forms a clearly distinct and tightly packed cluster, indicating that the pixel patterns for trousers are less similar to those of other classes. In contrast, Classes 4 (Coat), 6 (Shirt), and 2 (Pullover) show significant overlap, suggesting that these clothing items are harder to distinguish visually and may lead to more confusion during classification."
  },
  {
    "objectID": "slides2.html#modeling-and-results",
    "href": "slides2.html#modeling-and-results",
    "title": "Restricted Boltzmann Machines",
    "section": "Modeling and Results",
    "text": "Modeling and Results\nOur Goal We are classifying Fashion MNIST images into one of 10 categories. To evaluate performance, we’re comparing five different models — some trained on raw pixel values and others using features extracted by a Restricted Boltzmann Machine (RBM). Our objective is to assess whether incorporating RBM into the workflow improves classification accuracy compared to using raw image data alone."
  },
  {
    "objectID": "slides2.html#our-models",
    "href": "slides2.html#our-models",
    "title": "Restricted Boltzmann Machines",
    "section": "Our Models",
    "text": "Our Models\n\nLogistic Regression on Fashion MNIST Data\nFeed Forward Network on Fashion MNIST Data 3. Convolutional Neural Network on Fashion MNIST Data 4. Logistic Regression on RBM Hidden Features (of Fashion MNIST Data) 5. Feed Forward Network on RBM Hidden Features (of Fashion MNIST Data)\n\nNote: Outputs (50 trials) and Code are below for each model. Both the code and output can be toggled by the reader. • The first click reveals a toggle labeled “Code”. • Clicking “Code” will show the output. • Clicking again will switch from output to the actual code. • Clicking “Show Code and Output” again will collapse both views.\n\n\n\n\n\n\nAkiba, Takuya, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. 2019. “Optuna: A Next-Generation Hyperparameter Optimization Framework.” In The 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2623–31.\n\n\nAslan, Narin, Sengul Dogan, and Gonca Ozmen Koca. 2023. “Automated Classification of Brain Diseases Using the Restricted Boltzmann Machine and the Generative Adversarial Network.” Engineering Applications of Artificial Intelligence 126: 106794.\n\n\nFiore, Ugo, Francesco Palmieri, Aniello Castiglione, and Alfredo De Santis. 2013. “Network Anomaly Detection with the Restricted Boltzmann Machine.” Neurocomputing 122: 13–23.\n\n\nFischer, Asja, and Christian Igel. 2012. “An Introduction to Restricted Boltzmann Machines.” In Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications: 17th Iberoamerican Congress, CIARP 2012, Buenos Aires, Argentina, September 3-6, 2012. Proceedings 17, 14–36. Springer.\n\n\nGoodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. MIT Press.\n\n\nHinton, Geoffrey E. 2002. “Training Products of Experts by Minimizing Contrastive Divergence.” Neural Computation 14 (8): 1771–1800.\n\n\nMelko, Roger G, Giuseppe Carleo, Juan Carrasquilla, and J Ignacio Cirac. 2019. “Restricted Boltzmann Machines in Quantum Physics.” Nature Physics 15 (9): 887–92.\n\n\nNing, Lin, Randall Pittman, and Xipeng Shen. 2018. “LCD: A Fast Contrastive Divergence Based Algorithm for Restricted Boltzmann Machine.” Neural Networks 108: 399–410.\n\n\nO’Shea, Keiron, and Ryan Nash. 2015. “An Introduction to Convolutional Neural Networks.” https://arxiv.org/abs/1511.08458.\n\n\nOh, Sangchul, Abdelkader Baggag, and Hyunchul Nha. 2020. “Entropy, Free Energy, and Work of Restricted Boltzmann Machines.” Entropy 22 (5): 538.\n\n\nPeng, Chao-Ying Joanne, Kuk Lida Lee, and Gary M Ingersoll. 2002. “An Introduction to Logistic Regression Analysis and Reporting.” The Journal of Educational Research 96 (1): 3–14.\n\n\nSalakhutdinov, Ruslan, Andriy Mnih, and Geoffrey Hinton. 2007. “Restricted Boltzmann Machines for Collaborative Filtering.” In Proceedings of the 24th International Conference on Machine Learning, 791–98.\n\n\nSazlı, Murat H. 2006. “A Brief Review of Feed-Forward Neural Networks.” Communications Faculty of Sciences University of Ankara Series A2-A3 Physical Sciences and Engineering 50 (01).\n\n\nSmolensky, Paul et al. 1986. “Information Processing in Dynamical Systems: Foundations of Harmony Theory.”\n\n\nXiao, Han, Kashif Rasul, and Roland Vollgraf. 2017. “Fashion-MNIST: A Novel Image Dataset for Benchmarking Machine Learning Algorithms.” August 28, 2017. https://arxiv.org/abs/cs.LG/1708.07747.\n\n\nZaharia, Matei, Andrew Chen, Aaron Davidson, Ali Ghodsi, Sue Ann Hong, Andy Konwinski, Siddharth Murching, et al. 2018. “Accelerating the Machine Learning Lifecycle with MLflow.” IEEE Data Eng. Bull. 41 (4): 39–45."
  },
  {
    "objectID": "slides2.html#simple-boltzmann-machine-example",
    "href": "slides2.html#simple-boltzmann-machine-example",
    "title": "Restricted Boltzmann Machines",
    "section": "Simple Boltzmann Machine Example",
    "text": "Simple Boltzmann Machine Example\n\n\nVideo"
  },
  {
    "objectID": "slides2.html#example-of-boltzmann-machine-reaching-equilibrium",
    "href": "slides2.html#example-of-boltzmann-machine-reaching-equilibrium",
    "title": "Restricted Boltzmann Machines",
    "section": "Example of Boltzmann Machine Reaching Equilibrium",
    "text": "Example of Boltzmann Machine Reaching Equilibrium\n\nVideo"
  },
  {
    "objectID": "slides2.html#example-of-boltzmann-machine-equilibrium",
    "href": "slides2.html#example-of-boltzmann-machine-equilibrium",
    "title": "Restricted Boltzmann Machines",
    "section": "Example of Boltzmann Machine Equilibrium",
    "text": "Example of Boltzmann Machine Equilibrium\n\nVideo"
  },
  {
    "objectID": "slides2.html#boltzmann-machine-equilibrium-example",
    "href": "slides2.html#boltzmann-machine-equilibrium-example",
    "title": "Restricted Boltzmann Machines",
    "section": "Boltzmann Machine Equilibrium Example",
    "text": "Boltzmann Machine Equilibrium Example\n\nVideo"
  },
  {
    "objectID": "slides2.html#use-of-boltzmann-machines",
    "href": "slides2.html#use-of-boltzmann-machines",
    "title": "Restricted Boltzmann Machines",
    "section": "Use of Boltzmann Machines",
    "text": "Use of Boltzmann Machines\n\nThe weights of a Boltzmann machine can be set to represent an optimization problem and then the stochastic update of neurons can be used to search the solution space\nMore commonly, the state vectors represent data for which training is performed to learn the weights\nThe goal of training is to find weights that define a Boltzmann distribution where the training vectors have high probability (low energy)"
  },
  {
    "objectID": "slides2.html#hidden-units-in-boltzmann-machines",
    "href": "slides2.html#hidden-units-in-boltzmann-machines",
    "title": "Restricted Boltzmann Machines",
    "section": "Hidden Units in Boltzmann Machines",
    "text": "Hidden Units in Boltzmann Machines\n\nBoltzmann machines can have hidden units, which are not directly observed or specified\nHidden units allow the model to learn latent features of the data"
  },
  {
    "objectID": "slides2.html#restricted-boltzmann-machines",
    "href": "slides2.html#restricted-boltzmann-machines",
    "title": "Restricted Boltzmann Machines",
    "section": "Restricted Boltzmann Machines",
    "text": "Restricted Boltzmann Machines\n\nBoltzmann machines can have hidden units, which are not directly observed or specified and allow the model to learn latent features of the data\nThe Restricted Boltzmann Machine (RBM) is a special case where there are no connections between visible units or between hidden units, which allows for efficient training and sampling"
  },
  {
    "objectID": "slides2.html#hidden-units-and-restricted-boltzmann-machines",
    "href": "slides2.html#hidden-units-and-restricted-boltzmann-machines",
    "title": "Restricted Boltzmann Machines",
    "section": "Hidden Units and Restricted Boltzmann Machines",
    "text": "Hidden Units and Restricted Boltzmann Machines\n\nBoltzmann machines can have hidden units, which are not directly observed or specified\nHidden units allow the model to learn latent features of the data\nThe Restricted Boltzmann Machine (RBM) is a special case where there are no connections between visible units or between hidden units\nThis restriction allows for efficient training and sampling"
  },
  {
    "objectID": "slides2.html#hidden-units-restricted-boltzmann-machines",
    "href": "slides2.html#hidden-units-restricted-boltzmann-machines",
    "title": "Restricted Boltzmann Machines",
    "section": "Hidden Units & Restricted Boltzmann Machines",
    "text": "Hidden Units & Restricted Boltzmann Machines\n\nBoltzmann machines can have hidden units, which are not directly observed or specified\nHidden units allow the model to learn latent features of the data\nThe Restricted Boltzmann Machine (RBM) is a special case where there are no connections between visible units or between hidden units\nThis restriction allows for efficient training and sampling"
  },
  {
    "objectID": "slides2.html#restricted-boltzmann-machines-1",
    "href": "slides2.html#restricted-boltzmann-machines-1",
    "title": "Restricted Boltzmann Machines",
    "section": "Restricted Boltzmann Machines",
    "text": "Restricted Boltzmann Machines"
  },
  {
    "objectID": "slides2.html#training-rbms-using-contrastive-divergence",
    "href": "slides2.html#training-rbms-using-contrastive-divergence",
    "title": "Restricted Boltzmann Machines",
    "section": "Training RBMs Using Contrastive Divergence",
    "text": "Training RBMs Using Contrastive Divergence\n\nContrastive Divergence (CD) is a method for training RBMs where the weights are updated to minimize the difference between samples from the data and samples from the model\nGibbs sampling is used to sample from the model, where the visible units are sampled from the model and then the hidden units are sampled from the visible units\nThe weights are updated using the difference between the data and model samples"
  },
  {
    "objectID": "slides.html#simple-boltzmann-machine-example",
    "href": "slides.html#simple-boltzmann-machine-example",
    "title": "Restricted Boltzmann Machines",
    "section": "Simple Boltzmann Machine Example",
    "text": "Simple Boltzmann Machine Example\n\n\nVideo\nUnits are updated stochastically based on the total input to the node"
  },
  {
    "objectID": "slides.html#boltzmann-machine-equilibrium-example",
    "href": "slides.html#boltzmann-machine-equilibrium-example",
    "title": "Restricted Boltzmann Machines",
    "section": "Boltzmann Machine Equilibrium Example",
    "text": "Boltzmann Machine Equilibrium Example\n\n\nVideo\nConvergence to a distribution where the states with lower energy are more likely"
  },
  {
    "objectID": "slides.html#use-of-boltzmann-machines",
    "href": "slides.html#use-of-boltzmann-machines",
    "title": "Restricted Boltzmann Machines",
    "section": "Use of Boltzmann Machines",
    "text": "Use of Boltzmann Machines\n\nThe weights of a Boltzmann machine can be set to represent an optimization problem and then the stochastic update of neurons can be used to search the solution space\nMore commonly, the state vectors represent data for which training is performed to learn the weights\nThe goal of training is to find weights that define a Boltzmann distribution where the training vectors have high probability (low energy)"
  },
  {
    "objectID": "slides.html#restricted-boltzmann-machines",
    "href": "slides.html#restricted-boltzmann-machines",
    "title": "Restricted Boltzmann Machines",
    "section": "Restricted Boltzmann Machines",
    "text": "Restricted Boltzmann Machines\n\nBoltzmann machines can have hidden units, which are not directly observed or specified and allow the model to learn latent features of the data\nThe Restricted Boltzmann Machine (RBM) is a special case where there are no connections between visible units or between hidden units, which allows for efficient training and sampling\n\n\n\n\n\n(Figure 1)"
  },
  {
    "objectID": "slides.html#training-rbms-using-contrastive-divergence",
    "href": "slides.html#training-rbms-using-contrastive-divergence",
    "title": "Restricted Boltzmann Machines",
    "section": "Training RBMs Using Contrastive Divergence",
    "text": "Training RBMs Using Contrastive Divergence\n\nContrastive Divergence (CD) is a method for training RBMs where the weights are updated to minimize the difference between samples from the data and samples from the model\nGibbs sampling is used to sample from the model, where the visible units are sampled from the model and then the hidden units are sampled from the visible units\nThe weights are updated using the difference between the data and model samples"
  },
  {
    "objectID": "slides.html",
    "href": "slides.html",
    "title": "Restricted Boltzmann Machines",
    "section": "",
    "text": "Boltzmann Machines are undirected graphical models where nodes are connected with symmetric weights\nTotal input is calculated for a node by \\[z_i = b_i + \\sum_j s_j w_{ij}\\] where \\(b_i\\) is the bias, \\(s_j\\) is the state of the connected node, and \\(w_{ij}\\) is the weight of the connection\nNodes are updated stochastically by sampling from the bernoulli distribution given by \\[p(s_i = 1) = \\frac{1}{1 + e^{-z_i}}\\] where \\(s_i\\) is the state of the node"
  }
]