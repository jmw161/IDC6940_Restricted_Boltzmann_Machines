{
  "hash": "8f835173ac93fb20ee608704fc5be611",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"test\"\neditor: visual\nformat:\n  html:\n    code-fold: true\nself-contained: true\nexecute: \n  warning: false\n  message: false\n  freeze: auto\n---\n\n::: {#d35235f9 .cell execution_count=1}\n``` {.python .cell-code}\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nimport numpy as np\nimport mlflow\nimport optuna\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom torch.utils.data import DataLoader\n\n# Set device\ndevice = torch.device(\"mps\")\n\n# Load Fashion-MNIST dataset again for the first 3 models\ntransform = transforms.Compose([transforms.ToTensor()])\ntrain_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\ntest_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n```\n:::\n\n\n<details>\n\n<summary>Click to Show Code and Output</summary>\n\n::: {#e62a84b9 .cell execution_count=2}\n``` {.python .cell-code}\nCLASSIFIER = \"LogisticRegression\"  # Change for FNN, LogisticRegression, or CNN\n\n# Set MLflow experiment name\nif CLASSIFIER == \"LogisticRegression\":\n    experiment = mlflow.set_experiment(\"pytorch-fmnist-lr-noRBM\")\nelif CLASSIFIER == \"FNN\":\n    experiment = mlflow.set_experiment(\"pytorch-fmnist-fnn-noRBM\")\nelif CLASSIFIER == \"CNN\":\n    experiment = mlflow.set_experiment(\"pytorch-fmnist-cnn-noRBM\")\n\n# Define CNN model\nclass FashionCNN(nn.Module):\n    def __init__(self, filters1, filters2, kernel1, kernel2):\n        super(FashionCNN, self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=filters1, kernel_size=kernel1, padding=1),\n            nn.BatchNorm2d(filters1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(in_channels=filters1, out_channels=filters2, kernel_size=kernel2),\n            nn.BatchNorm2d(filters2),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        self.fc1 = None\n        self.drop = nn.Dropout2d(0.25)\n        self.fc2 = nn.Linear(in_features=600, out_features=120)\n        self.fc3 = nn.Linear(in_features=120, out_features=10)\n        \n\n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = out.view(out.size(0), -1)\n        if self.fc1 is None:\n            self.fc1 = nn.Linear(out.shape[1], 600).to(x.device)\n        out = self.fc1(out)\n        out = self.drop(out)\n        out = self.fc2(out)\n        out = self.fc3(out)\n        return out\n\n\n\n        # Dynamically calculate flattened size\n        out = out.view(out.size(0), -1)  # Flatten\n        if self.fc1 is None:\n            self.fc1 = nn.Linear(out.shape[1], 600).to(x.device)  # âœ… Update FC layer dynamically\n\n        out = self.fc1(out)\n        out = self.drop(out)\n        out = self.fc2(out)\n        out = self.fc3(out)\n        return out\n\n\n\n\n# Define Optuna objective function\ndef objective(trial):\n    batch_size = trial.suggest_int(\"batch_size\", 64, 256, step=32)\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n    mlflow.start_run(experiment_id=experiment.experiment_id)\n    num_classifier_epochs = trial.suggest_int(\"num_classifier_epochs\", 5, 5) \n    mlflow.log_param(\"num_classifier_epochs\", num_classifier_epochs)\n\n    if CLASSIFIER == \"FNN\":\n        hidden_size = trial.suggest_int(\"fnn_hidden\", 192, 384)\n        learning_rate = trial.suggest_float(\"learning_rate\", 0.0001, 0.0025)\n\n        mlflow.log_param(\"classifier\", \"FNN\")\n        mlflow.log_param(\"fnn_hidden\", hidden_size)\n        mlflow.log_param(\"learning_rate\", learning_rate)\n\n        model = nn.Sequential(\n            nn.Linear(784, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, 10)\n        ).to(device)\n\n\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n    elif CLASSIFIER == \"CNN\":\n        filters1 = trial.suggest_int(\"filters1\", 16, 64, step=16)\n        filters2 = trial.suggest_int(\"filters2\", 32, 128, step=32)\n        kernel1 = trial.suggest_int(\"kernel1\", 3, 5)\n        kernel2 = trial.suggest_int(\"kernel2\", 3, 5)\n        learning_rate = trial.suggest_float(\"learning_rate\", 0.0001, 0.0025)\n\n        mlflow.log_param(\"classifier\", \"CNN\")\n        mlflow.log_param(\"filters1\", filters1)\n        mlflow.log_param(\"filters2\", filters2)\n        mlflow.log_param(\"kernel1\", kernel1)\n        mlflow.log_param(\"kernel2\", kernel2)\n        mlflow.log_param(\"learning_rate\", learning_rate)\n\n        model = FashionCNN(filters1, filters2, kernel1, kernel2).to(device)\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n      \n    elif CLASSIFIER == \"LogisticRegression\":\n        mlflow.log_param(\"classifier\", \"LogisticRegression\")\n    \n        # Prepare data for Logistic Regression (Flatten 28x28 images to 784 features)\n        train_features = train_dataset.data.view(-1, 784).numpy()\n        train_labels = train_dataset.targets.numpy()\n        test_features = test_dataset.data.view(-1, 784).numpy()\n        test_labels = test_dataset.targets.numpy()\n    \n        # Normalize the pixel values to [0,1] for better convergence\n        train_features = train_features / 255.0\n        test_features = test_features / 255.0\n    \n    \n        C = trial.suggest_float(\"C\", 0.01, 10.0, log=True)  \n        solver = \"saga\" \n    \n        model = LogisticRegression(C=C, max_iter=num_classifier_epochs, solver=solver)\n        model.fit(train_features, train_labels)\n    \n    \n        predictions = model.predict(test_features)\n        accuracy = accuracy_score(test_labels, predictions) * 100\n        print(f\"Logistic Regression Test Accuracy: {accuracy:.2f}%\")\n    \n        mlflow.log_param(\"C\", C)\n        mlflow.log_metric(\"test_accuracy\", accuracy)\n        mlflow.end_run()\n        return accuracy\n\n    # Training Loop for FNN and CNN\n    criterion = nn.CrossEntropyLoss()\n\n\n    model.train()\n    for epoch in range(num_classifier_epochs):\n        running_loss = 0.0\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images) if CLASSIFIER == \"CNN\" else model(images.view(images.size(0), -1))\n\n            optimizer.zero_grad()\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n\n        print(f\"{CLASSIFIER} Epoch {epoch+1}: loss = {running_loss / len(train_loader):.4f}\")\n\n    # Model Evaluation\n    model.eval()\n    correct, total = 0, 0\n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images) if CLASSIFIER == \"CNN\" else model(images.view(images.size(0), -1))\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    accuracy = 100 * correct / total\n    print(f\"Test Accuracy: {accuracy:.2f}%\")\n\n    mlflow.log_metric(\"test_accuracy\", accuracy)\n    mlflow.end_run()\n    return accuracy\n\nif __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=50)\n    print(f\"Best Parameters for {CLASSIFIER}:\", study.best_params)\n    print(\"Best Accuracy:\", study.best_value)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Test Accuracy: 84.59%\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Test Accuracy: 84.49%\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Test Accuracy: 84.49%\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Test Accuracy: 84.59%\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Test Accuracy: 84.57%\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Test Accuracy: 84.57%\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Test Accuracy: 84.58%\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Test Accuracy: 84.53%\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Test Accuracy: 84.56%\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Test Accuracy: 84.15%\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Test Accuracy: 84.41%\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Test Accuracy: 84.69%\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Test Accuracy: 84.48%\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Test Accuracy: 84.45%\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Test Accuracy: 84.49%\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Test Accuracy: 84.46%\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Test Accuracy: 84.33%\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Test Accuracy: 84.56%\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Test Accuracy: 84.42%\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Test Accuracy: 84.36%\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Test Accuracy: 84.63%\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Test Accuracy: 84.42%\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Test Accuracy: 84.68%\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Test Accuracy: 84.57%\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Test Accuracy: 84.48%\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Test Accuracy: 84.46%\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Test Accuracy: 84.47%\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Test Accuracy: 84.55%\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Test Accuracy: 84.62%\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Test Accuracy: 84.58%\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Test Accuracy: 84.52%\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Test Accuracy: 84.68%\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Test Accuracy: 84.58%\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Test Accuracy: 84.40%\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Test Accuracy: 84.52%\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Test Accuracy: 84.35%\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Test Accuracy: 84.52%\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Test Accuracy: 84.53%\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Test Accuracy: 84.55%\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Test Accuracy: 84.54%\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Test Accuracy: 84.57%\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Test Accuracy: 84.67%\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Test Accuracy: 84.49%\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Test Accuracy: 84.50%\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Test Accuracy: 84.46%\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Test Accuracy: 84.57%\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Test Accuracy: 84.45%\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Test Accuracy: 84.48%\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Test Accuracy: 84.44%\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Test Accuracy: 84.41%\nBest Parameters for LogisticRegression: {'batch_size': 96, 'num_classifier_epochs': 5, 'C': 0.35628734474915175}\nBest Accuracy: 84.69\n```\n:::\n:::\n\n\n</details>\n\n",
    "supporting": [
      "testrender_files"
    ],
    "filters": [],
    "includes": {}
  }
}