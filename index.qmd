---
title: "Restricted Boltzmann Machines"
subtitle: "Theory of RBMs and Applications"
author: "Jessica Wells and Jason Gerstenberger (Advisor: Dr. Cohen)"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: Capstone Projects in Data Science IDC 6940
university: University of West Florida, Pensacola, FL
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Introduction
Restricted Boltzmann Machines (RBM) are a type of neural network that has been around since the 1980s. As a reminder to the reader, machine learning is generally divided into 3 categories: supervised learning (examples: classification tasks, regression), unsupervised learning (examples: clustering, dimensionality reduction, generative modeling), and reinforcement learning (examples: gaming/robotics). RBMs are primarily used for unsupervised learning tasks like dimensionality reduction and feature extraction, which help prepare datasets for machine learning models that may later be trained using supervised learning. They also have other applications which will be discussed further later.

Like Hopfield networks, Boltzmann machines are undirected graphical models, but they are different in that they are stochastic and can have hidden units. Both models are energy-based, meaning they learn by minimizing an energy function for each model. Boltzmann machines use a sigmoid activation function, which allows for the model to be probabilistic.

In the "Restricted" Boltzmann Machine, there are no interactions between neurons in the visible layer or between neurons in the hidden layer, creating a bipartite graph of neurons. Below is a diagram taken from Goodfellow, et al. [@Goodfellow-et-al-2016] (p. 577) for visualization of the connections.

![](markov_net.png){width=60%}
<br>

Goodfellow, et al. discuss the expense in drawing samples for most undirected graphical models; however, the RBM allows for block Gibbs sampling (p. 578) where the network alternates between sampling all hidden units simultaneously (etc. for visible). Derivatives are also simplified by the fact that the energy function of the RBM is a linear function of it's parameters.  

$$
E(v,h) = - \sum_{i} a_i v_i - \sum_{j} b_j h_j - \sum_{i} \sum_{j} v_i w_{i,j} h_j
$$
where v_i and h_j represent visible and hidden units; a_i and b_j are the bia terms of the visible and hidden units; and each w_{i,j} (weight) element represents the interaction between the visible and hidden units.

RBMs are trained using a process called Contrastive Divergence (CD) [@hinton2002training] where the weights are updated to minimize the difference between samples from the data and samples from the model. Learning rate, batch size, and number of hidden units are all hyperparameters that can affect the ability of the training to converge successfully and learn the underlying structure of the data.

## Methods
<!-- 

-   Detail the models or algorithms used.

-   Justify your choices based on the problem and data.

*The common non-parametric regression model is*
$Y_i = m(X_i) + \varepsilon_i$*, where* $Y_i$ *can be defined as the sum
of the regression function value* $m(x)$ *for* $X_i$*. Here* $m(x)$ *is
unknown and* $\varepsilon_i$ *some errors. With the help of this
definition, we can create the estimation for local averaging i.e.*
$m(x)$ *can be estimated with the product of* $Y_i$ *average and* $X_i$
*is near to* $x$*. In other words, this means that we are discovering
the line through the data points with the help of surrounding data
points. The estimation formula is printed below [@R-base]:*

$$
M_n(x) = \sum_{i=1}^{n} W_n (X_i) Y_i  \tag{1}
$$$W_n(x)$ *is the sum of weights that belongs to all real numbers.
Weights are positive numbers and small if* $X_i$ *is far from* $x$*.*


*Another equation:*

$$
y_i = \beta_0 + \beta_1 X_1 +\varepsilon_i
$$

-->

## Analysis and Results

### Data Exploration and Visualization

<!-- 

-   Describe your data sources and collection process.

-   Present initial findings and insights through visualizations.

-   Highlight unexpected patterns or anomalies.

A study was conducted to determine how...

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
```

```{r, warning=FALSE, echo=TRUE}
# Load Data
kable(head(murders))

ggplot1 = murders %>% ggplot(mapping = aes(x=population/10^6, y=total)) 

  ggplot1 + geom_point(aes(col=region), size = 4) +
  geom_text_repel(aes(label=abb)) +
  scale_x_log10() +
  scale_y_log10() +
  geom_smooth(formula = "y~x", method=lm,se = F)+
  xlab("Populations in millions (log10 scale)") + 
  ylab("Total number of murders (log10 scale)") +
  ggtitle("US Gun Murders in 2010") +
  scale_color_discrete(name = "Region")+
      theme_bw()
  

```
-->


### Modeling and Results
<!--
-   Explain your data preprocessing and cleaning steps.

-   Present your key findings in a clear and concise manner.

-   Use visuals to support your claims.

-   **Tell a story about what the data reveals.**

```{r}

```
-->
### Conclusion

-   Summarize your key findings.

-   Discuss the implications of your results.

## References


