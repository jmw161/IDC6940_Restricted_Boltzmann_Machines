[
  {
    "objectID": "litreview.html",
    "href": "litreview.html",
    "title": "Literature Review",
    "section": "",
    "text": "The goal of the paper was to describe the mathematics and theory behind RBMs. First, the authors go into detail about how Boltzmann Machines are undirected graphical models (Markov Random Fields–MRFs) and explain the theory behind them. The important point being that the probability distribution is a complex Gibbs distribution and sampling from it can be difficult to solve without some restrictions applied. The restriction applied here is that the RBM is an MRF where the graph’s connections are only between the hidden and visible layers but NOT between any nodes/variables in the same layer (this is the restricted bit) which means hidden and visible variables are independent. The authors describe how this simplifies the Gibbs sampling: all variables in a layer can be sampled in a block instead of sampling new variables one by one. This increase in efficiency can allow scientists to apply RBMs to their dataset, getting optimal weights and biases for the RBM and use this information to later feed into a classifier. More specifically, the researchers discuss how a trained RBM or deep belief network (DBN) is a neural network where the units in the output layer represent labels that correspond to observations and then you can use this network for further training by standard supervised learning algorithms (page 15 of the article).\n\n\n\nThe article was very dense with a lot of probability theory probably not familiar to the average graduate student. This foundational overview though is necessary to understand how RBMs work for further analysis and application.\n\n\n\nThere was no real probelm solved here. They simply explain the theory behind the RBMs and some possible applications.\n\n\n\nNo real results; the article was a combination of the authors’ research. The biggest takeaway I had personally was my experience in training classifiers (supervised learning) is large datasets can cause them to train super slow. It’s almost like GPU is needed to run these classifiers. Like the article mentions, the RBM might be a great first step for a large dataset before feeding data to a classifier since the units in the output layer are basically like labels."
  },
  {
    "objectID": "litreview.html#goal-of-paper",
    "href": "litreview.html#goal-of-paper",
    "title": "Literature Review",
    "section": "",
    "text": "The goal of the paper was to describe the mathematics and theory behind RBMs. First, the authors go into detail about how Boltzmann Machines are undirected graphical models (Markov Random Fields–MRFs) and explain the theory behind them. The important point being that the probability distribution is a complex Gibbs distribution and sampling from it can be difficult to solve without some restrictions applied. The restriction applied here is that the RBM is an MRF where the graph’s connections are only between the hidden and visible layers but NOT between any nodes/variables in the same layer (this is the restricted bit) which means hidden and visible variables are independent. The authors describe how this simplifies the Gibbs sampling: all variables in a layer can be sampled in a block instead of sampling new variables one by one. This increase in efficiency can allow scientists to apply RBMs to their dataset, getting optimal weights and biases for the RBM and use this information to later feed into a classifier. More specifically, the researchers discuss how a trained RBM or deep belief network (DBN) is a neural network where the units in the output layer represent labels that correspond to observations and then you can use this network for further training by standard supervised learning algorithms (page 15 of the article)."
  },
  {
    "objectID": "litreview.html#why-is-the-article-important",
    "href": "litreview.html#why-is-the-article-important",
    "title": "Literature Review",
    "section": "",
    "text": "The article was very dense with a lot of probability theory probably not familiar to the average graduate student. This foundational overview though is necessary to understand how RBMs work for further analysis and application."
  },
  {
    "objectID": "litreview.html#how-was-the-problem-solvedmethods-used",
    "href": "litreview.html#how-was-the-problem-solvedmethods-used",
    "title": "Literature Review",
    "section": "",
    "text": "There was no real probelm solved here. They simply explain the theory behind the RBMs and some possible applications."
  },
  {
    "objectID": "litreview.html#resultslimitations",
    "href": "litreview.html#resultslimitations",
    "title": "Literature Review",
    "section": "",
    "text": "No real results; the article was a combination of the authors’ research. The biggest takeaway I had personally was my experience in training classifiers (supervised learning) is large datasets can cause them to train super slow. It’s almost like GPU is needed to run these classifiers. Like the article mentions, the RBM might be a great first step for a large dataset before feeding data to a classifier since the units in the output layer are basically like labels."
  },
  {
    "objectID": "litreview.html#goal-of-paper-1",
    "href": "litreview.html#goal-of-paper-1",
    "title": "Literature Review",
    "section": "Goal of Paper:",
    "text": "Goal of Paper:\nThe goal of the paper was to introduce RBMs as a tool used in solving complex wave functions (wave functions are used in quantum physics to explain features–postion, momentum, etc.–of a particle or group of particles). The authors describe how RBMs can learn an unknown probability distribution from a set of data pulled from that distribution (this was discussed in Research Article 1 as well). The goal in this training of RBMs is to find the optimal parameters (weights and biases) that minimize the energy functional. Traditionally, complex wave functions in many body complex quantum problems have been solved with tensor networks (TNs) but these can’t really work to solve systems that are subject to volume law entanglement vice area law. Entanglement here means particles in a system do not act independently; they’re movements are correlated. So area law entanglement means entropy of the system scales with area (etc. for volume). I was able to read more about area law on page 2 here (Eisert, Cramer, and Plenio 2008)"
  },
  {
    "objectID": "litreview.html#why-is-the-article-important-1",
    "href": "litreview.html#why-is-the-article-important-1",
    "title": "Literature Review",
    "section": "Why is the Article Important:",
    "text": "Why is the Article Important:\nThis article is important for anyone working in the field of physics interested in the possibility of applying RBMs to their problems. For graduate students, this article would likely interest chemistry and physics students more than data science students. I personally struggled way less with this article as my undergraduate degree in Biochemistry and Molecular Biology required a lot of physics and understanding of particle interactions and energy states. This article delved into some probability theory of course since the Markov Random Field/undirected graphical model makes up the Restricted Boltzmann Machine, but familiarity with the Research Article 1 will allow the graduate student to understand this Research Article 2 as well."
  },
  {
    "objectID": "litreview.html#how-was-the-problem-solvedmethods-used-1",
    "href": "litreview.html#how-was-the-problem-solvedmethods-used-1",
    "title": "Literature Review",
    "section": "How was the Problem Solved/Methods Used:",
    "text": "How was the Problem Solved/Methods Used:\nThe authors reference another article (Chen et al. 2018) reference 27, to explain how RBMs can be translated into tensor network states (TNS) which can allow for solving many-body quantum systems (which is where instead of assessing an individual particle, all particles in the system are assessed especially with regard to their correlations with each other). Wave functions are the answer to this and there are many many possible wave functions depending on the quantum state. The ability of the RBM to handle volume law entanglement problems makes them useful in quantum physics."
  },
  {
    "objectID": "litreview.html#resultslimitations-1",
    "href": "litreview.html#resultslimitations-1",
    "title": "Literature Review",
    "section": "Results/Limitations:",
    "text": "Results/Limitations:\nThe authors discuss on page 889 how although RBMs take advantage of the excellence of neural networks and machine learning algorithms; they also inherit the drawbacks. The authors discuss optimization problems: basically, how many layers should the network have and other network-architecture questions that properly explain the physics problem at hand. An inefficient network architecture means a less-than-optimal algorithm for solving a cost function/minimizing energy states."
  },
  {
    "objectID": "litreview.html#goal-of-paper-2",
    "href": "litreview.html#goal-of-paper-2",
    "title": "Literature Review",
    "section": "Goal of Paper:",
    "text": "Goal of Paper:\nThe goal of the paper was to share their model (they refer to as R-E-BLS) and it’s superiority in predictive analysis tasks. The authors first explain the drawbacks of deep neural networks, explaining that building them deeply seriously increases computation time. Random vector function linked neural networks–and their successor, broad learning systems(BLS)–help by building the network wider instead of deeper. The Echo State Netowrk (ESN) contains a sparse reservoir (this contrasts a dense reservoir ex: fully connected network where every neuron in the reservoir is connected to every other neuron by having non zero weights). The authors introduce Broad Echo State Networks (BESNs) which combines broad learning system and echo state networks. Finally, they extend the thought further for time series forecasting problems by suggesting use of RBM in mapping layer, ESN in enhancement layer (R-E-BLS). They show the model and run experiments on it on 3 separate datasets. They explain issues with current time series forecasting like Moving Average (MA) and Autoregressive Moving Average (ARMA) where there is high dependence on linearity. They describe how the data input into the RBM mapping layer where RBM generates mapping nodes. The number of feature mapping nodes can be adjusted by adjusting network parameters of the RBM (connection weights between visible and hidden units, bias of visible unit, bias of hidden unit). Final output combines mapping and enhancement layers. The authors then discuss a huge advantage of R-E-BLS is the ability to incrementally learn (train only feature nodes or input data that needs to be added to the model without rebuilding the entire network (like standard deep learning networks))."
  },
  {
    "objectID": "litreview.html#why-is-the-article-important-2",
    "href": "litreview.html#why-is-the-article-important-2",
    "title": "Literature Review",
    "section": "Why is the Article Important:",
    "text": "Why is the Article Important:\nThe article explains how deep learning networks have such profound prediction capability, but often experience high computational load. The authors explain how using RBM prior to ESN in a broad learning system can increase predication accuracy while reducing computational workload. This is important to anyone seeking a strong predictive model with high performance without debilitating computational load."
  },
  {
    "objectID": "litreview.html#how-was-the-problem-solvedmethods-used-2",
    "href": "litreview.html#how-was-the-problem-solvedmethods-used-2",
    "title": "Literature Review",
    "section": "How was the Problem Solved/Methods Used:",
    "text": "How was the Problem Solved/Methods Used:\nThe authors used 3 datasets (air quality index, pm2.5, and electric power load) and tested various models (their R-E-BLS, LSTM, GRU, ESN, etc.) for predictive analysis (predictive boxplots, prediction error of model, and error scatter plots). They showed consistently that R-E-BLS outperforms with all 3 measures for all 3 datasets. In addition, they found the R-E-BLS fits the true data more closely even with datasets with serious fluctuations in data like the pm2.5 and power load datasets."
  },
  {
    "objectID": "litreview.html#resultslimitations-2",
    "href": "litreview.html#resultslimitations-2",
    "title": "Literature Review",
    "section": "Results/Limitations:",
    "text": "Results/Limitations:\nNone mentioned or alluded to in the article. However, future ideas were mentioned. The authors said they would like to apply the model to multi-column data prediction tasks in the future."
  },
  {
    "objectID": "litreview.html#goal-of-paper-3",
    "href": "litreview.html#goal-of-paper-3",
    "title": "Literature Review",
    "section": "Goal of Paper:",
    "text": "Goal of Paper:\nOverall, their goal was to test the Discriminative Restricted Boltzmann Machine (DRBM) on it’s capability to distinguish normal from anomalous network traffic. The authors discuss the difficulty in network anomaly detection where anomaly is “unusual” meaning non-normal traffic patterns that could be indicative of an attack. They describe the difficulty of obtaining datasets for supervised classification as many clients are reluctant to divulge information that could expose the internal structure of their networks and how this has basically led to slow progress in the industry of predicting anomalous network activity in general. The authors are testing whether or not there’s enough similarity in normal network behaviors for a model to learn all nuisances of normal traffic when faced with unseen anomalous network traffic. This led them to to the DRBM because of it’s classification ability with all the power of a generative model."
  },
  {
    "objectID": "litreview.html#why-is-the-article-important-3",
    "href": "litreview.html#why-is-the-article-important-3",
    "title": "Literature Review",
    "section": "Why is the Article Important:",
    "text": "Why is the Article Important:\nNetwork security is a very big deal. Administrators need to be able to prepare for zero day attacks by understanding what traffic patterns are normal and which are anomalous. By having a model that correctly predicts this, clients can protect their networks."
  },
  {
    "objectID": "litreview.html#how-was-the-problem-solvedmethods-used-3",
    "href": "litreview.html#how-was-the-problem-solvedmethods-used-3",
    "title": "Literature Review",
    "section": "How was the Problem Solved/Methods Used:",
    "text": "How was the Problem Solved/Methods Used:\nThe authors describe semi-supervised anomaly detection as useful because the classifier can be trained on the normal class so that anomalous events can be detected without having to know what they look like. However, this can cause a lot of misclassification where normal events are incorrectly classified as anomalous. Their experiment involves two datasets: a real network traffic dataset with two hosts (one with normal traffic and one with traffic infected by a bot) and the KDD ’99 training dataset which was tested against the real data. They used 28 features related to network traffic in the training and used accuracy, speed, comprehensibility, and time to learn as evaluation parameters. The free energy patterns seen in the diagrams of normal and anomalous activity show the RBM was able to distinguish between the two."
  },
  {
    "objectID": "litreview.html#resultslimitations-3",
    "href": "litreview.html#resultslimitations-3",
    "title": "Literature Review",
    "section": "Results/Limitations:",
    "text": "Results/Limitations:\nThe authors discuss that when a classifier is tested in a network vastly different from the one it was trained on, performance declines. For the DRBM which can learn directly from data without relying on a distribution, it can decipher new network traffic traffic; however, the downfall of not having a distribution means the model depends heavily on the training data and can overfit (explaining why performance declines on different network traffic). They noticed a significant drop in performance when training with the KDD vice the real dataset."
  },
  {
    "objectID": "litreview.html#goal-of-paper-4",
    "href": "litreview.html#goal-of-paper-4",
    "title": "Literature Review",
    "section": "Goal of Paper:",
    "text": "Goal of Paper:\nThe overall goal was to show how RBM-GAN (Restricted Boltzmann Machine Generative Adversarial Network) can significantly improve classification performance of MRI images, especially of the brain. The authors first discuss all the various applications of GAN to MRI imaging studies and the use of RBMs with deep learning to classify brain diseases. The diseases of the brain studied here are: brain atrophy, ischemia, and white matter density with a control group of a normal brain. The first step is data augmentation and resizing of the brain MR images, then normalization. Then, the RBM extracts features of only the brain region and feeds this input to the GAN generator and the pre-processed real image data are fed to the GAN discriminator where the generator tries to generate fake data similar to the real data it’s learned and the discriminator chooses what data is real and fake (these two parts of the GAN are trained simultaneously). After training with GAN, several classifiers were tested (tree, linear discriminant, naiive bayes, SVM, KNN, ensemble, neural network and K-mean)."
  },
  {
    "objectID": "litreview.html#why-is-the-article-important-4",
    "href": "litreview.html#why-is-the-article-important-4",
    "title": "Literature Review",
    "section": "Why is the Article Important:",
    "text": "Why is the Article Important:\nIt shows future physicians and/or individuals studying MRI images that applying RBM before GAN can significantly improve overall classification performance. They may additionally learn that SVM worked best for final classification."
  },
  {
    "objectID": "litreview.html#how-was-the-problem-solvedmethods-used-4",
    "href": "litreview.html#how-was-the-problem-solvedmethods-used-4",
    "title": "Literature Review",
    "section": "How was the Problem Solved/Methods Used:",
    "text": "How was the Problem Solved/Methods Used:\nThere wasn’t much of a problem to be solved as the Kaggle dataset they used was pre-labeled, but it allowed the researchers to test the hypothesis of RBM improving the overall classification process by selecting the most important features."
  },
  {
    "objectID": "litreview.html#resultslimitations-4",
    "href": "litreview.html#resultslimitations-4",
    "title": "Literature Review",
    "section": "Results/Limitations:",
    "text": "Results/Limitations:\nThey found native GAN significantly under-performed RBM-GAN and that SVM was the best classifier. The limitations discussed are that the data is from a single medical center and a small database. They also discussed hyperparameters were set to default and there’s perhaps room for optimization by adjusting hyperparameters."
  },
  {
    "objectID": "litreview.html#goal-of-paper-5",
    "href": "litreview.html#goal-of-paper-5",
    "title": "Literature Review",
    "section": "Goal of Paper:",
    "text": "Goal of Paper:\nEven with GPU, RBMs are still pretty slow during calculation of vector dot products which is done to compute the activation probability. Lean Contrastive Divergence (LCD) adds two optimization techniques which speed up the progress significantly, as discussed in results. The two optimization techniques are as follows. 1. Bounds-based filtering: uses lower and upper bounds of P(hj = 1, given v) to select a range of dot products to perform, avoiding any unnecessary dot products. The conservative bounds are found with triangle inequality. 2. Delta product: uses only the necessary operations in calculating dot products. The authors discuss how during RBM training, the network updates states/neurons across the Gibbs sampling steps/epochs. Toward the beginning, some neurons switch states (flip between 0 and 1) but later in training, less units flip states. They took advantage of the fact that many don’t flip states by realizing the non-flipped units are repeating computations unnecessarily. So, they only re-calculate the changed parts of the dot product. In Gaussian-Bernoulli RBM, visible units are real values while hidden units are binary. The authors used bounds based filtered on the hidden units and delta product to sample visible units."
  },
  {
    "objectID": "litreview.html#why-is-the-article-important-5",
    "href": "litreview.html#why-is-the-article-important-5",
    "title": "Literature Review",
    "section": "Why is the Article Important:",
    "text": "Why is the Article Important:\nRBMs are widely used in training deep belief networks which are becoming increasingly popular. The downside of RBMs is the computational expense during the process of calculating vector dot products during contrastive divergence. The authors propose the LCD with it’s two optimizing techniques (bounds-based filtering and delta product) to speed up this process."
  },
  {
    "objectID": "litreview.html#how-was-the-problem-solvedmethods-used-5",
    "href": "litreview.html#how-was-the-problem-solvedmethods-used-5",
    "title": "Literature Review",
    "section": "How was the Problem Solved/Methods Used:",
    "text": "How was the Problem Solved/Methods Used:\nThey use seven public datasets to test their theory that LCD can significantly speed up RBM training since it allows skipping of some calculations of vector dot products. They also solved an issue where although GPUs are good at handling regular calculations, LCD doesn’t work great with GPU. Because of this, the authors implemented two things. 1) Aggregated warp filterings. The RBM has to keep track of which nodes flipped states during iterations. Each thread checks if a difference was detected (neuron flipped states between iterations). Warps are groups of threads (ex: NVIDIA GPU a warp is 32 threads). Aggregated warp filterings means only the warp leader (first active thread in the warp) chooses where the writing is done in the array. Then the other threads write their differences in the locations the warp leader chose. This is all done in parallel, avoiding thread conflicts. ## Storing extra copies of W transpose. GPUs are bandwidth sensitive and in RBMs, the W matrix is used in sample the visible layer and it’s transpose is used in sampling the hidden layer so the authors store a copy of W transpose in it’s own array so it doesn’t need to be recomputed."
  },
  {
    "objectID": "litreview.html#resultslimitations-5",
    "href": "litreview.html#resultslimitations-5",
    "title": "Literature Review",
    "section": "Results/Limitations:",
    "text": "Results/Limitations:\nThe results show that LCD speeds up the training of RBM on GPU by 2–3X. No limitations were discussed or alluded to."
  },
  {
    "objectID": "litreview.html#goal-of-the-paper",
    "href": "litreview.html#goal-of-the-paper",
    "title": "Literature Review",
    "section": "Goal of the paper",
    "text": "Goal of the paper\nThe paper explains how Restricted Boltzmann Machines can be trained to predict user ratings in collaborative filtering. It details the training of such a model for predicting movie ratings using a large dataset of user ratings from Netflix."
  },
  {
    "objectID": "litreview.html#why-is-it-important",
    "href": "litreview.html#why-is-it-important",
    "title": "Literature Review",
    "section": "Why is it important?",
    "text": "Why is it important?\nCollaborative filtering is an important technique for recommendation systems and models at the time of the paper were not able to handle well the large datasets that were becoming available."
  },
  {
    "objectID": "litreview.html#how-is-it-solved-methods",
    "href": "litreview.html#how-is-it-solved-methods",
    "title": "Literature Review",
    "section": "How is it solved? – methods",
    "text": "How is it solved? – methods\nRestricted Boltzmann Machines are used, which are a type of neural network with one visible layer and one hidden layer. All nodes in the visible layer are connected to all nodes in the hidden layer, but there are no connections between nodes in the same layer. The contrastive divergence algorithm is used to train the model and works by updating the weights of the connections between the visible and hidden layers using an estimate of the gradient. The model also uses conditional RBMs to incorporate information about which movies a user has rated."
  },
  {
    "objectID": "litreview.html#resultslimitations-if-any",
    "href": "litreview.html#resultslimitations-if-any",
    "title": "Literature Review",
    "section": "Results/limitations, if any",
    "text": "Results/limitations, if any\nThe RBM model is only slightly better than SVD on the Netflix dataset, but since its errors are different from SVD, it can be combined with SVD to improve predictions."
  },
  {
    "objectID": "litreview.html#goal-of-the-paper-1",
    "href": "litreview.html#goal-of-the-paper-1",
    "title": "Literature Review",
    "section": "Goal of the paper",
    "text": "Goal of the paper\nThe paper describes Products of Experts where distributions are combined through multiplication instead of the mixture of experts approach where distributions are combined through addition. The contrastive divergence algorithm is used to train the model."
  },
  {
    "objectID": "litreview.html#why-is-it-important-1",
    "href": "litreview.html#why-is-it-important-1",
    "title": "Literature Review",
    "section": "Why is it important?",
    "text": "Why is it important?\nPoEs are able to model complex, high-dimensional data distributions. Since an RBM is a PoE with one expert per hidden unit, the contrastive divergence algorithm can be used to train RBMs."
  },
  {
    "objectID": "litreview.html#how-is-it-solved-methods-1",
    "href": "litreview.html#how-is-it-solved-methods-1",
    "title": "Literature Review",
    "section": "How is it solved? – methods",
    "text": "How is it solved? – methods\nTo train with contrastive divergence, one starts by setting the visible units to a training example and calculating hidden unit values based on randomized weights. Then, the inputs are reconstructed from the hidden units using sampling and the hidden unit values are re-computed from the reconstruction. Weights are incremented between active inputs and active hidden units for the real data and are decremented for the reconstructed data."
  },
  {
    "objectID": "litreview.html#resultslimitations-if-any-1",
    "href": "litreview.html#resultslimitations-if-any-1",
    "title": "Literature Review",
    "section": "Results/limitations, if any",
    "text": "Results/limitations, if any\nThe CD algorithm is significantly faster than other training algorithms for RBMs. PoEs can effective model complex data distributions. However, CD is an approximation and may not always converge to the best possible model."
  },
  {
    "objectID": "litreview.html#goal-of-the-paper-2",
    "href": "litreview.html#goal-of-the-paper-2",
    "title": "Literature Review",
    "section": "Goal of the paper",
    "text": "Goal of the paper\nThe paper describes a learning algorithm for deep belief networks, which are essentially stacks of RBMs. The algorithm is based on the contrastive divergence algorithm used to train RBMs."
  },
  {
    "objectID": "litreview.html#why-is-it-important-2",
    "href": "litreview.html#why-is-it-important-2",
    "title": "Literature Review",
    "section": "Why is it important?",
    "text": "Why is it important?\nDeep belief networks are able to model complex data as the paper demonstrates in the case of MNIST digits."
  },
  {
    "objectID": "litreview.html#how-is-it-solved-methods-2",
    "href": "litreview.html#how-is-it-solved-methods-2",
    "title": "Literature Review",
    "section": "How is it solved? – methods",
    "text": "How is it solved? – methods\nThe algorithm trains deep belief networks by training each layer of the network as an RBM. The weights learned in the first layer are then used to initialize the weights of the second layer. This is repeated for each layer in the network. As such, this is an example of a “greedy” algorithm, with each layer receiving a different representation of the data."
  },
  {
    "objectID": "litreview.html#resultslimitations-if-any-2",
    "href": "litreview.html#resultslimitations-if-any-2",
    "title": "Literature Review",
    "section": "Results/limitations, if any",
    "text": "Results/limitations, if any\nThe DBN model trained in the paper achieves a lower error rate on the MNIST dataset than other models. However, the model does not learn to attend to the most informative parts of the image. The DBN shows how a generative model can learn low-level features with requiring labeled data."
  },
  {
    "objectID": "litreview.html#goal-of-the-paper-3",
    "href": "litreview.html#goal-of-the-paper-3",
    "title": "Literature Review",
    "section": "Goal of the paper",
    "text": "Goal of the paper\nThe paper introduces the Persistent Contrastive Divergence algorithm for training RBMs. This algorithm preserves the state of the Markov chain (Gibbs sampling) between training examples, which makes training faster."
  },
  {
    "objectID": "litreview.html#why-is-it-important-3",
    "href": "litreview.html#why-is-it-important-3",
    "title": "Literature Review",
    "section": "Why is it important?",
    "text": "Why is it important?\nThe PCD algorithm is faster than CD which allows for training of larger models and datasets."
  },
  {
    "objectID": "litreview.html#how-is-it-solved-methods-3",
    "href": "litreview.html#how-is-it-solved-methods-3",
    "title": "Literature Review",
    "section": "How is it solved? – methods",
    "text": "How is it solved? – methods\nThe PCD algorithm is similar to CD, but instead of starting from a random state for each training example, it uses the sample from the previous training example as the starting point for the next one. The paper trains RBM models on MNIST digits, email data, and images of horses used to test image segmentation. A mini batch of training examples is used to calculate the gradient for each update of the weights."
  },
  {
    "objectID": "litreview.html#resultslimitations-if-any-3",
    "href": "litreview.html#resultslimitations-if-any-3",
    "title": "Literature Review",
    "section": "Results/limitations, if any",
    "text": "Results/limitations, if any\nFor the models trained in the paper, PCD was able to train the models faster than CD and typically achieved better results. However, PCD is still an approximation and may not always converge to the best possible model. PCD also requires a low learning rate."
  },
  {
    "objectID": "litreview.html#goal-of-the-paper-4",
    "href": "litreview.html#goal-of-the-paper-4",
    "title": "Literature Review",
    "section": "Goal of the paper",
    "text": "Goal of the paper\nSmolensky sought to encourage the exploration of mathematical analysis in the field of cognitive science, which he referenced as the subsymbolic paradigm, in contrast to the predominant focus on symbolic processing at the time. He bridged the two paradigms by demonstrating how graphical models could represent symbolic information."
  },
  {
    "objectID": "litreview.html#why-is-it-important-4",
    "href": "litreview.html#why-is-it-important-4",
    "title": "Literature Review",
    "section": "Why is it important?",
    "text": "Why is it important?\nThe harmonium model described in the paper is essentially a restricted Boltzmann machine and the harmony measure parallels the concept of energy in the Boltzmann machine. This paper encouraged further investigation into physics-based models of cognition and the discovery of more efficient learning algorithms for neural networks. The paper also reinforced the idea that effective models would possess information in the proability distribution of the data."
  },
  {
    "objectID": "litreview.html#how-is-it-solved---methods",
    "href": "litreview.html#how-is-it-solved---methods",
    "title": "Literature Review",
    "section": "How is it solved? - methods",
    "text": "How is it solved? - methods\nThe harmonium model is a bipartite graph with visible and hidden units, called representational features and knowledge atoms. A Hebbian learning rule is used to update the weights between the visible and hidden units, increasing the weights when both units are active and decreasing them when one is active and the other is not."
  },
  {
    "objectID": "litreview.html#resultslimitations-if-any-4",
    "href": "litreview.html#resultslimitations-if-any-4",
    "title": "Literature Review",
    "section": "Results/limitations, if any",
    "text": "Results/limitations, if any\nThe harmonium model is applied to some relatively trivial examples in the paper. Only later with the creation of the contrastive divergence algorithm was it possible to train RBMs on more complex data."
  },
  {
    "objectID": "litreview.html#goal-of-the-paper-5",
    "href": "litreview.html#goal-of-the-paper-5",
    "title": "Literature Review",
    "section": "Goal of the paper",
    "text": "Goal of the paper\nThe paper provides practical guidance on training Restricted Boltzmann Machines, including methods for maximizing the efficiency of the learning algorithm and choosing useful hyperparameter values."
  },
  {
    "objectID": "litreview.html#why-is-it-important-5",
    "href": "litreview.html#why-is-it-important-5",
    "title": "Literature Review",
    "section": "Why is it important?",
    "text": "Why is it important?\nRBM models are able to learn complex data distributions and can be used for a variety of tasks, including collaborative filtering and image recognition. However, without careful training procedures and hyperparameter selection, the models may not perform well."
  },
  {
    "objectID": "litreview.html#how-is-it-solved---methods-1",
    "href": "litreview.html#how-is-it-solved---methods-1",
    "title": "Literature Review",
    "section": "How is it solved? - methods",
    "text": "How is it solved? - methods\nThe paper provides guidance on how to effectively use contrastive divergence and update the weights of the model during training. It describes the considerations for choosing the size of mini-batches and the number of hidden units in the model. It provides details on choosing an initial learning rate and how to adjust it during training, and also how to use momentum to speed up training."
  },
  {
    "objectID": "litreview.html#resultslimitations-if-any-5",
    "href": "litreview.html#resultslimitations-if-any-5",
    "title": "Literature Review",
    "section": "Results/limitations, if any",
    "text": "Results/limitations, if any\nAs this paper focuses on training methods, it does not present any new results. It discusses problems such as hidden units being stuck with extremely small weights and overfitting and suggests methods for addressing these issues."
  },
  {
    "objectID": "slides.html#introduction",
    "href": "slides.html#introduction",
    "title": "Present a great story for data science projects",
    "section": "Introduction",
    "text": "Introduction\n\nDevelop a storyline that captures attention and maintains interest.\nYour audience is your peers\nClearly state the problem or question you’re addressing.\nIntroduce why it is relevant needs.\nProvide an overview of your approach.\n\nIn kernel estimator, weight function is known as kernel function (efr2008?). Cite this paper (bro2014principal?). The GEE (wang2014?). The PCA (daffertshofer2004pca?)*"
  },
  {
    "objectID": "slides.html#methods",
    "href": "slides.html#methods",
    "title": "Present a great story for data science projects",
    "section": "Methods",
    "text": "Methods\n\nDetail the models or algorithms used.\nJustify your choices based on the problem and data."
  },
  {
    "objectID": "slides.html#data-exploration-and-visualization",
    "href": "slides.html#data-exploration-and-visualization",
    "title": "Present a great story for data science projects",
    "section": "Data Exploration and Visualization",
    "text": "Data Exploration and Visualization\n\nDescribe your data sources and collection process.\nPresent initial findings and insights through visualizations.\nHighlight unexpected patterns or anomalies."
  },
  {
    "objectID": "slides.html#data-exploration-and-visualization-1",
    "href": "slides.html#data-exploration-and-visualization-1",
    "title": "Present a great story for data science projects",
    "section": "Data Exploration and Visualization",
    "text": "Data Exploration and Visualization\nA study was conducted to determine how…"
  },
  {
    "objectID": "slides.html#modeling-and-results",
    "href": "slides.html#modeling-and-results",
    "title": "Present a great story for data science projects",
    "section": "Modeling and Results",
    "text": "Modeling and Results\n\nExplain your data preprocessing and cleaning steps.\nPresent your key findings in a clear and concise manner.\nUse visuals to support your claims.\nTell a story about what the data reveals."
  },
  {
    "objectID": "slides.html#conclusion",
    "href": "slides.html#conclusion",
    "title": "Present a great story for data science projects",
    "section": "Conclusion",
    "text": "Conclusion\n\nSummarize your key findings.\nDiscuss the implications of your results."
  },
  {
    "objectID": "slides.html#references",
    "href": "slides.html#references",
    "title": "Present a great story for data science projects",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Restricted Boltzmann Machines",
    "section": "",
    "text": "Restricted Boltzmann Machines (RBM) are a type of neural network that has been around since the 1980s. As a reminder to the reader, machine learning is generally divided into 3 categories: supervised learning (examples: classification tasks, regression), unsupervised learning (examples: clustering, dimensionality reduction, generative modeling), and reinforcement learning (examples: gaming/robotics). RBMs are primarily used for unsupervised learning tasks like dimensionality reduction and feature extraction, which help prepare datasets for machine learning models that may later be trained using supervised learning. They also have other applications which will be discussed further later.\nLike Hopfield networks, Boltzmann machines are undirected graphical models, but they are different in that they are stochastic and can have hidden units. Both models are energy-based, meaning they learn by minimizing an energy function for each model (Smolensky et al. 1986). Boltzmann machines use a sigmoid activation function, which allows for the model to be probabilistic.\nIn the “Restricted” Boltzmann Machine, there are no interactions between neurons in the visible layer or between neurons in the hidden layer, creating a bipartite graph of neurons. Below is a diagram taken from Goodfellow, et al. (Goodfellow, Bengio, and Courville 2016) (p. 577) for visualization of the connections.\n\n\nCode\nreticulate::py_config()\n\n\npython:         /Users/jessicawells/.virtualenvs/r-reticulate/bin/python\nlibpython:      /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/config-3.12-darwin/libpython3.12.dylib\npythonhome:     /Users/jessicawells/.virtualenvs/r-reticulate:/Users/jessicawells/.virtualenvs/r-reticulate\nversion:        3.12.6 (v3.12.6:a4a2d2b0d85, Sep  6 2024, 16:08:03) [Clang 13.0.0 (clang-1300.0.29.30)]\nnumpy:          /Users/jessicawells/.virtualenvs/r-reticulate/lib/python3.12/site-packages/numpy\nnumpy_version:  1.26.1\n\n\n \nGoodfellow, et al. discuss the expense in drawing samples for most undirected graphical models; however, the RBM allows for block Gibbs sampling (p. 578) where the network alternates between sampling all hidden units simultaneously (etc. for visible). Derivatives are also simplified by the fact that the energy function of the RBM is a linear function of it’s parameters, which will be seen further in Methods.\nRBMs are trained using a process called Contrastive Divergence (CD) (G. E. Hinton 2002) where the weights are updated to minimize the difference between samples from the data and samples from the model. Learning rate, batch size, and number of hidden units are all hyperparameters that can affect the ability of the training to converge successfully and learn the underlying structure of the data.\n\n\n\nRBMs are probably best known for their success in collaborative filtering. The RBM model was used in the Netflix Prize competition to predict user ratings for movies, with the result that it outperformed the Singular Value Decomposition (SVD) method that was state-of-the-art at the time (Salakhutdinov, Mnih, and Hinton 2007). They have also been trained to recognize handwritten digits, such as the MNIST dataset (G. E. Hinton 2002).\nRBMs have been successfully used to distinguish normal and anomalous network traffic. Their potential use in improving network security for companies in the future is promising. There is slow progress in network anomaly detection due to the difficulty of obtaining datasets for training and testing networks. Clients are often reluctant to divulge information that could potentially harm their networks. In a real-life dataset where one host had normal traffic and one was infected by a bot, discriminative RBM (DRBM) was able to successfully distinguish the normal from anomalous traffic. DRBM doesn’t rely on knowing the data distribution ahead of time, which is useful, except that it also causes the DRBM to overfit. As a result, when trying to use the same trained RBM on the KDD ’99 training dataset performance declined. (Fiore et al. 2013)\nRBMs can provide greatly improved classification of brain disorders in MRI images. Generative Adversarial Networks (GANs) use two neural networks: a generator which generates fake data, and a discriminator which tries to distinguish between real and fake data. Loss from the discriminator is backpropagated through the generator so that both part are trained simultaneously. The RBM-GAN uses RBM features from real MRI images as inputs to the generator. Features from the discriminator are then used as inputs to a classifier. (Aslan, Dogan, and Koca 2023)\nThe many-body quantum wavefunction, which describes the quantum state of a system of particles is difficult to compute with classical computers. RBMs have been used to approximate it using variational Monte Carlo methods. (Melko et al. 2019)\nRBMs are notoriously slow to train. The process of computing the activation probability requires the calculation of vector dot products. Lean Constrastive Divergence (LCD) is a method which adds two techniques to speed up the process of training RBMs. The first is bounds-based filtering where upper and lower bounds of the probability select only a range of dot products to perform. Second, the delta product involves only recalculating the changed portions of the vector dot product. (Ning, Pittman, and Shen 2018)"
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Restricted Boltzmann Machines",
    "section": "",
    "text": "Restricted Boltzmann Machines (RBM) are a type of neural network that has been around since the 1980s. As a reminder to the reader, machine learning is generally divided into 3 categories: supervised learning (examples: classification tasks, regression), unsupervised learning (examples: clustering, dimensionality reduction, generative modeling), and reinforcement learning (examples: gaming/robotics). RBMs are primarily used for unsupervised learning tasks like dimensionality reduction and feature extraction, which help prepare datasets for machine learning models that may later be trained using supervised learning. They also have other applications which will be discussed further later.\nLike Hopfield networks, Boltzmann machines are undirected graphical models, but they are different in that they are stochastic and can have hidden units. Both models are energy-based, meaning they learn by minimizing an energy function for each model (Smolensky et al. 1986). Boltzmann machines use a sigmoid activation function, which allows for the model to be probabilistic.\nIn the “Restricted” Boltzmann Machine, there are no interactions between neurons in the visible layer or between neurons in the hidden layer, creating a bipartite graph of neurons. Below is a diagram taken from Goodfellow, et al. (Goodfellow, Bengio, and Courville 2016) (p. 577) for visualization of the connections.\n\n\nCode\nreticulate::py_config()\n\n\npython:         /Users/jessicawells/.virtualenvs/r-reticulate/bin/python\nlibpython:      /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/config-3.12-darwin/libpython3.12.dylib\npythonhome:     /Users/jessicawells/.virtualenvs/r-reticulate:/Users/jessicawells/.virtualenvs/r-reticulate\nversion:        3.12.6 (v3.12.6:a4a2d2b0d85, Sep  6 2024, 16:08:03) [Clang 13.0.0 (clang-1300.0.29.30)]\nnumpy:          /Users/jessicawells/.virtualenvs/r-reticulate/lib/python3.12/site-packages/numpy\nnumpy_version:  1.26.1\n\n\n \nGoodfellow, et al. discuss the expense in drawing samples for most undirected graphical models; however, the RBM allows for block Gibbs sampling (p. 578) where the network alternates between sampling all hidden units simultaneously (etc. for visible). Derivatives are also simplified by the fact that the energy function of the RBM is a linear function of it’s parameters, which will be seen further in Methods.\nRBMs are trained using a process called Contrastive Divergence (CD) (G. E. Hinton 2002) where the weights are updated to minimize the difference between samples from the data and samples from the model. Learning rate, batch size, and number of hidden units are all hyperparameters that can affect the ability of the training to converge successfully and learn the underlying structure of the data.\n\n\n\nRBMs are probably best known for their success in collaborative filtering. The RBM model was used in the Netflix Prize competition to predict user ratings for movies, with the result that it outperformed the Singular Value Decomposition (SVD) method that was state-of-the-art at the time (Salakhutdinov, Mnih, and Hinton 2007). They have also been trained to recognize handwritten digits, such as the MNIST dataset (G. E. Hinton 2002).\nRBMs have been successfully used to distinguish normal and anomalous network traffic. Their potential use in improving network security for companies in the future is promising. There is slow progress in network anomaly detection due to the difficulty of obtaining datasets for training and testing networks. Clients are often reluctant to divulge information that could potentially harm their networks. In a real-life dataset where one host had normal traffic and one was infected by a bot, discriminative RBM (DRBM) was able to successfully distinguish the normal from anomalous traffic. DRBM doesn’t rely on knowing the data distribution ahead of time, which is useful, except that it also causes the DRBM to overfit. As a result, when trying to use the same trained RBM on the KDD ’99 training dataset performance declined. (Fiore et al. 2013)\nRBMs can provide greatly improved classification of brain disorders in MRI images. Generative Adversarial Networks (GANs) use two neural networks: a generator which generates fake data, and a discriminator which tries to distinguish between real and fake data. Loss from the discriminator is backpropagated through the generator so that both part are trained simultaneously. The RBM-GAN uses RBM features from real MRI images as inputs to the generator. Features from the discriminator are then used as inputs to a classifier. (Aslan, Dogan, and Koca 2023)\nThe many-body quantum wavefunction, which describes the quantum state of a system of particles is difficult to compute with classical computers. RBMs have been used to approximate it using variational Monte Carlo methods. (Melko et al. 2019)\nRBMs are notoriously slow to train. The process of computing the activation probability requires the calculation of vector dot products. Lean Constrastive Divergence (LCD) is a method which adds two techniques to speed up the process of training RBMs. The first is bounds-based filtering where upper and lower bounds of the probability select only a range of dot products to perform. Second, the delta product involves only recalculating the changed portions of the vector dot product. (Ning, Pittman, and Shen 2018)"
  },
  {
    "objectID": "index.html#methods",
    "href": "index.html#methods",
    "title": "Restricted Boltzmann Machines",
    "section": "Methods",
    "text": "Methods\nBelow is the energy function of the RBM.\n\\[\nE(v,h) = - \\sum_{i} a_i v_i - \\sum_{j} b_j h_j - \\sum_{i} \\sum_{j} v_i w_{i,j} h_j\n\\tag{1}\\] where vi and hj represent visible and hidden units; ai and bj are the bias terms of the visible and hidden units; and each w{i,j} (weight) element represents the interaction between the visible and hidden units. (Fischer and Igel 2012)\nIt is well known neural networks are prone to overfitting and often techniques such as early stopping are employed to prevent it. Some methods to prevent overfitting in RBMs are weight decay (L2 regularization), dropout, dropconnect, and weight uncertainty (Zhang et al. 2018). Dropout is a fairly well known concept in deep learning. For example, a dropout value of 0.3 added to a layer means 30% of neurons are dropped during training. This prevents the network from learning certain features too well. L2 regularization is also a commonly employed technique in deep learning. It assigns a penalty to large weights to allow for more generalization. Dropconnect is a method where a subset of weights within the network are randomly set to zero during training. Weight uncertainty is where each weight in the network has it’s own probability distribution vice a fixed value. This addition allows the network to learn more useful features.\nIf the learning rate is too high, training of the model may not converge. If it is too low, training may take a long time. To fully maximize the training of the model it is helpful to reduce the learning rate over time. This is known as learning rate decay. (G. Hinton 2010)\n\nModel Categories\nWe train Logistic Regression (with and without RBM features as input), Feed Forward Network (with and without RBM features as input), and Convolutional Neural Network. Below is a brief reminder of the basics of each model.\nFor the models incoroporating the RBM, we take the Fashion MNIST features/pixels and train the RBM (unsupervised learning) to extract hidden features from the visible layer and then feed these features into either logistic regression or feed forward network. We then use the trained model to predict labels for the test data, evaluating how well the RBM-derived features perform in a supervised classification task.\n\n1. Logistic Regression\nMathematically, the concept behind binary logistic regression is the logit (the natural logarithm of an odds ratio)(Peng, Lee, and Ingersoll 2002). However, since we have 10 labels, our classification task falls into “Multinomial Logistic Regression.”\n\\[\nP(Y = k | X) = \\frac{e^{\\beta_{0k} + \\beta_k^T X}}{\\sum_{l=1}^{K} e^{\\beta_{0l} + \\beta_l^T X}}\n\\tag{2}\\]\n\n\n2. Simple Feed Forward Neural Network\nThe feed forward network (FNN) is one where information flows in one direction from input to output with no loops or feedback. There can be zero hidden layers in between (called single FNN) or one or more hidden layers (multilayer FNN).  (Sazlı 2006) \n\n\n3. Convolutional Neural Network\nThe convolutional neural network (CNN) is a type of feed forward network except that unlike the traditional ANN, CNNs are primarily used for pattern recognition with images (O’Shea and Nash 2015). The CNN has 3 layers which are stacked to form the full CNN: convolutional, pooling, and fully-connected layers. \n\n\nBelow is our Process for creating the RBM:\nStep 1: We first initialize the RBM with random weights and biases and set visible units to 784 and hidden units to 256. We also set the number of contrastive divergence steps (k) to 1.  Step 2: Sample hidden units from visible. The math behind computing the hidden unit activations from the given input can be seen in Equation 3 (Fischer and Igel 2012) where the probability is used to sample from the Bernoulli distribution.  \\[\np(H_i = 1 | \\mathbf{v}) = \\sigma \\left( \\sum_{j=1}^{m} w_{ij} v_j + c_i \\right)\n\\tag{3}\\]\nwhere p(.) is the probability of the ith hidden state being activated (=1) given the visible input vector. σ is the sigmoid activation function (below) which maps the weighted sum to a probability between 0 and 1. m is the number of visible units. wij is the weight connecting visible unit j to hidden unit i. vj is the value of the jth visible unit. and ci is the bias term for the hidden unit. \\[\n\\sigma(x) = \\frac{1}{1 + e^{-x}}\n\\]\nStep 3: Sample visible units from hidden. The math behind computing visible unit activations from the hidden layer can be seen in Equation 4 (Fischer and Igel 2012) Visible states are sampled using the Bernoulli distribution. This way we can see how well the RBM learned from the inputs.  \\[\np(V_j = 1 | \\mathbf{h}) = \\sigma \\left( \\sum_{i=1}^{n} w_{ij} h_i + b_j \\right)\n\\tag{4}\\]\nwhere p(.) is the probability of the ith visible unit being activated (=1) given the hidden vector h. σ is same as above. n is the number of hidden units. wij is the weight connecting hidden unit i to visible unit j. bj is the bias term for the jth visible unit.\nStep 4: K=1 steps of Contrastive Divergence (Feed Forward, Feed Backward) which executes steps 2 and 3. Contrastive Divergence updates the RBM’s weights by minimizing the difference between the original input and the reconstructed input created by the RBM.  Step 5: Free energy is computed. The free energy F is given by the logarithm of the partition function Z (Oh, Baggag, and Nha 2020) where the partition function is  \\[\nZ(\\theta) \\equiv \\sum_{v,h} e^{-E(v,h; \\theta)}\n\\tag{5}\\] and the free energy function is  \\[\nF(\\theta) = -\\ln Z(\\theta)\n\\tag{6}\\] where lower free energy means the RBM learned the visible state well.\nStep 6: Train the RBM. Model weights updated via gradient descent. Step 7: Feature extraction for classification with LR. The hidden layer activations of the RBM are used as features for Logistic Regression and Feed Forward Network.\n\n\nHyperparameter Tuning\nWe use the Tree-structured Parzen Estimator algorithm from Optuna (Akiba et al. 2019) to tune the hyperparameters of the RBM and the classifier models, and we use MLFlow (Zaharia et al. 2018) to record and visualize the results of the hyperparameter tuning process. The hyperparameters we tune include the learning rate, batch size, number of hidden units, and number of epochs.\n\n\n\nMetrics Used\n1. Accuracy Accuracy is defined as the number of correct classifications divided by the total number of classifications \\[\n\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}\n\\]\n2. Macro F1 Score Macro F1 score is the unweighted average of the individual F1 scores of each class. It takes no regard for class imbalance; however, we saw earlier the classes are all balanced in Fashion MNIST. The F1 score for each individual class is as follows \\[\n\\text{F1} = \\frac{2 \\cdot \\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n\\] where precision for each class is \\[\n\\text{Precision} = \\frac{TP}{TP + FP}\n\\] and recall for each class is \\[\n\\text{Recall} = \\frac{TP}{TP + FN}\n\\] The definitions of these terms for multiclass problems are more complicated than binary and are best displayed as examples. \n\n\n\n\n\n\n\n\nAcronymn\nExample for a trouser image\n\n\n\n\nTP = True Positives\nthe image is a trouser and the model predicts a trouser\n\n\nTN = True Negatives\nthe image is not a trouser and the model predicts anything but trouser\n\n\nFP = False Positives\nthe image is anything but trouser but the model predicts trouser\n\n\nFN = False Negatives\nthe image is a trouser and the model predicts another class (like shirt)\n\n\n\nAs stated earlier, the individual F1 scores for each class are taken and averaged to compute the Macro F1 score in a multiclass problem like Fashion MNIST."
  },
  {
    "objectID": "index.html#analysis-and-results",
    "href": "index.html#analysis-and-results",
    "title": "Restricted Boltzmann Machines",
    "section": "Analysis and Results",
    "text": "Analysis and Results\n\nData Exploration and Visualization\nWe use the Fashion MNIST dataset from Zalando Research (Xiao, Rasul, and Vollgraf 2017). The set includes 70,000 grayscale images of clothing items, 60,000 for training and 10,000 for testing. Each image is 28x28 pixels (784 pixels total). Each pixel has a value associated with it ranging from 0 (white) to 255 (very dark) – whole numbers only. There are 785 columns in total as one column is dedicated to the label.\n \nThere are 10 labels in total:\n0 T-shirt/top 1 Trouser 2 Pullover 3 Dress 4 Coat 5 Sandal 6 Shirt 7 Sneaker 8 Bag 9 Ankle boot\nBelow we load the dataset.\n\n\nCode\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nimport torch\nimport torchvision.datasets\nimport torchvision.models\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\n\n\n\ntrain_data = torchvision.datasets.FashionMNIST(\n    root=\"./data\", \n    train=True, \n    download=True, \n    transform=transforms.ToTensor()  # Converts to tensor but does NOT normalize\n)\n\ntest_data = torchvision.datasets.FashionMNIST(\n    root=\"./data\", \n    train=False, \n    download=True, \n    transform=transforms.ToTensor()  \n)\n\n\nGet the seventh image to show a sample\n\n\nCode\n# Extract the first image (or choose any index)\nimage_tensor, label = train_data[6]  # shape: [1, 28, 28]\n\n# Convert tensor to NumPy array\nimage_array = image_tensor.numpy().squeeze()  \n\n# Plot the image\nplt.figure(figsize=(5,5))\nplt.imshow(image_array, cmap=\"gray\")\nplt.title(f\"FashionMNIST Image (Label: {label})\")\nplt.axis(\"off\")  # Hide axes\n\n\n(-0.5, 27.5, 27.5, -0.5)\n\n\nCode\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\ntrain_images = train_data.data.numpy()  # Raw pixel values (0-255)\ntrain_labels = train_data.targets.numpy()\nX = train_images.reshape(-1, 784)  # Flatten 28x28 images into 1D (60000, 784)\n\n\n\n\nCode\n#print(train_images[:5])\nflattened = train_images[:5].reshape(5, -1) \n\n# Create a DataFrame\ndf_flat = pd.DataFrame(flattened)\nprint(df_flat.head())\n\n\n   0    1    2    3    4    5    6    ...  777  778  779  780  781  782  783\n0    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n1    0    0    0    0    0    1    0  ...   76    0    0    0    0    0    0\n2    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n3    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n4    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n\n[5 rows x 784 columns]\n\n\nCode\n#train_df.info() #datatypes are integers\n\n\nThere are no missing values in the data.\n\n\nCode\nprint(np.isnan(train_images).any()) \n\n\nFalse\n\n\nThere appears to be no class imbalance\n\n\nCode\nunique_labels, counts = np.unique(train_labels, return_counts=True)\n\n# Print the counts sorted by label\nfor label, count in zip(unique_labels, counts):\n    print(f\"Label {label}: {count}\")\n\n\nLabel 0: 6000\nLabel 1: 6000\nLabel 2: 6000\nLabel 3: 6000\nLabel 4: 6000\nLabel 5: 6000\nLabel 6: 6000\nLabel 7: 6000\nLabel 8: 6000\nLabel 9: 6000\n\n\n\n\nCode\nprint(f\"X shape: {X.shape}\")\n\n\nX shape: (60000, 784)\n\n\nt-SNE Visualization t-distributed Stochastic Neighbor Embedding (t-SNE) is used here to visualize the separation between classes in a high-dimensional dataset. Each point represents a single fashion item (e.g., T-shirt, Trouser, etc.), and the color corresponds to its true label across the 10 categories listed above.\n\n\nCode\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\n# Run t-SNE to reduce dimensionality\n#embeddings = TSNE(n_jobs=2).fit_transform(X)\n\ntsne = TSNE(n_jobs=-1, random_state=42)  # Use -1 to use all available cores\nembeddings = tsne.fit_transform(X) #use scikitlearn instead\n\n\n# Create scatter plot\nfigure = plt.figure(figsize=(15,7))\nplt.scatter(embeddings[:, 0], embeddings[:, 1], c=train_labels,\n            cmap=plt.cm.get_cmap(\"jet\", 10), marker='.')\nplt.colorbar(ticks=range(10))\n\n\n&lt;matplotlib.colorbar.Colorbar object at 0x3194f5460&gt;\n\n\nCode\nplt.clim(-0.5, 9.5)\nplt.title(\"t-SNE Visualization of Fashion MNIST\")\nplt.show()\n\n\n\n\n\n\n\n\n\nWhat the visualization shows:  Class 1 (blue / Trousers) forms a clearly distinct and tightly packed cluster, indicating that the pixel patterns for trousers are less similar to those of other classes. In contrast, Classes 4 (Coat), 6 (Shirt), and 2 (Pullover) show significant overlap, suggesting that these clothing items are harder to distinguish visually and may lead to more confusion during classification.\n\n\n\nModeling and Results\n\nOur Goal We are classifying Fashion MNIST images into one of 10 categories. To evaluate performance, we’re comparing five different models — some trained on raw pixel values and others using features extracted by a Restricted Boltzmann Machine (RBM). Our objective is to assess whether incorporating RBM into the workflow improves classification accuracy compared to using raw image data alone.\nOur Models 1. Logistic Regression on Fashion MNIST Data 2. Feed Forward Network on Fashion MNIST Data 3. Convolutional Neural Network on Fashion MNIST Data 4. Logistic Regression on RBM Hidden Features (of Fashion MNIST Data) 5. Feed Forward Network on RBM Hidden Features (of Fashion MNIST Data)\nNote: Outputs (50 trials) and Code are below for each model. Both the code and output can be toggled by the reader. • The first click reveals a toggle labeled “Code”. • Clicking “Code” will show the output. • Clicking again will switch from output to the actual code. • Clicking “Show Code and Output” again will collapse both views.\n\nImport Libraries and Re-load data for first 3 models\n\n\nCode\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nimport numpy as np\nimport mlflow\nimport optuna\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom torch.utils.data import DataLoader\n\n# Set device\ndevice = torch.device(\"mps\")\n\n# Load Fashion-MNIST dataset again for the first 3 models\ntransform = transforms.Compose([transforms.ToTensor()])\ntrain_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\ntest_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n\n\n\n\nCode\n#mlflow.end_run()\n#run this in terminal when need to fully clean out expierment after you delete it in the ui\n#rm -rf mlruns/.trash/*\n\n\n\nModel 1: Logistic Regression on Fashion MNIST Data\n\n\nClick to Show Code and Output\n\n\n\nCode\nfrom sklearn.metrics import f1_score\n\nCLASSIFIER = \"LogisticRegression\"  # Change for FNN, LogisticRegression, or CNN\n\n\n\n# Define CNN model\nclass FashionCNN(nn.Module):\n    def __init__(self, filters1, filters2, kernel1, kernel2):\n        super(FashionCNN, self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=filters1, kernel_size=kernel1, padding=1),\n            nn.BatchNorm2d(filters1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(in_channels=filters1, out_channels=filters2, kernel_size=kernel2),\n            nn.BatchNorm2d(filters2),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        self.fc1 = None\n        self.drop = nn.Dropout2d(0.25)\n        self.fc2 = nn.Linear(in_features=600, out_features=120)\n        self.fc3 = nn.Linear(in_features=120, out_features=10)\n        \n\n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = out.view(out.size(0), -1)\n        if self.fc1 is None:\n            self.fc1 = nn.Linear(out.shape[1], 600).to(x.device)\n        out = self.fc1(out)\n        out = self.drop(out)\n        out = self.fc2(out)\n        out = self.fc3(out)\n        return out\n\n        # Dynamically calculate flattened size\n        out = out.view(out.size(0), -1)  # Flatten\n        if self.fc1 is None:\n            self.fc1 = nn.Linear(out.shape[1], 600).to(x.device)  # ✅ Update FC layer dynamically\n\n        out = self.fc1(out)\n        out = self.drop(out)\n        out = self.fc2(out)\n        out = self.fc3(out)\n        return out\n\n\n\n\n# Define Optuna objective function\ndef objective(trial):\n      # Set MLflow experiment name\n    if CLASSIFIER == \"LogisticRegression\":\n        experiment = mlflow.set_experiment(\"new-pytorch-fmnist-lr-noRBM\")\n    elif CLASSIFIER == \"FNN\":\n        experiment = mlflow.set_experiment(\"new-pytorch-fmnist-fnn-noRBM\")\n    elif CLASSIFIER == \"CNN\":\n        experiment = mlflow.set_experiment(\"new-pytorch-fmnist-cnn-noRBM\")\n    batch_size = trial.suggest_int(\"batch_size\", 64, 256, step=32)\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n    mlflow.start_run(experiment_id=experiment.experiment_id)\n    num_classifier_epochs = trial.suggest_int(\"num_classifier_epochs\", 5, 5) \n    mlflow.log_param(\"num_classifier_epochs\", num_classifier_epochs)\n\n    if CLASSIFIER == \"FNN\":\n        hidden_size = trial.suggest_int(\"fnn_hidden\", 192, 384)\n        learning_rate = trial.suggest_float(\"learning_rate\", 0.0001, 0.0025)\n\n        mlflow.log_param(\"classifier\", \"FNN\")\n        mlflow.log_param(\"fnn_hidden\", hidden_size)\n        mlflow.log_param(\"learning_rate\", learning_rate)\n\n        model = nn.Sequential(\n            nn.Linear(784, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, 10)\n        ).to(device)\n\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n    elif CLASSIFIER == \"CNN\":\n        filters1 = trial.suggest_int(\"filters1\", 16, 64, step=16)\n        filters2 = trial.suggest_int(\"filters2\", 32, 128, step=32)\n        kernel1 = trial.suggest_int(\"kernel1\", 3, 5)\n        kernel2 = trial.suggest_int(\"kernel2\", 3, 5)\n        learning_rate = trial.suggest_float(\"learning_rate\", 0.0001, 0.0025)\n\n        mlflow.log_param(\"classifier\", \"CNN\")\n        mlflow.log_param(\"filters1\", filters1)\n        mlflow.log_param(\"filters2\", filters2)\n        mlflow.log_param(\"kernel1\", kernel1)\n        mlflow.log_param(\"kernel2\", kernel2)\n        mlflow.log_param(\"learning_rate\", learning_rate)\n\n        model = FashionCNN(filters1, filters2, kernel1, kernel2).to(device)\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n      \n    elif CLASSIFIER == \"LogisticRegression\":\n        mlflow.log_param(\"classifier\", \"LogisticRegression\")\n    \n        # Prepare data for Logistic Regression (Flatten 28x28 images to 784 features)\n        train_features = train_dataset.data.view(-1, 784).numpy()\n        train_labels = train_dataset.targets.numpy()\n        test_features = test_dataset.data.view(-1, 784).numpy()\n        test_labels = test_dataset.targets.numpy()\n    \n        # Normalize the pixel values to [0,1] for better convergence\n        train_features = train_features / 255.0\n        test_features = test_features / 255.0\n    \n    \n        C = trial.suggest_float(\"C\", 0.01, 10.0, log=True)  \n        solver = \"saga\" \n    \n        model = LogisticRegression(C=C, max_iter=num_classifier_epochs, solver=solver)\n        model.fit(train_features, train_labels)\n    \n    \n        predictions = model.predict(test_features)\n        accuracy = accuracy_score(test_labels, predictions) * 100\n        \n        macro_f1 = f1_score(test_labels, predictions, average=\"macro\") #for f1\n        print(f\"Logistic Regression Test Accuracy: {accuracy:.2f}%\")\n        print(f\"Macro F1 Score: {macro_f1:.4f}\") #for f1\n    \n        mlflow.log_param(\"C\", C)\n        mlflow.log_metric(\"test_accuracy\", accuracy)\n        mlflow.log_metric(\"macro_f1\", macro_f1) #for f1\n        mlflow.end_run()\n        return accuracy\n\n    # Training Loop for FNN and CNN\n    criterion = nn.CrossEntropyLoss()\n\n    model.train()\n    for epoch in range(num_classifier_epochs):\n        running_loss = 0.0\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images) if CLASSIFIER == \"CNN\" else model(images.view(images.size(0), -1))\n\n            optimizer.zero_grad()\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n\n        print(f\"{CLASSIFIER} Epoch {epoch+1}: loss = {running_loss / len(train_loader):.4f}\")\n\n    # Model Evaluation\n    model.eval()\n    correct, total = 0, 0\n    all_preds = []   # for f1\n    all_labels = [] \n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images) if CLASSIFIER == \"CNN\" else model(images.view(images.size(0), -1))\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            all_preds.extend(predicted.cpu().numpy())   #for f1\n            all_labels.extend(labels.cpu().numpy()) #for f1\n\n    accuracy = 100 * correct / total\n    macro_f1 = f1_score(all_labels, all_preds, average=\"macro\") #for f1\n    print(f\"Test Accuracy: {accuracy:.2f}%\")\n    print(f\"Macro F1 Score: {macro_f1:.4f}\") #for f1\n\n    mlflow.log_metric(\"test_accuracy\", accuracy)\n    mlflow.log_metric(\"macro_f1\", macro_f1) #for f1\n    mlflow.end_run()\n    return accuracy\n\nif __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=50)\n    print(f\"Best Parameters for {CLASSIFIER}:\", study.best_params)\n    print(\"Best Accuracy:\", study.best_value)\n\n\nLogistic Regression Test Accuracy: 84.64%\nMacro F1 Score: 0.8458\nLogistic Regression Test Accuracy: 84.29%\nMacro F1 Score: 0.8415\nLogistic Regression Test Accuracy: 84.60%\nMacro F1 Score: 0.8452\nLogistic Regression Test Accuracy: 84.20%\nMacro F1 Score: 0.8412\nLogistic Regression Test Accuracy: 84.47%\nMacro F1 Score: 0.8438\nLogistic Regression Test Accuracy: 84.15%\nMacro F1 Score: 0.8403\nLogistic Regression Test Accuracy: 84.45%\nMacro F1 Score: 0.8442\nLogistic Regression Test Accuracy: 84.42%\nMacro F1 Score: 0.8432\nLogistic Regression Test Accuracy: 84.41%\nMacro F1 Score: 0.8431\nLogistic Regression Test Accuracy: 84.52%\nMacro F1 Score: 0.8445\nLogistic Regression Test Accuracy: 84.48%\nMacro F1 Score: 0.8439\nLogistic Regression Test Accuracy: 84.61%\nMacro F1 Score: 0.8449\nLogistic Regression Test Accuracy: 84.54%\nMacro F1 Score: 0.8444\nLogistic Regression Test Accuracy: 84.41%\nMacro F1 Score: 0.8435\nLogistic Regression Test Accuracy: 84.52%\nMacro F1 Score: 0.8447\nLogistic Regression Test Accuracy: 84.45%\nMacro F1 Score: 0.8438\nLogistic Regression Test Accuracy: 84.47%\nMacro F1 Score: 0.8434\nLogistic Regression Test Accuracy: 84.52%\nMacro F1 Score: 0.8445\nLogistic Regression Test Accuracy: 84.42%\nMacro F1 Score: 0.8437\nLogistic Regression Test Accuracy: 84.51%\nMacro F1 Score: 0.8444\nLogistic Regression Test Accuracy: 84.51%\nMacro F1 Score: 0.8434\nLogistic Regression Test Accuracy: 84.48%\nMacro F1 Score: 0.8440\nLogistic Regression Test Accuracy: 84.51%\nMacro F1 Score: 0.8438\nLogistic Regression Test Accuracy: 84.59%\nMacro F1 Score: 0.8447\nLogistic Regression Test Accuracy: 84.67%\nMacro F1 Score: 0.8458\nLogistic Regression Test Accuracy: 84.40%\nMacro F1 Score: 0.8440\nLogistic Regression Test Accuracy: 84.67%\nMacro F1 Score: 0.8458\nLogistic Regression Test Accuracy: 84.50%\nMacro F1 Score: 0.8439\nLogistic Regression Test Accuracy: 84.47%\nMacro F1 Score: 0.8438\nLogistic Regression Test Accuracy: 84.52%\nMacro F1 Score: 0.8440\nLogistic Regression Test Accuracy: 84.56%\nMacro F1 Score: 0.8449\nLogistic Regression Test Accuracy: 84.62%\nMacro F1 Score: 0.8452\nLogistic Regression Test Accuracy: 84.55%\nMacro F1 Score: 0.8441\nLogistic Regression Test Accuracy: 84.54%\nMacro F1 Score: 0.8442\nLogistic Regression Test Accuracy: 84.48%\nMacro F1 Score: 0.8441\nLogistic Regression Test Accuracy: 84.58%\nMacro F1 Score: 0.8450\nLogistic Regression Test Accuracy: 84.44%\nMacro F1 Score: 0.8434\nLogistic Regression Test Accuracy: 84.61%\nMacro F1 Score: 0.8452\nLogistic Regression Test Accuracy: 84.54%\nMacro F1 Score: 0.8450\nLogistic Regression Test Accuracy: 84.54%\nMacro F1 Score: 0.8441\nLogistic Regression Test Accuracy: 84.49%\nMacro F1 Score: 0.8443\nLogistic Regression Test Accuracy: 84.60%\nMacro F1 Score: 0.8453\nLogistic Regression Test Accuracy: 84.51%\nMacro F1 Score: 0.8443\nLogistic Regression Test Accuracy: 84.62%\nMacro F1 Score: 0.8453\nLogistic Regression Test Accuracy: 84.52%\nMacro F1 Score: 0.8444\nLogistic Regression Test Accuracy: 84.57%\nMacro F1 Score: 0.8446\nLogistic Regression Test Accuracy: 84.48%\nMacro F1 Score: 0.8444\nLogistic Regression Test Accuracy: 84.43%\nMacro F1 Score: 0.8436\nLogistic Regression Test Accuracy: 84.22%\nMacro F1 Score: 0.8407\nLogistic Regression Test Accuracy: 84.43%\nMacro F1 Score: 0.8439\nBest Parameters for LogisticRegression: {'batch_size': 128, 'num_classifier_epochs': 5, 'C': 0.9910546414812195}\nBest Accuracy: 84.67\n\n[I 2025-03-28 14:03:17,300] A new study created in memory with name: no-name-a763725a-0f38-4792-8ede-a3a5148e6dbc\n[I 2025-03-28 14:03:24,080] Trial 0 finished with value: 84.64 and parameters: {'batch_size': 160, 'num_classifier_epochs': 5, 'C': 4.005755197570405}. Best is trial 0 with value: 84.64.\n[I 2025-03-28 14:03:30,796] Trial 1 finished with value: 84.28999999999999 and parameters: {'batch_size': 160, 'num_classifier_epochs': 5, 'C': 0.029468017102199123}. Best is trial 0 with value: 84.64.\n[I 2025-03-28 14:03:37,401] Trial 2 finished with value: 84.6 and parameters: {'batch_size': 128, 'num_classifier_epochs': 5, 'C': 0.6212506932865256}. Best is trial 0 with value: 84.64.\n[I 2025-03-28 14:03:44,073] Trial 3 finished with value: 84.2 and parameters: {'batch_size': 160, 'num_classifier_epochs': 5, 'C': 0.020955917529814606}. Best is trial 0 with value: 84.64.\n[I 2025-03-28 14:03:50,782] Trial 4 finished with value: 84.47 and parameters: {'batch_size': 224, 'num_classifier_epochs': 5, 'C': 0.3121125440383706}. Best is trial 0 with value: 84.64.\n[I 2025-03-28 14:03:57,481] Trial 5 finished with value: 84.15 and parameters: {'batch_size': 192, 'num_classifier_epochs': 5, 'C': 0.012386979715403923}. Best is trial 0 with value: 84.64.\n[I 2025-03-28 14:04:04,158] Trial 6 finished with value: 84.45 and parameters: {'batch_size': 192, 'num_classifier_epochs': 5, 'C': 0.19973940966972406}. Best is trial 0 with value: 84.64.\n[I 2025-03-28 14:04:10,859] Trial 7 finished with value: 84.42 and parameters: {'batch_size': 160, 'num_classifier_epochs': 5, 'C': 0.05330585940873715}. Best is trial 0 with value: 84.64.\n[I 2025-03-28 14:04:17,548] Trial 8 finished with value: 84.41 and parameters: {'batch_size': 224, 'num_classifier_epochs': 5, 'C': 0.09050694362352474}. Best is trial 0 with value: 84.64.\n[I 2025-03-28 14:04:24,157] Trial 9 finished with value: 84.52 and parameters: {'batch_size': 160, 'num_classifier_epochs': 5, 'C': 0.05230937969970407}. Best is trial 0 with value: 84.64.\n[I 2025-03-28 14:04:30,816] Trial 10 finished with value: 84.48 and parameters: {'batch_size': 64, 'num_classifier_epochs': 5, 'C': 7.3972194801993325}. Best is trial 0 with value: 84.64.\n[I 2025-03-28 14:04:37,468] Trial 11 finished with value: 84.61 and parameters: {'batch_size': 96, 'num_classifier_epochs': 5, 'C': 2.5511072593183397}. Best is trial 0 with value: 84.64.\n[I 2025-03-28 14:04:44,180] Trial 12 finished with value: 84.54 and parameters: {'batch_size': 96, 'num_classifier_epochs': 5, 'C': 3.853822640057023}. Best is trial 0 with value: 84.64.\n[I 2025-03-28 14:04:50,803] Trial 13 finished with value: 84.41 and parameters: {'batch_size': 96, 'num_classifier_epochs': 5, 'C': 1.657072564650452}. Best is trial 0 with value: 84.64.\n[I 2025-03-28 14:04:57,541] Trial 14 finished with value: 84.52 and parameters: {'batch_size': 64, 'num_classifier_epochs': 5, 'C': 1.5754830120834762}. Best is trial 0 with value: 84.64.\n[I 2025-03-28 14:05:04,206] Trial 15 finished with value: 84.45 and parameters: {'batch_size': 256, 'num_classifier_epochs': 5, 'C': 9.303014569590758}. Best is trial 0 with value: 84.64.\n[I 2025-03-28 14:05:10,862] Trial 16 finished with value: 84.47 and parameters: {'batch_size': 96, 'num_classifier_epochs': 5, 'C': 1.9593403650111556}. Best is trial 0 with value: 84.64.\n[I 2025-03-28 14:05:17,513] Trial 17 finished with value: 84.52 and parameters: {'batch_size': 128, 'num_classifier_epochs': 5, 'C': 0.756116552630944}. Best is trial 0 with value: 84.64.\n[I 2025-03-28 14:05:24,185] Trial 18 finished with value: 84.42 and parameters: {'batch_size': 128, 'num_classifier_epochs': 5, 'C': 4.021504777647571}. Best is trial 0 with value: 84.64.\n[I 2025-03-28 14:05:30,751] Trial 19 finished with value: 84.50999999999999 and parameters: {'batch_size': 192, 'num_classifier_epochs': 5, 'C': 0.7938897482060593}. Best is trial 0 with value: 84.64.\n[I 2025-03-28 14:05:37,318] Trial 20 finished with value: 84.50999999999999 and parameters: {'batch_size': 64, 'num_classifier_epochs': 5, 'C': 4.117376299327255}. Best is trial 0 with value: 84.64.\n[I 2025-03-28 14:05:44,003] Trial 21 finished with value: 84.48 and parameters: {'batch_size': 128, 'num_classifier_epochs': 5, 'C': 0.3828085370182236}. Best is trial 0 with value: 84.64.\n[I 2025-03-28 14:05:50,572] Trial 22 finished with value: 84.50999999999999 and parameters: {'batch_size': 128, 'num_classifier_epochs': 5, 'C': 0.8485932090577519}. Best is trial 0 with value: 84.64.\n[I 2025-03-28 14:05:57,182] Trial 23 finished with value: 84.59 and parameters: {'batch_size': 96, 'num_classifier_epochs': 5, 'C': 2.5624426088303056}. Best is trial 0 with value: 84.64.\n[I 2025-03-28 14:06:03,715] Trial 24 finished with value: 84.67 and parameters: {'batch_size': 128, 'num_classifier_epochs': 5, 'C': 0.9910546414812195}. Best is trial 24 with value: 84.67.\n[I 2025-03-28 14:06:10,248] Trial 25 finished with value: 84.39999999999999 and parameters: {'batch_size': 96, 'num_classifier_epochs': 5, 'C': 1.2655095430293073}. Best is trial 24 with value: 84.67.\n[I 2025-03-28 14:06:16,856] Trial 26 finished with value: 84.67 and parameters: {'batch_size': 128, 'num_classifier_epochs': 5, 'C': 5.8825032994772455}. Best is trial 24 with value: 84.67.\n[I 2025-03-28 14:06:23,390] Trial 27 finished with value: 84.5 and parameters: {'batch_size': 192, 'num_classifier_epochs': 5, 'C': 6.088761728305264}. Best is trial 24 with value: 84.67.\n[I 2025-03-28 14:06:30,022] Trial 28 finished with value: 84.47 and parameters: {'batch_size': 128, 'num_classifier_epochs': 5, 'C': 5.311149351588817}. Best is trial 24 with value: 84.67.\n[I 2025-03-28 14:06:36,565] Trial 29 finished with value: 84.52 and parameters: {'batch_size': 160, 'num_classifier_epochs': 5, 'C': 9.85585339156705}. Best is trial 24 with value: 84.67.\n[I 2025-03-28 14:06:43,167] Trial 30 finished with value: 84.56 and parameters: {'batch_size': 224, 'num_classifier_epochs': 5, 'C': 3.038695101823179}. Best is trial 24 with value: 84.67.\n[I 2025-03-28 14:06:49,784] Trial 31 finished with value: 84.61999999999999 and parameters: {'batch_size': 128, 'num_classifier_epochs': 5, 'C': 2.550424666091026}. Best is trial 24 with value: 84.67.\n[I 2025-03-28 14:06:56,403] Trial 32 finished with value: 84.55 and parameters: {'batch_size': 160, 'num_classifier_epochs': 5, 'C': 1.2651919043301858}. Best is trial 24 with value: 84.67.\n[I 2025-03-28 14:07:02,980] Trial 33 finished with value: 84.54 and parameters: {'batch_size': 128, 'num_classifier_epochs': 5, 'C': 0.43525367961019507}. Best is trial 24 with value: 84.67.\n[I 2025-03-28 14:07:09,643] Trial 34 finished with value: 84.48 and parameters: {'batch_size': 128, 'num_classifier_epochs': 5, 'C': 5.306178723559651}. Best is trial 24 with value: 84.67.\n[I 2025-03-28 14:07:16,322] Trial 35 finished with value: 84.58 and parameters: {'batch_size': 160, 'num_classifier_epochs': 5, 'C': 2.515204699890743}. Best is trial 24 with value: 84.67.\n[I 2025-03-28 14:07:22,938] Trial 36 finished with value: 84.44 and parameters: {'batch_size': 128, 'num_classifier_epochs': 5, 'C': 0.19782092093669573}. Best is trial 24 with value: 84.67.\n[I 2025-03-28 14:07:29,476] Trial 37 finished with value: 84.61 and parameters: {'batch_size': 192, 'num_classifier_epochs': 5, 'C': 0.5468215159366026}. Best is trial 24 with value: 84.67.\n[I 2025-03-28 14:07:36,026] Trial 38 finished with value: 84.54 and parameters: {'batch_size': 160, 'num_classifier_epochs': 5, 'C': 0.2167562124440949}. Best is trial 24 with value: 84.67.\n[I 2025-03-28 14:07:42,635] Trial 39 finished with value: 84.54 and parameters: {'batch_size': 160, 'num_classifier_epochs': 5, 'C': 1.0824430211479827}. Best is trial 24 with value: 84.67.\n[I 2025-03-28 14:07:49,245] Trial 40 finished with value: 84.49 and parameters: {'batch_size': 160, 'num_classifier_epochs': 5, 'C': 3.2944308410829497}. Best is trial 24 with value: 84.67.\n[I 2025-03-28 14:07:55,859] Trial 41 finished with value: 84.6 and parameters: {'batch_size': 96, 'num_classifier_epochs': 5, 'C': 2.104154517495734}. Best is trial 24 with value: 84.67.\n[I 2025-03-28 14:08:02,484] Trial 42 finished with value: 84.50999999999999 and parameters: {'batch_size': 96, 'num_classifier_epochs': 5, 'C': 6.420389154315065}. Best is trial 24 with value: 84.67.\n[I 2025-03-28 14:08:09,130] Trial 43 finished with value: 84.61999999999999 and parameters: {'batch_size': 128, 'num_classifier_epochs': 5, 'C': 4.412163667982693}. Best is trial 24 with value: 84.67.\n[I 2025-03-28 14:08:15,750] Trial 44 finished with value: 84.52 and parameters: {'batch_size': 128, 'num_classifier_epochs': 5, 'C': 4.663741408256952}. Best is trial 24 with value: 84.67.\n[I 2025-03-28 14:08:22,338] Trial 45 finished with value: 84.57000000000001 and parameters: {'batch_size': 128, 'num_classifier_epochs': 5, 'C': 7.880736706498457}. Best is trial 24 with value: 84.67.\n[I 2025-03-28 14:08:28,992] Trial 46 finished with value: 84.48 and parameters: {'batch_size': 160, 'num_classifier_epochs': 5, 'C': 1.8031331902833885}. Best is trial 24 with value: 84.67.\n[I 2025-03-28 14:08:35,668] Trial 47 finished with value: 84.43 and parameters: {'batch_size': 128, 'num_classifier_epochs': 5, 'C': 3.1167541278023543}. Best is trial 24 with value: 84.67.\n[I 2025-03-28 14:08:42,345] Trial 48 finished with value: 84.22 and parameters: {'batch_size': 128, 'num_classifier_epochs': 5, 'C': 0.02127464220882997}. Best is trial 24 with value: 84.67.\n[I 2025-03-28 14:08:48,965] Trial 49 finished with value: 84.43 and parameters: {'batch_size': 160, 'num_classifier_epochs': 5, 'C': 7.184870291367165}. Best is trial 24 with value: 84.67.\n\n\n\nTest Accuracy of Logistic Regression by C (inverse regularization strength) \nModel 2: Feed Forward Network on Fashion MNIST Data\n\n\nClick to Show Code and Output\n\n\n\nCode\nfrom sklearn.metrics import f1_score\n\nCLASSIFIER = \"FNN\"  # Change for FNN, LogisticRegression, or CNN\n\n# Define CNN model\nclass FashionCNN(nn.Module):\n    def __init__(self, filters1, filters2, kernel1, kernel2):\n        super(FashionCNN, self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=filters1, kernel_size=kernel1, padding=1),\n            nn.BatchNorm2d(filters1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(in_channels=filters1, out_channels=filters2, kernel_size=kernel2),\n            nn.BatchNorm2d(filters2),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        self.fc1 = None\n        self.drop = nn.Dropout2d(0.25)\n        self.fc2 = nn.Linear(in_features=600, out_features=120)\n        self.fc3 = nn.Linear(in_features=120, out_features=10)\n        \n\n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = out.view(out.size(0), -1)\n        if self.fc1 is None:\n            self.fc1 = nn.Linear(out.shape[1], 600).to(x.device)\n        out = self.fc1(out)\n        out = self.drop(out)\n        out = self.fc2(out)\n        out = self.fc3(out)\n        return out\n\n        # Dynamically calculate flattened size\n        out = out.view(out.size(0), -1)  # Flatten\n        if self.fc1 is None:\n            self.fc1 = nn.Linear(out.shape[1], 600).to(x.device)  # ✅ Update FC layer dynamically\n\n        out = self.fc1(out)\n        out = self.drop(out)\n        out = self.fc2(out)\n        out = self.fc3(out)\n        return out\n\n\n\n\n# Define Optuna objective function\ndef objective(trial):\n      # Set MLflow experiment name\n    if CLASSIFIER == \"LogisticRegression\":\n        experiment = mlflow.set_experiment(\"new-pytorch-fmnist-lr-noRBM\")\n    elif CLASSIFIER == \"FNN\":\n        experiment = mlflow.set_experiment(\"new-pytorch-fmnist-fnn-noRBM\")\n    elif CLASSIFIER == \"CNN\":\n        experiment = mlflow.set_experiment(\"new-pytorch-fmnist-cnn-noRBM\")\n    batch_size = trial.suggest_int(\"batch_size\", 64, 256, step=32)\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n    mlflow.start_run(experiment_id=experiment.experiment_id)\n    num_classifier_epochs = trial.suggest_int(\"num_classifier_epochs\", 5, 5) \n    mlflow.log_param(\"num_classifier_epochs\", num_classifier_epochs)\n\n    if CLASSIFIER == \"FNN\":\n        hidden_size = trial.suggest_int(\"fnn_hidden\", 192, 384)\n        learning_rate = trial.suggest_float(\"learning_rate\", 0.0001, 0.0025)\n\n        mlflow.log_param(\"classifier\", \"FNN\")\n        mlflow.log_param(\"fnn_hidden\", hidden_size)\n        mlflow.log_param(\"learning_rate\", learning_rate)\n\n        model = nn.Sequential(\n            nn.Linear(784, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, 10)\n        ).to(device)\n\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n    elif CLASSIFIER == \"CNN\":\n        filters1 = trial.suggest_int(\"filters1\", 16, 64, step=16)\n        filters2 = trial.suggest_int(\"filters2\", 32, 128, step=32)\n        kernel1 = trial.suggest_int(\"kernel1\", 3, 5)\n        kernel2 = trial.suggest_int(\"kernel2\", 3, 5)\n        learning_rate = trial.suggest_float(\"learning_rate\", 0.0001, 0.0025)\n\n        mlflow.log_param(\"classifier\", \"CNN\")\n        mlflow.log_param(\"filters1\", filters1)\n        mlflow.log_param(\"filters2\", filters2)\n        mlflow.log_param(\"kernel1\", kernel1)\n        mlflow.log_param(\"kernel2\", kernel2)\n        mlflow.log_param(\"learning_rate\", learning_rate)\n\n        model = FashionCNN(filters1, filters2, kernel1, kernel2).to(device)\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n      \n    elif CLASSIFIER == \"LogisticRegression\":\n        mlflow.log_param(\"classifier\", \"LogisticRegression\")\n    \n        # Prepare data for Logistic Regression (Flatten 28x28 images to 784 features)\n        train_features = train_dataset.data.view(-1, 784).numpy()\n        train_labels = train_dataset.targets.numpy()\n        test_features = test_dataset.data.view(-1, 784).numpy()\n        test_labels = test_dataset.targets.numpy()\n    \n        # Normalize the pixel values to [0,1] for better convergence\n        train_features = train_features / 255.0\n        test_features = test_features / 255.0\n    \n    \n        C = trial.suggest_float(\"C\", 0.01, 10.0, log=True)  \n        solver = \"saga\" \n    \n        model = LogisticRegression(C=C, max_iter=num_classifier_epochs, solver=solver)\n        model.fit(train_features, train_labels)\n    \n    \n        predictions = model.predict(test_features)\n        accuracy = accuracy_score(test_labels, predictions) * 100\n        \n        macro_f1 = f1_score(test_labels, predictions, average=\"macro\") #for f1\n        print(f\"Logistic Regression Test Accuracy: {accuracy:.2f}%\")\n        print(f\"Macro F1 Score: {macro_f1:.4f}\") #for f1\n    \n        mlflow.log_param(\"C\", C)\n        mlflow.log_metric(\"test_accuracy\", accuracy)\n        mlflow.log_metric(\"macro_f1\", macro_f1) #for f1\n        mlflow.end_run()\n        return accuracy\n\n    # Training Loop for FNN and CNN\n    criterion = nn.CrossEntropyLoss()\n\n    model.train()\n    for epoch in range(num_classifier_epochs):\n        running_loss = 0.0\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images) if CLASSIFIER == \"CNN\" else model(images.view(images.size(0), -1))\n\n            optimizer.zero_grad()\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n\n        print(f\"{CLASSIFIER} Epoch {epoch+1}: loss = {running_loss / len(train_loader):.4f}\")\n\n    # Model Evaluation\n    model.eval()\n    correct, total = 0, 0\n    all_preds = []   # for f1\n    all_labels = [] \n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images) if CLASSIFIER == \"CNN\" else model(images.view(images.size(0), -1))\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            all_preds.extend(predicted.cpu().numpy())   #for f1\n            all_labels.extend(labels.cpu().numpy()) #for f1\n\n    accuracy = 100 * correct / total\n    macro_f1 = f1_score(all_labels, all_preds, average=\"macro\") #for f1\n    print(f\"Test Accuracy: {accuracy:.2f}%\")\n    print(f\"Macro F1 Score: {macro_f1:.4f}\") #for f1\n\n    mlflow.log_metric(\"test_accuracy\", accuracy)\n    mlflow.log_metric(\"macro_f1\", macro_f1) #for f1\n    mlflow.end_run()\n    return accuracy\n\nif __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=50)\n    print(f\"Best Parameters for {CLASSIFIER}:\", study.best_params)\n    print(\"Best Accuracy:\", study.best_value)\n\n\nFNN Epoch 1: loss = 0.5087\nFNN Epoch 2: loss = 0.3764\nFNN Epoch 3: loss = 0.3368\nFNN Epoch 4: loss = 0.3098\nFNN Epoch 5: loss = 0.2934\nTest Accuracy: 87.28%\nMacro F1 Score: 0.8673\nFNN Epoch 1: loss = 0.6897\nFNN Epoch 2: loss = 0.4514\nFNN Epoch 3: loss = 0.4053\nFNN Epoch 4: loss = 0.3766\nFNN Epoch 5: loss = 0.3588\nTest Accuracy: 85.83%\nMacro F1 Score: 0.8543\nFNN Epoch 1: loss = 0.5476\nFNN Epoch 2: loss = 0.3894\nFNN Epoch 3: loss = 0.3446\nFNN Epoch 4: loss = 0.3158\nFNN Epoch 5: loss = 0.2967\nTest Accuracy: 87.59%\nMacro F1 Score: 0.8761\nFNN Epoch 1: loss = 0.6250\nFNN Epoch 2: loss = 0.4280\nFNN Epoch 3: loss = 0.3871\nFNN Epoch 4: loss = 0.3577\nFNN Epoch 5: loss = 0.3362\nTest Accuracy: 86.81%\nMacro F1 Score: 0.8653\nFNN Epoch 1: loss = 0.5223\nFNN Epoch 2: loss = 0.3784\nFNN Epoch 3: loss = 0.3355\nFNN Epoch 4: loss = 0.3142\nFNN Epoch 5: loss = 0.2912\nTest Accuracy: 87.62%\nMacro F1 Score: 0.8757\nFNN Epoch 1: loss = 0.5931\nFNN Epoch 2: loss = 0.4156\nFNN Epoch 3: loss = 0.3729\nFNN Epoch 4: loss = 0.3451\nFNN Epoch 5: loss = 0.3290\nTest Accuracy: 86.71%\nMacro F1 Score: 0.8650\nFNN Epoch 1: loss = 0.5084\nFNN Epoch 2: loss = 0.3782\nFNN Epoch 3: loss = 0.3366\nFNN Epoch 4: loss = 0.3124\nFNN Epoch 5: loss = 0.2921\nTest Accuracy: 87.95%\nMacro F1 Score: 0.8784\nFNN Epoch 1: loss = 1.0307\nFNN Epoch 2: loss = 0.5889\nFNN Epoch 3: loss = 0.5123\nFNN Epoch 4: loss = 0.4749\nFNN Epoch 5: loss = 0.4493\nTest Accuracy: 83.61%\nMacro F1 Score: 0.8337\nFNN Epoch 1: loss = 0.5636\nFNN Epoch 2: loss = 0.3984\nFNN Epoch 3: loss = 0.3531\nFNN Epoch 4: loss = 0.3285\nFNN Epoch 5: loss = 0.3064\nTest Accuracy: 86.77%\nMacro F1 Score: 0.8655\nFNN Epoch 1: loss = 0.8425\nFNN Epoch 2: loss = 0.5113\nFNN Epoch 3: loss = 0.4562\nFNN Epoch 4: loss = 0.4261\nFNN Epoch 5: loss = 0.4051\nTest Accuracy: 85.08%\nMacro F1 Score: 0.8500\nFNN Epoch 1: loss = 0.5062\nFNN Epoch 2: loss = 0.3709\nFNN Epoch 3: loss = 0.3365\nFNN Epoch 4: loss = 0.3101\nFNN Epoch 5: loss = 0.2914\nTest Accuracy: 86.99%\nMacro F1 Score: 0.8688\nFNN Epoch 1: loss = 0.4901\nFNN Epoch 2: loss = 0.3667\nFNN Epoch 3: loss = 0.3305\nFNN Epoch 4: loss = 0.3047\nFNN Epoch 5: loss = 0.2863\nTest Accuracy: 87.77%\nMacro F1 Score: 0.8780\nFNN Epoch 1: loss = 0.4942\nFNN Epoch 2: loss = 0.3671\nFNN Epoch 3: loss = 0.3286\nFNN Epoch 4: loss = 0.3050\nFNN Epoch 5: loss = 0.2882\nTest Accuracy: 87.48%\nMacro F1 Score: 0.8760\nFNN Epoch 1: loss = 0.5249\nFNN Epoch 2: loss = 0.3827\nFNN Epoch 3: loss = 0.3405\nFNN Epoch 4: loss = 0.3160\nFNN Epoch 5: loss = 0.2958\nTest Accuracy: 85.96%\nMacro F1 Score: 0.8622\nFNN Epoch 1: loss = 0.5469\nFNN Epoch 2: loss = 0.3992\nFNN Epoch 3: loss = 0.3564\nFNN Epoch 4: loss = 0.3279\nFNN Epoch 5: loss = 0.3082\nTest Accuracy: 87.03%\nMacro F1 Score: 0.8688\nFNN Epoch 1: loss = 0.5054\nFNN Epoch 2: loss = 0.3690\nFNN Epoch 3: loss = 0.3302\nFNN Epoch 4: loss = 0.3059\nFNN Epoch 5: loss = 0.2908\nTest Accuracy: 87.53%\nMacro F1 Score: 0.8738\nFNN Epoch 1: loss = 0.5027\nFNN Epoch 2: loss = 0.3665\nFNN Epoch 3: loss = 0.3294\nFNN Epoch 4: loss = 0.3087\nFNN Epoch 5: loss = 0.2909\nTest Accuracy: 87.50%\nMacro F1 Score: 0.8738\nFNN Epoch 1: loss = 0.5466\nFNN Epoch 2: loss = 0.3952\nFNN Epoch 3: loss = 0.3516\nFNN Epoch 4: loss = 0.3246\nFNN Epoch 5: loss = 0.2997\nTest Accuracy: 87.29%\nMacro F1 Score: 0.8709\nFNN Epoch 1: loss = 0.6288\nFNN Epoch 2: loss = 0.4358\nFNN Epoch 3: loss = 0.3942\nFNN Epoch 4: loss = 0.3681\nFNN Epoch 5: loss = 0.3479\nTest Accuracy: 86.39%\nMacro F1 Score: 0.8649\nFNN Epoch 1: loss = 0.5035\nFNN Epoch 2: loss = 0.3697\nFNN Epoch 3: loss = 0.3303\nFNN Epoch 4: loss = 0.3064\nFNN Epoch 5: loss = 0.2906\nTest Accuracy: 87.66%\nMacro F1 Score: 0.8754\nFNN Epoch 1: loss = 0.4961\nFNN Epoch 2: loss = 0.3652\nFNN Epoch 3: loss = 0.3309\nFNN Epoch 4: loss = 0.3078\nFNN Epoch 5: loss = 0.2920\nTest Accuracy: 86.23%\nMacro F1 Score: 0.8587\nFNN Epoch 1: loss = 0.4997\nFNN Epoch 2: loss = 0.3696\nFNN Epoch 3: loss = 0.3328\nFNN Epoch 4: loss = 0.3061\nFNN Epoch 5: loss = 0.2899\nTest Accuracy: 87.66%\nMacro F1 Score: 0.8746\nFNN Epoch 1: loss = 0.4945\nFNN Epoch 2: loss = 0.3645\nFNN Epoch 3: loss = 0.3292\nFNN Epoch 4: loss = 0.3084\nFNN Epoch 5: loss = 0.2878\nTest Accuracy: 87.65%\nMacro F1 Score: 0.8757\nFNN Epoch 1: loss = 0.5335\nFNN Epoch 2: loss = 0.3910\nFNN Epoch 3: loss = 0.3472\nFNN Epoch 4: loss = 0.3213\nFNN Epoch 5: loss = 0.3000\nTest Accuracy: 87.11%\nMacro F1 Score: 0.8709\nFNN Epoch 1: loss = 0.5038\nFNN Epoch 2: loss = 0.3696\nFNN Epoch 3: loss = 0.3325\nFNN Epoch 4: loss = 0.3082\nFNN Epoch 5: loss = 0.2882\nTest Accuracy: 86.11%\nMacro F1 Score: 0.8625\nFNN Epoch 1: loss = 0.5646\nFNN Epoch 2: loss = 0.3979\nFNN Epoch 3: loss = 0.3554\nFNN Epoch 4: loss = 0.3274\nFNN Epoch 5: loss = 0.3064\nTest Accuracy: 87.78%\nMacro F1 Score: 0.8760\nFNN Epoch 1: loss = 0.5547\nFNN Epoch 2: loss = 0.4028\nFNN Epoch 3: loss = 0.3557\nFNN Epoch 4: loss = 0.3287\nFNN Epoch 5: loss = 0.3106\nTest Accuracy: 87.35%\nMacro F1 Score: 0.8728\nFNN Epoch 1: loss = 0.5345\nFNN Epoch 2: loss = 0.3877\nFNN Epoch 3: loss = 0.3428\nFNN Epoch 4: loss = 0.3162\nFNN Epoch 5: loss = 0.2983\nTest Accuracy: 87.87%\nMacro F1 Score: 0.8781\nFNN Epoch 1: loss = 0.6026\nFNN Epoch 2: loss = 0.4247\nFNN Epoch 3: loss = 0.3849\nFNN Epoch 4: loss = 0.3544\nFNN Epoch 5: loss = 0.3342\nTest Accuracy: 86.89%\nMacro F1 Score: 0.8676\nFNN Epoch 1: loss = 0.5496\nFNN Epoch 2: loss = 0.3985\nFNN Epoch 3: loss = 0.3574\nFNN Epoch 4: loss = 0.3277\nFNN Epoch 5: loss = 0.3045\nTest Accuracy: 87.16%\nMacro F1 Score: 0.8700\nFNN Epoch 1: loss = 0.5537\nFNN Epoch 2: loss = 0.3964\nFNN Epoch 3: loss = 0.3555\nFNN Epoch 4: loss = 0.3250\nFNN Epoch 5: loss = 0.3043\nTest Accuracy: 86.52%\nMacro F1 Score: 0.8607\nFNN Epoch 1: loss = 0.5199\nFNN Epoch 2: loss = 0.3781\nFNN Epoch 3: loss = 0.3376\nFNN Epoch 4: loss = 0.3090\nFNN Epoch 5: loss = 0.2903\nTest Accuracy: 86.14%\nMacro F1 Score: 0.8651\nFNN Epoch 1: loss = 0.5423\nFNN Epoch 2: loss = 0.3924\nFNN Epoch 3: loss = 0.3503\nFNN Epoch 4: loss = 0.3227\nFNN Epoch 5: loss = 0.3023\nTest Accuracy: 87.04%\nMacro F1 Score: 0.8698\nFNN Epoch 1: loss = 0.5308\nFNN Epoch 2: loss = 0.3882\nFNN Epoch 3: loss = 0.3439\nFNN Epoch 4: loss = 0.3196\nFNN Epoch 5: loss = 0.2953\nTest Accuracy: 87.78%\nMacro F1 Score: 0.8770\nFNN Epoch 1: loss = 0.6554\nFNN Epoch 2: loss = 0.4416\nFNN Epoch 3: loss = 0.3996\nFNN Epoch 4: loss = 0.3730\nFNN Epoch 5: loss = 0.3523\nTest Accuracy: 86.74%\nMacro F1 Score: 0.8670\nFNN Epoch 1: loss = 0.5975\nFNN Epoch 2: loss = 0.4166\nFNN Epoch 3: loss = 0.3752\nFNN Epoch 4: loss = 0.3500\nFNN Epoch 5: loss = 0.3280\nTest Accuracy: 86.97%\nMacro F1 Score: 0.8686\nFNN Epoch 1: loss = 0.5616\nFNN Epoch 2: loss = 0.3995\nFNN Epoch 3: loss = 0.3581\nFNN Epoch 4: loss = 0.3303\nFNN Epoch 5: loss = 0.3112\nTest Accuracy: 86.01%\nMacro F1 Score: 0.8624\nFNN Epoch 1: loss = 0.6865\nFNN Epoch 2: loss = 0.4488\nFNN Epoch 3: loss = 0.4080\nFNN Epoch 4: loss = 0.3788\nFNN Epoch 5: loss = 0.3601\nTest Accuracy: 86.15%\nMacro F1 Score: 0.8595\nFNN Epoch 1: loss = 0.5225\nFNN Epoch 2: loss = 0.3804\nFNN Epoch 3: loss = 0.3394\nFNN Epoch 4: loss = 0.3153\nFNN Epoch 5: loss = 0.2945\nTest Accuracy: 87.46%\nMacro F1 Score: 0.8747\nFNN Epoch 1: loss = 0.5712\nFNN Epoch 2: loss = 0.4067\nFNN Epoch 3: loss = 0.3648\nFNN Epoch 4: loss = 0.3370\nFNN Epoch 5: loss = 0.3139\nTest Accuracy: 86.31%\nMacro F1 Score: 0.8640\nFNN Epoch 1: loss = 0.5433\nFNN Epoch 2: loss = 0.3874\nFNN Epoch 3: loss = 0.3427\nFNN Epoch 4: loss = 0.3149\nFNN Epoch 5: loss = 0.2957\nTest Accuracy: 85.75%\nMacro F1 Score: 0.8571\nFNN Epoch 1: loss = 0.4929\nFNN Epoch 2: loss = 0.3677\nFNN Epoch 3: loss = 0.3292\nFNN Epoch 4: loss = 0.3018\nFNN Epoch 5: loss = 0.2864\nTest Accuracy: 87.70%\nMacro F1 Score: 0.8764\nFNN Epoch 1: loss = 0.4886\nFNN Epoch 2: loss = 0.3673\nFNN Epoch 3: loss = 0.3274\nFNN Epoch 4: loss = 0.3068\nFNN Epoch 5: loss = 0.2878\nTest Accuracy: 87.52%\nMacro F1 Score: 0.8737\nFNN Epoch 1: loss = 0.4845\nFNN Epoch 2: loss = 0.3652\nFNN Epoch 3: loss = 0.3275\nFNN Epoch 4: loss = 0.3064\nFNN Epoch 5: loss = 0.2892\nTest Accuracy: 87.33%\nMacro F1 Score: 0.8724\nFNN Epoch 1: loss = 0.5210\nFNN Epoch 2: loss = 0.3791\nFNN Epoch 3: loss = 0.3399\nFNN Epoch 4: loss = 0.3124\nFNN Epoch 5: loss = 0.2948\nTest Accuracy: 87.62%\nMacro F1 Score: 0.8742\nFNN Epoch 1: loss = 0.5101\nFNN Epoch 2: loss = 0.3730\nFNN Epoch 3: loss = 0.3291\nFNN Epoch 4: loss = 0.3068\nFNN Epoch 5: loss = 0.2881\nTest Accuracy: 87.65%\nMacro F1 Score: 0.8758\nFNN Epoch 1: loss = 0.6214\nFNN Epoch 2: loss = 0.4269\nFNN Epoch 3: loss = 0.3902\nFNN Epoch 4: loss = 0.3595\nFNN Epoch 5: loss = 0.3372\nTest Accuracy: 86.25%\nMacro F1 Score: 0.8624\nFNN Epoch 1: loss = 0.5439\nFNN Epoch 2: loss = 0.3959\nFNN Epoch 3: loss = 0.3516\nFNN Epoch 4: loss = 0.3219\nFNN Epoch 5: loss = 0.3036\nTest Accuracy: 87.66%\nMacro F1 Score: 0.8760\nFNN Epoch 1: loss = 0.5466\nFNN Epoch 2: loss = 0.3964\nFNN Epoch 3: loss = 0.3551\nFNN Epoch 4: loss = 0.3275\nFNN Epoch 5: loss = 0.3086\nTest Accuracy: 86.08%\nMacro F1 Score: 0.8592\nFNN Epoch 1: loss = 0.4928\nFNN Epoch 2: loss = 0.3662\nFNN Epoch 3: loss = 0.3298\nFNN Epoch 4: loss = 0.3058\nFNN Epoch 5: loss = 0.2842\nTest Accuracy: 86.82%\nMacro F1 Score: 0.8693\nBest Parameters for FNN: {'batch_size': 64, 'num_classifier_epochs': 5, 'fnn_hidden': 304, 'learning_rate': 0.0010717942740929352}\nBest Accuracy: 87.95\n\n[I 2025-03-28 14:08:49,504] A new study created in memory with name: no-name-983a1dcb-4266-42aa-ba3a-6807a1759ec8\n[I 2025-03-28 14:09:04,255] Trial 0 finished with value: 87.28 and parameters: {'batch_size': 128, 'num_classifier_epochs': 5, 'fnn_hidden': 316, 'learning_rate': 0.0019643108659146637}. Best is trial 0 with value: 87.28.\n[I 2025-03-28 14:09:16,162] Trial 1 finished with value: 85.83 and parameters: {'batch_size': 224, 'num_classifier_epochs': 5, 'fnn_hidden': 218, 'learning_rate': 0.0005743084788275644}. Best is trial 0 with value: 87.28.\n[I 2025-03-28 14:09:27,990] Trial 2 finished with value: 87.59 and parameters: {'batch_size': 224, 'num_classifier_epochs': 5, 'fnn_hidden': 371, 'learning_rate': 0.0019863794468380712}. Best is trial 2 with value: 87.59.\n[I 2025-03-28 14:09:40,429] Trial 3 finished with value: 86.81 and parameters: {'batch_size': 192, 'num_classifier_epochs': 5, 'fnn_hidden': 286, 'learning_rate': 0.000712170280473209}. Best is trial 2 with value: 87.59.\n[I 2025-03-28 14:09:53,781] Trial 4 finished with value: 87.62 and parameters: {'batch_size': 160, 'num_classifier_epochs': 5, 'fnn_hidden': 334, 'learning_rate': 0.0020136851145242008}. Best is trial 4 with value: 87.62.\n[I 2025-03-28 14:10:07,119] Trial 5 finished with value: 86.71 and parameters: {'batch_size': 160, 'num_classifier_epochs': 5, 'fnn_hidden': 234, 'learning_rate': 0.000879950233252207}. Best is trial 4 with value: 87.62.\n[I 2025-03-28 14:25:49,791] Trial 6 finished with value: 87.95 and parameters: {'batch_size': 64, 'num_classifier_epochs': 5, 'fnn_hidden': 304, 'learning_rate': 0.0010717942740929352}. Best is trial 6 with value: 87.95.\n[I 2025-03-28 14:26:02,383] Trial 7 finished with value: 83.61 and parameters: {'batch_size': 192, 'num_classifier_epochs': 5, 'fnn_hidden': 343, 'learning_rate': 0.00010498879450062599}. Best is trial 6 with value: 87.95.\n[I 2025-03-28 14:26:13,821] Trial 8 finished with value: 86.77 and parameters: {'batch_size': 256, 'num_classifier_epochs': 5, 'fnn_hidden': 234, 'learning_rate': 0.002295260294245649}. Best is trial 6 with value: 87.95.\n[I 2025-03-28 14:26:25,825] Trial 9 finished with value: 85.08 and parameters: {'batch_size': 224, 'num_classifier_epochs': 5, 'fnn_hidden': 325, 'learning_rate': 0.00024344110913700564}. Best is trial 6 with value: 87.95.\n[I 2025-03-28 14:26:45,755] Trial 10 finished with value: 86.99 and parameters: {'batch_size': 64, 'num_classifier_epochs': 5, 'fnn_hidden': 278, 'learning_rate': 0.0012933878073006246}. Best is trial 6 with value: 87.95.\n[I 2025-03-28 14:27:05,756] Trial 11 finished with value: 87.77 and parameters: {'batch_size': 64, 'num_classifier_epochs': 5, 'fnn_hidden': 384, 'learning_rate': 0.001456679915410512}. Best is trial 6 with value: 87.95.\n[I 2025-03-28 14:27:25,685] Trial 12 finished with value: 87.48 and parameters: {'batch_size': 64, 'num_classifier_epochs': 5, 'fnn_hidden': 377, 'learning_rate': 0.00134565096431046}. Best is trial 6 with value: 87.95.\n[I 2025-03-28 14:27:42,214] Trial 13 finished with value: 85.96 and parameters: {'batch_size': 96, 'num_classifier_epochs': 5, 'fnn_hidden': 267, 'learning_rate': 0.0013802805587902047}. Best is trial 6 with value: 87.95.\n[I 2025-03-28 14:27:58,589] Trial 14 finished with value: 87.03 and parameters: {'batch_size': 96, 'num_classifier_epochs': 5, 'fnn_hidden': 197, 'learning_rate': 0.0010918734361520083}. Best is trial 6 with value: 87.95.\n[I 2025-03-28 14:28:15,043] Trial 15 finished with value: 87.53 and parameters: {'batch_size': 96, 'num_classifier_epochs': 5, 'fnn_hidden': 354, 'learning_rate': 0.0016476428641274438}. Best is trial 6 with value: 87.95.\n[I 2025-03-28 14:28:34,936] Trial 16 finished with value: 87.5 and parameters: {'batch_size': 64, 'num_classifier_epochs': 5, 'fnn_hidden': 295, 'learning_rate': 0.0016223464008806217}. Best is trial 6 with value: 87.95.\n[I 2025-03-28 14:28:49,545] Trial 17 finished with value: 87.29 and parameters: {'batch_size': 128, 'num_classifier_epochs': 5, 'fnn_hidden': 384, 'learning_rate': 0.0010056153978433512}. Best is trial 6 with value: 87.95.\n[I 2025-03-28 14:29:03,976] Trial 18 finished with value: 86.39 and parameters: {'batch_size': 128, 'num_classifier_epochs': 5, 'fnn_hidden': 262, 'learning_rate': 0.0005037017776003777}. Best is trial 6 with value: 87.95.\n[I 2025-03-28 14:29:24,158] Trial 19 finished with value: 87.66 and parameters: {'batch_size': 64, 'num_classifier_epochs': 5, 'fnn_hidden': 305, 'learning_rate': 0.0016580598066331673}. Best is trial 6 with value: 87.95.\n[I 2025-03-28 14:29:40,807] Trial 20 finished with value: 86.23 and parameters: {'batch_size': 96, 'num_classifier_epochs': 5, 'fnn_hidden': 355, 'learning_rate': 0.002390707562410727}. Best is trial 6 with value: 87.95.\n[I 2025-03-28 14:30:00,723] Trial 21 finished with value: 87.66 and parameters: {'batch_size': 64, 'num_classifier_epochs': 5, 'fnn_hidden': 306, 'learning_rate': 0.0015819689156265399}. Best is trial 6 with value: 87.95.\n[I 2025-03-28 14:30:20,585] Trial 22 finished with value: 87.65 and parameters: {'batch_size': 64, 'num_classifier_epochs': 5, 'fnn_hidden': 309, 'learning_rate': 0.0017706227694859965}. Best is trial 6 with value: 87.95.\n[I 2025-03-28 14:30:37,245] Trial 23 finished with value: 87.11 and parameters: {'batch_size': 96, 'num_classifier_epochs': 5, 'fnn_hidden': 257, 'learning_rate': 0.0011721567248532275}. Best is trial 6 with value: 87.95.\n[I 2025-03-28 14:30:57,144] Trial 24 finished with value: 86.11 and parameters: {'batch_size': 64, 'num_classifier_epochs': 5, 'fnn_hidden': 297, 'learning_rate': 0.0014901384184530485}. Best is trial 6 with value: 87.95.\n[I 2025-03-28 14:31:11,770] Trial 25 finished with value: 87.78 and parameters: {'batch_size': 128, 'num_classifier_epochs': 5, 'fnn_hidden': 347, 'learning_rate': 0.0008857376095970037}. Best is trial 6 with value: 87.95.\n[I 2025-03-28 14:31:26,472] Trial 26 finished with value: 87.35 and parameters: {'batch_size': 128, 'num_classifier_epochs': 5, 'fnn_hidden': 362, 'learning_rate': 0.0008330810395781131}. Best is trial 6 with value: 87.95.\n[I 2025-03-28 14:31:43,017] Trial 27 finished with value: 87.87 and parameters: {'batch_size': 96, 'num_classifier_epochs': 5, 'fnn_hidden': 342, 'learning_rate': 0.0009495722354665602}. Best is trial 6 with value: 87.95.\n[I 2025-03-28 14:31:57,652] Trial 28 finished with value: 86.89 and parameters: {'batch_size': 128, 'num_classifier_epochs': 5, 'fnn_hidden': 340, 'learning_rate': 0.0005170836542807583}. Best is trial 6 with value: 87.95.\n[I 2025-03-28 14:32:12,291] Trial 29 finished with value: 87.16 and parameters: {'batch_size': 128, 'num_classifier_epochs': 5, 'fnn_hidden': 321, 'learning_rate': 0.0009883480804708245}. Best is trial 6 with value: 87.95.\n[I 2025-03-28 14:32:28,922] Trial 30 finished with value: 86.52 and parameters: {'batch_size': 96, 'num_classifier_epochs': 5, 'fnn_hidden': 330, 'learning_rate': 0.0007201210827906603}. Best is trial 6 with value: 87.95.\n[I 2025-03-28 14:32:45,538] Trial 31 finished with value: 86.14 and parameters: {'batch_size': 96, 'num_classifier_epochs': 5, 'fnn_hidden': 364, 'learning_rate': 0.0011965834722640672}. Best is trial 6 with value: 87.95.\n[I 2025-03-28 14:33:02,031] Trial 32 finished with value: 87.04 and parameters: {'batch_size': 96, 'num_classifier_epochs': 5, 'fnn_hidden': 348, 'learning_rate': 0.0008632040329805744}. Best is trial 6 with value: 87.95.\n[I 2025-03-28 14:33:21,870] Trial 33 finished with value: 87.78 and parameters: {'batch_size': 64, 'num_classifier_epochs': 5, 'fnn_hidden': 371, 'learning_rate': 0.0006580877176856504}. Best is trial 6 with value: 87.95.\n[I 2025-03-28 14:33:36,422] Trial 34 finished with value: 86.74 and parameters: {'batch_size': 128, 'num_classifier_epochs': 5, 'fnn_hidden': 365, 'learning_rate': 0.00034264315176449806}. Best is trial 6 with value: 87.95.\n[I 2025-03-28 14:33:49,798] Trial 35 finished with value: 86.97 and parameters: {'batch_size': 160, 'num_classifier_epochs': 5, 'fnn_hidden': 339, 'learning_rate': 0.0006735523042490962}. Best is trial 6 with value: 87.95.\n[I 2025-03-28 14:49:51,445] Trial 36 finished with value: 86.01 and parameters: {'batch_size': 96, 'num_classifier_epochs': 5, 'fnn_hidden': 371, 'learning_rate': 0.0006317392699132341}. Best is trial 6 with value: 87.95.\n[I 2025-03-28 14:50:54,737] Trial 37 finished with value: 86.15 and parameters: {'batch_size': 192, 'num_classifier_epochs': 5, 'fnn_hidden': 316, 'learning_rate': 0.00041861541457776253}. Best is trial 6 with value: 87.95.\n[I 2025-03-28 14:52:31,018] Trial 38 finished with value: 87.46 and parameters: {'batch_size': 64, 'num_classifier_epochs': 5, 'fnn_hidden': 349, 'learning_rate': 0.0008043968547649064}. Best is trial 6 with value: 87.95.\n[I 2025-03-28 14:57:10,840] Trial 39 finished with value: 86.31 and parameters: {'batch_size': 160, 'num_classifier_epochs': 5, 'fnn_hidden': 333, 'learning_rate': 0.0009800005538671466}. Best is trial 6 with value: 87.95.\n[I 2025-03-28 14:58:36,766] Trial 40 finished with value: 85.75 and parameters: {'batch_size': 128, 'num_classifier_epochs': 5, 'fnn_hidden': 324, 'learning_rate': 0.0011877036439728564}. Best is trial 6 with value: 87.95.\n[I 2025-03-28 15:06:40,962] Trial 41 finished with value: 87.7 and parameters: {'batch_size': 64, 'num_classifier_epochs': 5, 'fnn_hidden': 383, 'learning_rate': 0.0014521830179726831}. Best is trial 6 with value: 87.95.\n[I 2025-03-28 15:07:01,028] Trial 42 finished with value: 87.52 and parameters: {'batch_size': 64, 'num_classifier_epochs': 5, 'fnn_hidden': 373, 'learning_rate': 0.001844578972387457}. Best is trial 6 with value: 87.95.\n[I 2025-03-28 15:07:20,882] Trial 43 finished with value: 87.33 and parameters: {'batch_size': 64, 'num_classifier_epochs': 5, 'fnn_hidden': 355, 'learning_rate': 0.0021212829675613725}. Best is trial 6 with value: 87.95.\n[I 2025-03-28 15:07:38,719] Trial 44 finished with value: 87.62 and parameters: {'batch_size': 96, 'num_classifier_epochs': 5, 'fnn_hidden': 374, 'learning_rate': 0.001091989921026165}. Best is trial 6 with value: 87.95.\n[I 2025-03-28 15:08:02,511] Trial 45 finished with value: 87.65 and parameters: {'batch_size': 64, 'num_classifier_epochs': 5, 'fnn_hidden': 361, 'learning_rate': 0.0012472755708203321}. Best is trial 6 with value: 87.95.\n[I 2025-03-28 15:08:15,197] Trial 46 finished with value: 86.25 and parameters: {'batch_size': 256, 'num_classifier_epochs': 5, 'fnn_hidden': 279, 'learning_rate': 0.0009346075763025649}. Best is trial 6 with value: 87.95.\n[I 2025-03-28 15:08:31,922] Trial 47 finished with value: 87.66 and parameters: {'batch_size': 96, 'num_classifier_epochs': 5, 'fnn_hidden': 345, 'learning_rate': 0.0007707965198842552}. Best is trial 6 with value: 87.95.\n[I 2025-03-28 15:08:48,744] Trial 48 finished with value: 86.08 and parameters: {'batch_size': 96, 'num_classifier_epochs': 5, 'fnn_hidden': 243, 'learning_rate': 0.0010686543058994109}. Best is trial 6 with value: 87.95.\n[I 2025-03-28 15:09:08,744] Trial 49 finished with value: 86.82 and parameters: {'batch_size': 64, 'num_classifier_epochs': 5, 'fnn_hidden': 382, 'learning_rate': 0.0013456789092006166}. Best is trial 6 with value: 87.95.\n\n\n\nTest Accuracy by FNN Hidden Units \nModel 3: Convolutional Neural Network on Fashion MNIST Data Base code for CNN structure borrowed from Kaggle\n\n\nClick to Show Code and Output\n\n\n\nCode\nfrom sklearn.metrics import f1_score\n\nCLASSIFIER = \"CNN\"  # Change for FNN, LogisticRegression, or CNN\n\n# Define CNN model\nclass FashionCNN(nn.Module):\n    def __init__(self, filters1, filters2, kernel1, kernel2):\n        super(FashionCNN, self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=filters1, kernel_size=kernel1, padding=1),\n            nn.BatchNorm2d(filters1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(in_channels=filters1, out_channels=filters2, kernel_size=kernel2),\n            nn.BatchNorm2d(filters2),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        self.fc1 = None\n        self.drop = nn.Dropout2d(0.25)\n        self.fc2 = nn.Linear(in_features=600, out_features=120)\n        self.fc3 = nn.Linear(in_features=120, out_features=10)\n        \n\n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = out.view(out.size(0), -1)\n        if self.fc1 is None:\n            self.fc1 = nn.Linear(out.shape[1], 600).to(x.device)\n        out = self.fc1(out)\n        out = self.drop(out)\n        out = self.fc2(out)\n        out = self.fc3(out)\n        return out\n\n        # Dynamically calculate flattened size\n        out = out.view(out.size(0), -1)  # Flatten\n        if self.fc1 is None:\n            self.fc1 = nn.Linear(out.shape[1], 600).to(x.device)  # ✅ Update FC layer dynamically\n\n        out = self.fc1(out)\n        out = self.drop(out)\n        out = self.fc2(out)\n        out = self.fc3(out)\n        return out\n\n\n\n\n# Define Optuna objective function\ndef objective(trial):\n        # Set MLflow experiment name\n    if CLASSIFIER == \"LogisticRegression\":\n        experiment = mlflow.set_experiment(\"new-pytorch-fmnist-lr-noRBM\")\n    elif CLASSIFIER == \"FNN\":\n        experiment = mlflow.set_experiment(\"new-pytorch-fmnist-fnn-noRBM\")\n    elif CLASSIFIER == \"CNN\":\n        experiment = mlflow.set_experiment(\"new-pytorch-fmnist-cnn-noRBM\")\n    batch_size = trial.suggest_int(\"batch_size\", 64, 256, step=32)\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n    mlflow.start_run(experiment_id=experiment.experiment_id)\n    num_classifier_epochs = trial.suggest_int(\"num_classifier_epochs\", 5, 5) \n    mlflow.log_param(\"num_classifier_epochs\", num_classifier_epochs)\n\n    if CLASSIFIER == \"FNN\":\n        hidden_size = trial.suggest_int(\"fnn_hidden\", 192, 384)\n        learning_rate = trial.suggest_float(\"learning_rate\", 0.0001, 0.0025)\n\n        mlflow.log_param(\"classifier\", \"FNN\")\n        mlflow.log_param(\"fnn_hidden\", hidden_size)\n        mlflow.log_param(\"learning_rate\", learning_rate)\n\n        model = nn.Sequential(\n            nn.Linear(784, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, 10)\n        ).to(device)\n\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n    elif CLASSIFIER == \"CNN\":\n        filters1 = trial.suggest_int(\"filters1\", 16, 64, step=16)\n        filters2 = trial.suggest_int(\"filters2\", 32, 128, step=32)\n        kernel1 = trial.suggest_int(\"kernel1\", 3, 5)\n        kernel2 = trial.suggest_int(\"kernel2\", 3, 5)\n        learning_rate = trial.suggest_float(\"learning_rate\", 0.0001, 0.0025)\n\n        mlflow.log_param(\"classifier\", \"CNN\")\n        mlflow.log_param(\"filters1\", filters1)\n        mlflow.log_param(\"filters2\", filters2)\n        mlflow.log_param(\"kernel1\", kernel1)\n        mlflow.log_param(\"kernel2\", kernel2)\n        mlflow.log_param(\"learning_rate\", learning_rate)\n\n        model = FashionCNN(filters1, filters2, kernel1, kernel2).to(device)\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n      \n    elif CLASSIFIER == \"LogisticRegression\":\n        mlflow.log_param(\"classifier\", \"LogisticRegression\")\n    \n        # Prepare data for Logistic Regression (Flatten 28x28 images to 784 features)\n        train_features = train_dataset.data.view(-1, 784).numpy()\n        train_labels = train_dataset.targets.numpy()\n        test_features = test_dataset.data.view(-1, 784).numpy()\n        test_labels = test_dataset.targets.numpy()\n    \n        # Normalize the pixel values to [0,1] for better convergence\n        train_features = train_features / 255.0\n        test_features = test_features / 255.0\n    \n    \n        C = trial.suggest_float(\"C\", 0.01, 10.0, log=True)  \n        solver = \"saga\" \n    \n        model = LogisticRegression(C=C, max_iter=num_classifier_epochs, solver=solver)\n        model.fit(train_features, train_labels)\n    \n    \n        predictions = model.predict(test_features)\n        accuracy = accuracy_score(test_labels, predictions) * 100\n        \n        macro_f1 = f1_score(test_labels, predictions, average=\"macro\") #for f1\n        print(f\"Logistic Regression Test Accuracy: {accuracy:.2f}%\")\n        print(f\"Macro F1 Score: {macro_f1:.4f}\") #for f1\n    \n        mlflow.log_param(\"C\", C)\n        mlflow.log_metric(\"test_accuracy\", accuracy)\n        mlflow.log_metric(\"macro_f1\", macro_f1) #for f1\n        mlflow.end_run()\n        return accuracy\n\n    # Training Loop for FNN and CNN\n    criterion = nn.CrossEntropyLoss()\n\n    model.train()\n    for epoch in range(num_classifier_epochs):\n        running_loss = 0.0\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images) if CLASSIFIER == \"CNN\" else model(images.view(images.size(0), -1))\n\n            optimizer.zero_grad()\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n\n        print(f\"{CLASSIFIER} Epoch {epoch+1}: loss = {running_loss / len(train_loader):.4f}\")\n\n    # Model Evaluation\n    model.eval()\n    correct, total = 0, 0\n    all_preds = []   # for f1\n    all_labels = [] \n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images) if CLASSIFIER == \"CNN\" else model(images.view(images.size(0), -1))\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            all_preds.extend(predicted.cpu().numpy())   #for f1\n            all_labels.extend(labels.cpu().numpy()) #for f1\n\n    accuracy = 100 * correct / total\n    macro_f1 = f1_score(all_labels, all_preds, average=\"macro\") #for f1\n    print(f\"Test Accuracy: {accuracy:.2f}%\")\n    print(f\"Macro F1 Score: {macro_f1:.4f}\") #for f1\n\n    mlflow.log_metric(\"test_accuracy\", accuracy)\n    mlflow.log_metric(\"macro_f1\", macro_f1) #for f1\n    mlflow.end_run()\n    return accuracy\n\nif __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=50)\n    print(f\"Best Parameters for {CLASSIFIER}:\", study.best_params)\n    print(\"Best Accuracy:\", study.best_value)\n\n\nCNN Epoch 1: loss = 0.4142\nCNN Epoch 2: loss = 0.3038\nCNN Epoch 3: loss = 0.2718\nCNN Epoch 4: loss = 0.2433\nCNN Epoch 5: loss = 0.2250\nTest Accuracy: 89.01%\nMacro F1 Score: 0.8920\nCNN Epoch 1: loss = 0.4187\nCNN Epoch 2: loss = 0.2990\nCNN Epoch 3: loss = 0.2635\nCNN Epoch 4: loss = 0.2406\nCNN Epoch 5: loss = 0.2229\nTest Accuracy: 89.64%\nMacro F1 Score: 0.8964\nCNN Epoch 1: loss = 0.7600\nCNN Epoch 2: loss = 0.4102\nCNN Epoch 3: loss = 0.3472\nCNN Epoch 4: loss = 0.3177\nCNN Epoch 5: loss = 0.2972\nTest Accuracy: 88.70%\nMacro F1 Score: 0.8863\nCNN Epoch 1: loss = 0.4309\nCNN Epoch 2: loss = 0.3029\nCNN Epoch 3: loss = 0.2658\nCNN Epoch 4: loss = 0.2438\nCNN Epoch 5: loss = 0.2262\nTest Accuracy: 90.06%\nMacro F1 Score: 0.9009\nCNN Epoch 1: loss = 0.4653\nCNN Epoch 2: loss = 0.3246\nCNN Epoch 3: loss = 0.2935\nCNN Epoch 4: loss = 0.2697\nCNN Epoch 5: loss = 0.2527\nTest Accuracy: 89.10%\nMacro F1 Score: 0.8901\nCNN Epoch 1: loss = 0.6973\nCNN Epoch 2: loss = 0.3831\nCNN Epoch 3: loss = 0.3287\nCNN Epoch 4: loss = 0.2999\nCNN Epoch 5: loss = 0.2794\nTest Accuracy: 89.05%\nMacro F1 Score: 0.8893\nCNN Epoch 1: loss = 0.5288\nCNN Epoch 2: loss = 0.3632\nCNN Epoch 3: loss = 0.3297\nCNN Epoch 4: loss = 0.3076\nCNN Epoch 5: loss = 0.2952\nTest Accuracy: 87.87%\nMacro F1 Score: 0.8755\nCNN Epoch 1: loss = 0.4134\nCNN Epoch 2: loss = 0.2940\nCNN Epoch 3: loss = 0.2597\nCNN Epoch 4: loss = 0.2390\nCNN Epoch 5: loss = 0.2225\nTest Accuracy: 89.62%\nMacro F1 Score: 0.8967\nCNN Epoch 1: loss = 0.5063\nCNN Epoch 2: loss = 0.3385\nCNN Epoch 3: loss = 0.2986\nCNN Epoch 4: loss = 0.2803\nCNN Epoch 5: loss = 0.2600\nTest Accuracy: 88.94%\nMacro F1 Score: 0.8894\nCNN Epoch 1: loss = 0.4016\nCNN Epoch 2: loss = 0.2801\nCNN Epoch 3: loss = 0.2490\nCNN Epoch 4: loss = 0.2251\nCNN Epoch 5: loss = 0.2067\nTest Accuracy: 90.66%\nMacro F1 Score: 0.9066\nCNN Epoch 1: loss = 0.4207\nCNN Epoch 2: loss = 0.3202\nCNN Epoch 3: loss = 0.2878\nCNN Epoch 4: loss = 0.2679\nCNN Epoch 5: loss = 0.2524\nTest Accuracy: 89.03%\nMacro F1 Score: 0.8876\nCNN Epoch 1: loss = 0.4363\nCNN Epoch 2: loss = 0.3068\nCNN Epoch 3: loss = 0.2734\nCNN Epoch 4: loss = 0.2506\nCNN Epoch 5: loss = 0.2341\nTest Accuracy: 90.32%\nMacro F1 Score: 0.9029\nCNN Epoch 1: loss = 0.4417\nCNN Epoch 2: loss = 0.3104\nCNN Epoch 3: loss = 0.2801\nCNN Epoch 4: loss = 0.2627\nCNN Epoch 5: loss = 0.2508\nTest Accuracy: 90.57%\nMacro F1 Score: 0.9036\nCNN Epoch 1: loss = 0.4490\nCNN Epoch 2: loss = 0.3182\nCNN Epoch 3: loss = 0.2877\nCNN Epoch 4: loss = 0.2672\nCNN Epoch 5: loss = 0.2551\nTest Accuracy: 90.72%\nMacro F1 Score: 0.9071\nCNN Epoch 1: loss = 0.4475\nCNN Epoch 2: loss = 0.3125\nCNN Epoch 3: loss = 0.2839\nCNN Epoch 4: loss = 0.2675\nCNN Epoch 5: loss = 0.2553\nTest Accuracy: 89.95%\nMacro F1 Score: 0.8995\nCNN Epoch 1: loss = 0.4153\nCNN Epoch 2: loss = 0.3026\nCNN Epoch 3: loss = 0.2689\nCNN Epoch 4: loss = 0.2477\nCNN Epoch 5: loss = 0.2294\nTest Accuracy: 90.06%\nMacro F1 Score: 0.8993\nCNN Epoch 1: loss = 0.4508\nCNN Epoch 2: loss = 0.3101\nCNN Epoch 3: loss = 0.2834\nCNN Epoch 4: loss = 0.2615\nCNN Epoch 5: loss = 0.2463\nTest Accuracy: 90.46%\nMacro F1 Score: 0.9057\nCNN Epoch 1: loss = 0.5764\nCNN Epoch 2: loss = 0.3532\nCNN Epoch 3: loss = 0.3157\nCNN Epoch 4: loss = 0.2901\nCNN Epoch 5: loss = 0.2754\nTest Accuracy: 88.71%\nMacro F1 Score: 0.8865\nCNN Epoch 1: loss = 0.4089\nCNN Epoch 2: loss = 0.3025\nCNN Epoch 3: loss = 0.2707\nCNN Epoch 4: loss = 0.2446\nCNN Epoch 5: loss = 0.2278\nTest Accuracy: 90.61%\nMacro F1 Score: 0.9052\nCNN Epoch 1: loss = 0.4471\nCNN Epoch 2: loss = 0.3439\nCNN Epoch 3: loss = 0.3163\nCNN Epoch 4: loss = 0.2981\nCNN Epoch 5: loss = 0.2857\nTest Accuracy: 88.90%\nMacro F1 Score: 0.8888\nCNN Epoch 1: loss = 0.4267\nCNN Epoch 2: loss = 0.3039\nCNN Epoch 3: loss = 0.2697\nCNN Epoch 4: loss = 0.2476\nCNN Epoch 5: loss = 0.2324\nTest Accuracy: 90.20%\nMacro F1 Score: 0.9004\nCNN Epoch 1: loss = 0.4007\nCNN Epoch 2: loss = 0.2934\nCNN Epoch 3: loss = 0.2662\nCNN Epoch 4: loss = 0.2444\nCNN Epoch 5: loss = 0.2286\nTest Accuracy: 90.59%\nMacro F1 Score: 0.9053\nCNN Epoch 1: loss = 0.4259\nCNN Epoch 2: loss = 0.3050\nCNN Epoch 3: loss = 0.2688\nCNN Epoch 4: loss = 0.2507\nCNN Epoch 5: loss = 0.2319\nTest Accuracy: 89.37%\nMacro F1 Score: 0.8954\nCNN Epoch 1: loss = 0.4184\nCNN Epoch 2: loss = 0.3066\nCNN Epoch 3: loss = 0.2757\nCNN Epoch 4: loss = 0.2559\nCNN Epoch 5: loss = 0.2391\nTest Accuracy: 89.50%\nMacro F1 Score: 0.8956\nCNN Epoch 1: loss = 0.4162\nCNN Epoch 2: loss = 0.2946\nCNN Epoch 3: loss = 0.2613\nCNN Epoch 4: loss = 0.2372\nCNN Epoch 5: loss = 0.2225\nTest Accuracy: 90.40%\nMacro F1 Score: 0.9034\nCNN Epoch 1: loss = 0.4351\nCNN Epoch 2: loss = 0.3094\nCNN Epoch 3: loss = 0.2759\nCNN Epoch 4: loss = 0.2547\nCNN Epoch 5: loss = 0.2396\nTest Accuracy: 90.47%\nMacro F1 Score: 0.9028\nCNN Epoch 1: loss = 0.4139\nCNN Epoch 2: loss = 0.3120\nCNN Epoch 3: loss = 0.2817\nCNN Epoch 4: loss = 0.2602\nCNN Epoch 5: loss = 0.2470\nTest Accuracy: 90.54%\nMacro F1 Score: 0.9041\nCNN Epoch 1: loss = 0.5075\nCNN Epoch 2: loss = 0.3379\nCNN Epoch 3: loss = 0.3067\nCNN Epoch 4: loss = 0.2872\nCNN Epoch 5: loss = 0.2717\nTest Accuracy: 89.07%\nMacro F1 Score: 0.8904\nCNN Epoch 1: loss = 0.4301\nCNN Epoch 2: loss = 0.3226\nCNN Epoch 3: loss = 0.2851\nCNN Epoch 4: loss = 0.2600\nCNN Epoch 5: loss = 0.2436\nTest Accuracy: 88.91%\nMacro F1 Score: 0.8865\nCNN Epoch 1: loss = 0.4362\nCNN Epoch 2: loss = 0.3067\nCNN Epoch 3: loss = 0.2723\nCNN Epoch 4: loss = 0.2491\nCNN Epoch 5: loss = 0.2273\nTest Accuracy: 88.65%\nMacro F1 Score: 0.8873\nCNN Epoch 1: loss = 0.4248\nCNN Epoch 2: loss = 0.3153\nCNN Epoch 3: loss = 0.2845\nCNN Epoch 4: loss = 0.2639\nCNN Epoch 5: loss = 0.2519\nTest Accuracy: 90.41%\nMacro F1 Score: 0.9027\nCNN Epoch 1: loss = 0.4092\nCNN Epoch 2: loss = 0.2970\nCNN Epoch 3: loss = 0.2692\nCNN Epoch 4: loss = 0.2482\nCNN Epoch 5: loss = 0.2305\nTest Accuracy: 90.80%\nMacro F1 Score: 0.9069\nCNN Epoch 1: loss = 0.4054\nCNN Epoch 2: loss = 0.2987\nCNN Epoch 3: loss = 0.2691\nCNN Epoch 4: loss = 0.2478\nCNN Epoch 5: loss = 0.2322\nTest Accuracy: 90.07%\nMacro F1 Score: 0.9006\nCNN Epoch 1: loss = 0.4314\nCNN Epoch 2: loss = 0.3167\nCNN Epoch 3: loss = 0.2855\nCNN Epoch 4: loss = 0.2675\nCNN Epoch 5: loss = 0.2541\nTest Accuracy: 89.69%\nMacro F1 Score: 0.8972\nCNN Epoch 1: loss = 0.4089\nCNN Epoch 2: loss = 0.2978\nCNN Epoch 3: loss = 0.2682\nCNN Epoch 4: loss = 0.2434\nCNN Epoch 5: loss = 0.2267\nTest Accuracy: 89.76%\nMacro F1 Score: 0.8966\nCNN Epoch 1: loss = 0.4152\nCNN Epoch 2: loss = 0.3107\nCNN Epoch 3: loss = 0.2766\nCNN Epoch 4: loss = 0.2571\nCNN Epoch 5: loss = 0.2399\nTest Accuracy: 90.07%\nMacro F1 Score: 0.8994\nCNN Epoch 1: loss = 0.4580\nCNN Epoch 2: loss = 0.3330\nCNN Epoch 3: loss = 0.2997\nCNN Epoch 4: loss = 0.2813\nCNN Epoch 5: loss = 0.2663\nTest Accuracy: 89.50%\nMacro F1 Score: 0.8910\nCNN Epoch 1: loss = 0.4317\nCNN Epoch 2: loss = 0.3087\nCNN Epoch 3: loss = 0.2760\nCNN Epoch 4: loss = 0.2483\nCNN Epoch 5: loss = 0.2314\nTest Accuracy: 89.52%\nMacro F1 Score: 0.8963\nCNN Epoch 1: loss = 0.4271\nCNN Epoch 2: loss = 0.3116\nCNN Epoch 3: loss = 0.2782\nCNN Epoch 4: loss = 0.2563\nCNN Epoch 5: loss = 0.2451\nTest Accuracy: 90.13%\nMacro F1 Score: 0.8997\nCNN Epoch 1: loss = 0.4143\nCNN Epoch 2: loss = 0.3035\nCNN Epoch 3: loss = 0.2696\nCNN Epoch 4: loss = 0.2497\nCNN Epoch 5: loss = 0.2333\nTest Accuracy: 90.29%\nMacro F1 Score: 0.9026\nCNN Epoch 1: loss = 0.4845\nCNN Epoch 2: loss = 0.3127\nCNN Epoch 3: loss = 0.2779\nCNN Epoch 4: loss = 0.2596\nCNN Epoch 5: loss = 0.2401\nTest Accuracy: 89.62%\nMacro F1 Score: 0.8953\nCNN Epoch 1: loss = 0.4058\nCNN Epoch 2: loss = 0.3002\nCNN Epoch 3: loss = 0.2676\nCNN Epoch 4: loss = 0.2463\nCNN Epoch 5: loss = 0.2321\nTest Accuracy: 90.62%\nMacro F1 Score: 0.9057\nCNN Epoch 1: loss = 0.4072\nCNN Epoch 2: loss = 0.2997\nCNN Epoch 3: loss = 0.2663\nCNN Epoch 4: loss = 0.2460\nCNN Epoch 5: loss = 0.2309\nTest Accuracy: 89.75%\nMacro F1 Score: 0.8966\nCNN Epoch 1: loss = 0.4278\nCNN Epoch 2: loss = 0.3188\nCNN Epoch 3: loss = 0.2846\nCNN Epoch 4: loss = 0.2606\nCNN Epoch 5: loss = 0.2442\nTest Accuracy: 89.74%\nMacro F1 Score: 0.8971\nCNN Epoch 1: loss = 0.4355\nCNN Epoch 2: loss = 0.3071\nCNN Epoch 3: loss = 0.2800\nCNN Epoch 4: loss = 0.2587\nCNN Epoch 5: loss = 0.2459\nTest Accuracy: 89.94%\nMacro F1 Score: 0.9001\nCNN Epoch 1: loss = 0.4154\nCNN Epoch 2: loss = 0.3004\nCNN Epoch 3: loss = 0.2682\nCNN Epoch 4: loss = 0.2464\nCNN Epoch 5: loss = 0.2292\nTest Accuracy: 89.21%\nMacro F1 Score: 0.8904\nCNN Epoch 1: loss = 0.4167\nCNN Epoch 2: loss = 0.3086\nCNN Epoch 3: loss = 0.2803\nCNN Epoch 4: loss = 0.2568\nCNN Epoch 5: loss = 0.2417\nTest Accuracy: 90.07%\nMacro F1 Score: 0.9000\nCNN Epoch 1: loss = 0.4117\nCNN Epoch 2: loss = 0.3026\nCNN Epoch 3: loss = 0.2720\nCNN Epoch 4: loss = 0.2514\nCNN Epoch 5: loss = 0.2347\nTest Accuracy: 90.46%\nMacro F1 Score: 0.9016\nCNN Epoch 1: loss = 0.4738\nCNN Epoch 2: loss = 0.3234\nCNN Epoch 3: loss = 0.2905\nCNN Epoch 4: loss = 0.2703\nCNN Epoch 5: loss = 0.2589\nTest Accuracy: 90.09%\nMacro F1 Score: 0.8998\nCNN Epoch 1: loss = 0.3981\nCNN Epoch 2: loss = 0.2907\nCNN Epoch 3: loss = 0.2582\nCNN Epoch 4: loss = 0.2387\nCNN Epoch 5: loss = 0.2174\nTest Accuracy: 90.19%\nMacro F1 Score: 0.9036\nBest Parameters for CNN: {'batch_size': 64, 'num_classifier_epochs': 5, 'filters1': 32, 'filters2': 64, 'kernel1': 3, 'kernel2': 5, 'learning_rate': 0.0015478685740995355}\nBest Accuracy: 90.8\n\n[I 2025-03-28 15:09:09,142] A new study created in memory with name: no-name-50d0bad0-9df4-4c8f-ac1e-663e44131eb2\n[I 2025-03-28 15:10:00,337] Trial 0 finished with value: 89.01 and parameters: {'batch_size': 96, 'num_classifier_epochs': 5, 'filters1': 48, 'filters2': 128, 'kernel1': 4, 'kernel2': 5, 'learning_rate': 0.0015248505233275051}. Best is trial 0 with value: 89.01.\n[I 2025-03-28 15:10:47,448] Trial 1 finished with value: 89.64 and parameters: {'batch_size': 96, 'num_classifier_epochs': 5, 'filters1': 48, 'filters2': 128, 'kernel1': 4, 'kernel2': 4, 'learning_rate': 0.0012229501836420783}. Best is trial 1 with value: 89.64.\n[I 2025-03-28 15:11:21,298] Trial 2 finished with value: 88.7 and parameters: {'batch_size': 160, 'num_classifier_epochs': 5, 'filters1': 64, 'filters2': 32, 'kernel1': 5, 'kernel2': 4, 'learning_rate': 0.00013313360782682066}. Best is trial 1 with value: 89.64.\n[I 2025-03-28 15:12:04,707] Trial 3 finished with value: 90.06 and parameters: {'batch_size': 160, 'num_classifier_epochs': 5, 'filters1': 64, 'filters2': 96, 'kernel1': 5, 'kernel2': 4, 'learning_rate': 0.0013252696356154724}. Best is trial 3 with value: 90.06.\n[I 2025-03-28 15:12:37,933] Trial 4 finished with value: 89.1 and parameters: {'batch_size': 256, 'num_classifier_epochs': 5, 'filters1': 48, 'filters2': 96, 'kernel1': 5, 'kernel2': 3, 'learning_rate': 0.0016266946027195346}. Best is trial 3 with value: 90.06.\n[I 2025-03-28 15:13:07,637] Trial 5 finished with value: 89.05 and parameters: {'batch_size': 192, 'num_classifier_epochs': 5, 'filters1': 32, 'filters2': 96, 'kernel1': 4, 'kernel2': 4, 'learning_rate': 0.00017858037364493448}. Best is trial 3 with value: 90.06.\n[I 2025-03-28 15:13:33,636] Trial 6 finished with value: 87.87 and parameters: {'batch_size': 192, 'num_classifier_epochs': 5, 'filters1': 16, 'filters2': 32, 'kernel1': 4, 'kernel2': 5, 'learning_rate': 0.0007601148742759145}. Best is trial 3 with value: 90.06.\n[I 2025-03-28 15:14:12,029] Trial 7 finished with value: 89.62 and parameters: {'batch_size': 160, 'num_classifier_epochs': 5, 'filters1': 48, 'filters2': 64, 'kernel1': 3, 'kernel2': 5, 'learning_rate': 0.0020459440534046185}. Best is trial 3 with value: 90.06.\n[I 2025-03-28 15:14:42,736] Trial 8 finished with value: 88.94 and parameters: {'batch_size': 192, 'num_classifier_epochs': 5, 'filters1': 48, 'filters2': 32, 'kernel1': 4, 'kernel2': 5, 'learning_rate': 0.0006940093332337714}. Best is trial 3 with value: 90.06.\n[I 2025-03-28 15:15:35,761] Trial 9 finished with value: 90.66 and parameters: {'batch_size': 128, 'num_classifier_epochs': 5, 'filters1': 48, 'filters2': 128, 'kernel1': 3, 'kernel2': 5, 'learning_rate': 0.0014227934892389901}. Best is trial 9 with value: 90.66.\n[I 2025-03-28 15:16:23,120] Trial 10 finished with value: 89.03 and parameters: {'batch_size': 64, 'num_classifier_epochs': 5, 'filters1': 16, 'filters2': 128, 'kernel1': 3, 'kernel2': 3, 'learning_rate': 0.0023379932249639446}. Best is trial 9 with value: 90.66.\n[I 2025-03-28 15:17:07,979] Trial 11 finished with value: 90.32 and parameters: {'batch_size': 128, 'num_classifier_epochs': 5, 'filters1': 64, 'filters2': 96, 'kernel1': 3, 'kernel2': 4, 'learning_rate': 0.0011412008327813165}. Best is trial 9 with value: 90.66.\n[I 2025-03-28 15:17:43,163] Trial 12 finished with value: 90.57 and parameters: {'batch_size': 128, 'num_classifier_epochs': 5, 'filters1': 64, 'filters2': 64, 'kernel1': 3, 'kernel2': 3, 'learning_rate': 0.0008932519608388764}. Best is trial 9 with value: 90.66.\n[I 2025-03-28 15:18:20,994] Trial 13 finished with value: 90.72 and parameters: {'batch_size': 96, 'num_classifier_epochs': 5, 'filters1': 32, 'filters2': 64, 'kernel1': 3, 'kernel2': 3, 'learning_rate': 0.000759338062230861}. Best is trial 13 with value: 90.72.\n[I 2025-03-28 15:19:08,668] Trial 14 finished with value: 89.95 and parameters: {'batch_size': 64, 'num_classifier_epochs': 5, 'filters1': 32, 'filters2': 64, 'kernel1': 3, 'kernel2': 3, 'learning_rate': 0.0005171672224923642}. Best is trial 13 with value: 90.72.\n[I 2025-03-28 15:19:46,931] Trial 15 finished with value: 90.06 and parameters: {'batch_size': 96, 'num_classifier_epochs': 5, 'filters1': 32, 'filters2': 64, 'kernel1': 3, 'kernel2': 5, 'learning_rate': 0.0019349248934961896}. Best is trial 13 with value: 90.72.\n[I 2025-03-28 15:20:19,823] Trial 16 finished with value: 90.46 and parameters: {'batch_size': 128, 'num_classifier_epochs': 5, 'filters1': 32, 'filters2': 128, 'kernel1': 3, 'kernel2': 3, 'learning_rate': 0.000990739247525854}. Best is trial 13 with value: 90.72.\n[I 2025-03-28 15:20:43,224] Trial 17 finished with value: 88.71 and parameters: {'batch_size': 256, 'num_classifier_epochs': 5, 'filters1': 16, 'filters2': 96, 'kernel1': 3, 'kernel2': 4, 'learning_rate': 0.0005322502897563381}. Best is trial 13 with value: 90.72.\n[I 2025-03-28 15:21:33,148] Trial 18 finished with value: 90.61 and parameters: {'batch_size': 64, 'num_classifier_epochs': 5, 'filters1': 32, 'filters2': 64, 'kernel1': 3, 'kernel2': 5, 'learning_rate': 0.001620196730554928}. Best is trial 13 with value: 90.72.\n[I 2025-03-28 15:22:11,027] Trial 19 finished with value: 88.9 and parameters: {'batch_size': 96, 'num_classifier_epochs': 5, 'filters1': 32, 'filters2': 32, 'kernel1': 4, 'kernel2': 3, 'learning_rate': 0.001839707721726052}. Best is trial 13 with value: 90.72.\n[I 2025-03-28 15:22:53,680] Trial 20 finished with value: 90.2 and parameters: {'batch_size': 224, 'num_classifier_epochs': 5, 'filters1': 48, 'filters2': 128, 'kernel1': 3, 'kernel2': 4, 'learning_rate': 0.0024889796133159615}. Best is trial 13 with value: 90.72.\n[I 2025-03-28 15:23:43,801] Trial 21 finished with value: 90.59 and parameters: {'batch_size': 64, 'num_classifier_epochs': 5, 'filters1': 32, 'filters2': 64, 'kernel1': 3, 'kernel2': 5, 'learning_rate': 0.0015269816451829285}. Best is trial 13 with value: 90.72.\n[I 2025-03-28 15:24:14,489] Trial 22 finished with value: 89.37 and parameters: {'batch_size': 128, 'num_classifier_epochs': 5, 'filters1': 32, 'filters2': 64, 'kernel1': 3, 'kernel2': 5, 'learning_rate': 0.0016903502364860711}. Best is trial 13 with value: 90.72.\n[I 2025-03-28 15:24:59,127] Trial 23 finished with value: 89.5 and parameters: {'batch_size': 64, 'num_classifier_epochs': 5, 'filters1': 16, 'filters2': 64, 'kernel1': 3, 'kernel2': 5, 'learning_rate': 0.0013748872433316651}. Best is trial 13 with value: 90.72.\n[I 2025-03-28 15:25:38,067] Trial 24 finished with value: 90.4 and parameters: {'batch_size': 96, 'num_classifier_epochs': 5, 'filters1': 32, 'filters2': 96, 'kernel1': 3, 'kernel2': 5, 'learning_rate': 0.0010741522450846625}. Best is trial 13 with value: 90.72.\n[I 2025-03-28 15:26:13,188] Trial 25 finished with value: 90.47 and parameters: {'batch_size': 128, 'num_classifier_epochs': 5, 'filters1': 48, 'filters2': 64, 'kernel1': 4, 'kernel2': 4, 'learning_rate': 0.001336707440991061}. Best is trial 13 with value: 90.72.\n[I 2025-03-28 15:26:59,105] Trial 26 finished with value: 90.54 and parameters: {'batch_size': 64, 'num_classifier_epochs': 5, 'filters1': 32, 'filters2': 32, 'kernel1': 3, 'kernel2': 5, 'learning_rate': 0.002210539142056378}. Best is trial 13 with value: 90.72.\n[I 2025-03-28 15:27:36,611] Trial 27 finished with value: 89.07 and parameters: {'batch_size': 96, 'num_classifier_epochs': 5, 'filters1': 16, 'filters2': 96, 'kernel1': 3, 'kernel2': 4, 'learning_rate': 0.00039540026208312746}. Best is trial 13 with value: 90.72.\n[I 2025-03-28 15:28:16,775] Trial 28 finished with value: 88.91 and parameters: {'batch_size': 96, 'num_classifier_epochs': 5, 'filters1': 48, 'filters2': 64, 'kernel1': 4, 'kernel2': 5, 'learning_rate': 0.0017796312115365507}. Best is trial 13 with value: 90.72.\n[I 2025-03-28 15:28:52,596] Trial 29 finished with value: 88.65 and parameters: {'batch_size': 128, 'num_classifier_epochs': 5, 'filters1': 32, 'filters2': 128, 'kernel1': 4, 'kernel2': 5, 'learning_rate': 0.0014599676720398055}. Best is trial 13 with value: 90.72.\n[I 2025-03-28 15:29:43,201] Trial 30 finished with value: 90.41 and parameters: {'batch_size': 64, 'num_classifier_epochs': 5, 'filters1': 48, 'filters2': 64, 'kernel1': 3, 'kernel2': 3, 'learning_rate': 0.0008604550337401655}. Best is trial 13 with value: 90.72.\n[I 2025-03-28 15:30:33,482] Trial 31 finished with value: 90.8 and parameters: {'batch_size': 64, 'num_classifier_epochs': 5, 'filters1': 32, 'filters2': 64, 'kernel1': 3, 'kernel2': 5, 'learning_rate': 0.0015478685740995355}. Best is trial 31 with value: 90.8.\n[I 2025-03-28 15:31:23,711] Trial 32 finished with value: 90.07 and parameters: {'batch_size': 64, 'num_classifier_epochs': 5, 'filters1': 32, 'filters2': 64, 'kernel1': 3, 'kernel2': 5, 'learning_rate': 0.0015729170015998481}. Best is trial 31 with value: 90.8.\n[I 2025-03-28 15:32:01,635] Trial 33 finished with value: 89.69 and parameters: {'batch_size': 96, 'num_classifier_epochs': 5, 'filters1': 32, 'filters2': 32, 'kernel1': 3, 'kernel2': 5, 'learning_rate': 0.001202790673346334}. Best is trial 31 with value: 90.8.\n[I 2025-03-28 15:32:42,542] Trial 34 finished with value: 89.76 and parameters: {'batch_size': 96, 'num_classifier_epochs': 5, 'filters1': 48, 'filters2': 64, 'kernel1': 3, 'kernel2': 5, 'learning_rate': 0.00200718923791879}. Best is trial 31 with value: 90.8.\n[I 2025-03-28 15:33:32,313] Trial 35 finished with value: 90.07 and parameters: {'batch_size': 64, 'num_classifier_epochs': 5, 'filters1': 32, 'filters2': 96, 'kernel1': 5, 'kernel2': 4, 'learning_rate': 0.0017332842584397142}. Best is trial 31 with value: 90.8.\n[I 2025-03-28 15:33:59,965] Trial 36 finished with value: 89.5 and parameters: {'batch_size': 160, 'num_classifier_epochs': 5, 'filters1': 16, 'filters2': 64, 'kernel1': 3, 'kernel2': 4, 'learning_rate': 0.0012724540542330427}. Best is trial 31 with value: 90.8.\n[I 2025-03-28 15:34:46,415] Trial 37 finished with value: 89.52 and parameters: {'batch_size': 160, 'num_classifier_epochs': 5, 'filters1': 64, 'filters2': 96, 'kernel1': 4, 'kernel2': 5, 'learning_rate': 0.0014487192931249753}. Best is trial 31 with value: 90.8.\n[I 2025-03-28 15:35:24,262] Trial 38 finished with value: 90.13 and parameters: {'batch_size': 96, 'num_classifier_epochs': 5, 'filters1': 48, 'filters2': 32, 'kernel1': 3, 'kernel2': 5, 'learning_rate': 0.0018917869487398652}. Best is trial 31 with value: 90.8.\n[I 2025-03-28 15:36:06,806] Trial 39 finished with value: 90.29 and parameters: {'batch_size': 96, 'num_classifier_epochs': 5, 'filters1': 48, 'filters2': 96, 'kernel1': 4, 'kernel2': 4, 'learning_rate': 0.0015880880328107737}. Best is trial 31 with value: 90.8.\n[I 2025-03-28 15:36:56,119] Trial 40 finished with value: 89.62 and parameters: {'batch_size': 64, 'num_classifier_epochs': 5, 'filters1': 32, 'filters2': 64, 'kernel1': 3, 'kernel2': 5, 'learning_rate': 0.00025652536965903435}. Best is trial 31 with value: 90.8.\n[I 2025-03-28 15:37:45,573] Trial 41 finished with value: 90.62 and parameters: {'batch_size': 64, 'num_classifier_epochs': 5, 'filters1': 32, 'filters2': 64, 'kernel1': 3, 'kernel2': 5, 'learning_rate': 0.0014891838000063498}. Best is trial 31 with value: 90.8.\n[I 2025-03-28 15:38:34,993] Trial 42 finished with value: 89.75 and parameters: {'batch_size': 64, 'num_classifier_epochs': 5, 'filters1': 32, 'filters2': 64, 'kernel1': 3, 'kernel2': 5, 'learning_rate': 0.0016906631687284544}. Best is trial 31 with value: 90.8.\n[I 2025-03-28 15:39:25,040] Trial 43 finished with value: 89.74 and parameters: {'batch_size': 64, 'num_classifier_epochs': 5, 'filters1': 32, 'filters2': 64, 'kernel1': 5, 'kernel2': 5, 'learning_rate': 0.0013995085634476376}. Best is trial 31 with value: 90.8.\n[I 2025-03-28 15:39:56,426] Trial 44 finished with value: 89.94 and parameters: {'batch_size': 128, 'num_classifier_epochs': 5, 'filters1': 32, 'filters2': 32, 'kernel1': 3, 'kernel2': 5, 'learning_rate': 0.0010698644382101088}. Best is trial 31 with value: 90.8.\n[I 2025-03-28 15:40:34,772] Trial 45 finished with value: 89.21 and parameters: {'batch_size': 96, 'num_classifier_epochs': 5, 'filters1': 32, 'filters2': 64, 'kernel1': 3, 'kernel2': 5, 'learning_rate': 0.0012315042772556648}. Best is trial 31 with value: 90.8.\n[I 2025-03-28 15:41:19,478] Trial 46 finished with value: 90.07 and parameters: {'batch_size': 64, 'num_classifier_epochs': 5, 'filters1': 16, 'filters2': 64, 'kernel1': 3, 'kernel2': 5, 'learning_rate': 0.0016297503646640705}. Best is trial 31 with value: 90.8.\n[I 2025-03-28 15:42:01,265] Trial 47 finished with value: 90.46 and parameters: {'batch_size': 96, 'num_classifier_epochs': 5, 'filters1': 48, 'filters2': 128, 'kernel1': 3, 'kernel2': 3, 'learning_rate': 0.002079290834484217}. Best is trial 31 with value: 90.8.\n[I 2025-03-28 15:42:33,040] Trial 48 finished with value: 90.09 and parameters: {'batch_size': 128, 'num_classifier_epochs': 5, 'filters1': 32, 'filters2': 64, 'kernel1': 3, 'kernel2': 4, 'learning_rate': 0.0006739979013646886}. Best is trial 31 with value: 90.8.\n[I 2025-03-28 15:43:27,818] Trial 49 finished with value: 90.19 and parameters: {'batch_size': 64, 'num_classifier_epochs': 5, 'filters1': 48, 'filters2': 96, 'kernel1': 3, 'kernel2': 5, 'learning_rate': 0.0014883408074560338}. Best is trial 31 with value: 90.8.\n\n\n\nTest Accuracy Based on the Number of Filters in the First Conv2D Layer \nTest Accuracy Based on the Number of Filters in the Second Conv2D Layer \nTest Accuracy Based on Kernel Size in the First Conv2D Layer \nTest Accuracy Based on Kernel Size in the Second Conv2D Layer \nModel 4: Logistic Regression on RBM Hidden Features (of Fashion MNIST Data)\n\n\nClick to Show Code and Output\n\n\n\nCode\nfrom sklearn.metrics import accuracy_score, f1_score\nCLASSIFIER = 'LogisticRegression'\n\nif CLASSIFIER == 'LogisticRegression':\n    experiment = mlflow.set_experiment(\"new-pytorch-fmnist-lr-withrbm\")\nelse:\n    experiment = mlflow.set_experiment(\"new-pytorch-fmnist-fnn-withrbm\")\n\n\nclass RBM(nn.Module):\n    def __init__(self, n_visible=784, n_hidden=256, k=1):\n        super(RBM, self).__init__()\n        self.n_visible = n_visible\n        self.n_hidden = n_hidden\n        # Initialize weights and biases\n        self.W = nn.Parameter(torch.randn(n_hidden, n_visible) * 0.1)\n        self.v_bias = nn.Parameter(torch.zeros(n_visible))\n        self.h_bias = nn.Parameter(torch.zeros(n_hidden))\n        self.k = k  # CD-k steps\n\n    def sample_h(self, v):\n        # Given visible v, sample hidden h\n        p_h = torch.sigmoid(F.linear(v, self.W, self.h_bias))  # p(h=1|v)\n        h_sample = torch.bernoulli(p_h)                        # sample Bernoulli\n        return p_h, h_sample\n\n    def sample_v(self, h):\n        # Given hidden h, sample visible v\n        p_v = torch.sigmoid(F.linear(h, self.W.t(), self.v_bias))  # p(v=1|h)\n        v_sample = torch.bernoulli(p_v)\n        return p_v, v_sample\n\n    def forward(self, v):\n        # Perform k steps of contrastive divergence starting from v\n        v_k = v.clone()\n        for _ in range(self.k):\n            _, h_k = self.sample_h(v_k)    # sample hidden from current visible\n            _, v_k = self.sample_v(h_k)    # sample visible from hidden\n        return v_k  # k-step reconstructed visible\n\n    def free_energy(self, v):\n        # Compute the visible bias term for each sample in the batch\n        vbias_term = (v * self.v_bias).sum(dim=1)  # shape: [batch_size]\n        # Compute the activation of the hidden units\n        wx_b = F.linear(v, self.W, self.h_bias)     # shape: [batch_size, n_hidden]\n        # Compute the hidden term\n        hidden_term = torch.sum(torch.log1p(torch.exp(wx_b)), dim=1)  # shape: [batch_size]\n        # Return the mean free energy over the batch\n        return - (vbias_term + hidden_term).mean()\n    \ntransform = transforms.Compose([transforms.ToTensor()])\ntrain_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\ntest_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n\ndef objective(trial):\n    num_rbm_epochs = trial.suggest_int(\"num_rbm_epochs\", 5, 5)# 24, 33)\n    batch_size = trial.suggest_int(\"batch_size\", 192, 1024)\n    rbm_lr = trial.suggest_float(\"rbm_lr\", 0.05, 0.1)\n    rbm_hidden = trial.suggest_int(\"rbm_hidden\", 384, 8192)\n\n    mlflow.start_run(experiment_id=experiment.experiment_id)\n    if CLASSIFIER != 'LogisticRegression':\n        fnn_hidden = trial.suggest_int(\"fnn_hidden\", 192, 384)\n        fnn_lr = trial.suggest_float(\"fnn_lr\", 0.0001, 0.0025)\n        mlflow.log_param(\"fnn_hidden\", fnn_hidden)\n        mlflow.log_param(\"fnn_lr\", fnn_lr)\n\n    num_classifier_epochs = trial.suggest_int(\"num_classifier_epochs\", 5, 5)# 40, 60)\n\n    mlflow.log_param(\"num_rbm_epochs\", num_rbm_epochs)\n    mlflow.log_param(\"batch_size\", batch_size)\n    mlflow.log_param(\"rbm_lr\", rbm_lr)\n    mlflow.log_param(\"rbm_hidden\", rbm_hidden)\n    mlflow.log_param(\"num_classifier_epochs\", num_classifier_epochs)\n\n    # Instantiate RBM and optimizer\n    device = torch.device(\"mps\")\n    rbm = RBM(n_visible=784, n_hidden=rbm_hidden, k=1).to(device)\n    optimizer = torch.optim.SGD(rbm.parameters(), lr=rbm_lr)\n\n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n    rbm_training_failed = False\n    # Training loop (assuming train_loader yields batches of images and labels)\n    for epoch in range(num_rbm_epochs):\n        total_loss = 0.0\n        for images, _ in train_loader:\n            # Flatten images and binarize\n            v0 = images.view(-1, 784).to(rbm.W.device)      # shape [batch_size, 784]\n            v0 = torch.bernoulli(v0)                        # sample binary input\n            vk = rbm(v0)                                    # k-step CD reconstruction\n            # Compute contrastive divergence loss (free energy difference)\n            loss = rbm.free_energy(v0) - rbm.free_energy(vk)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n        print(f\"Epoch {epoch+1}: avg free-energy loss = {total_loss/len(train_loader):.4f}\")\n        if np.isnan(total_loss):\n            rbm_training_failed = True\n            break\n\n        if rbm_training_failed:\n            accuracy = 0.0\n            macro_f1 = 0.0 \n            print(\"RBM training failed — returning 0.0 for accuracy and macro F1\")  # 🔥 for visibility\n            mlflow.log_metric(\"test_accuracy\", accuracy)\n            mlflow.log_metric(\"macro_f1\", macro_f1)\n            mlflow.set_tag(\"status\", \"rbm_failed\")  # Optional tag\n            mlflow.end_run()\n            return float(accuracy)\n    else:\n        rbm.eval()  # set in evaluation mode if using any layers that behave differently in training\n        features_list = []\n        labels_list = []\n        for images, labels in train_loader:\n            v = images.view(-1, 784).to(rbm.W.device)\n            v = v  # (optionally binarize or use raw normalized pixels)\n            h_prob, h_sample = rbm.sample_h(v)  # get hidden activations\n            features_list.append(h_prob.cpu().detach().numpy())\n            labels_list.append(labels.numpy())\n        train_features = np.concatenate(features_list)  # shape: [N_train, n_hidden]\n        train_labels = np.concatenate(labels_list)\n\n        # Convert pre-extracted training features and labels to tensors and create a DataLoader\n        train_features_tensor = torch.tensor(train_features, dtype=torch.float32)\n        train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n        train_feature_dataset = torch.utils.data.TensorDataset(train_features_tensor, train_labels_tensor)\n        train_feature_loader = torch.utils.data.DataLoader(train_feature_dataset, batch_size=batch_size, shuffle=True)\n\n            \n        if CLASSIFIER == 'LogisticRegression':\n            # add optuna tuning same as log reg without RBM features...\n            lr_C = trial.suggest_float(\"lr_C\", 0.01, 10.0, log=True)  \n            mlflow.log_param(\"lr_C\", lr_C)  # Log the chosen C value\n\n            classifier = LogisticRegression(max_iter=num_classifier_epochs, C=lr_C, solver=\"saga\") \n            classifier.fit(train_features, train_labels)            \n            \n        else:\n            classifier = nn.Sequential(\n                nn.Linear(rbm.n_hidden, fnn_hidden),\n                nn.ReLU(),\n                nn.Linear(fnn_hidden, 10)\n            )\n\n            # Move classifier to the same device as the RBM\n            classifier = classifier.to(device)\n            criterion = nn.CrossEntropyLoss()\n            classifier_optimizer = torch.optim.Adam(classifier.parameters(), lr=fnn_lr)\n\n            classifier.train()\n            for epoch in range(num_classifier_epochs):\n                running_loss = 0.0\n                for features, labels in train_feature_loader:\n                    features = features.to(device)\n                    labels = labels.to(device)\n                    \n                    # Forward pass through classifier\n                    outputs = classifier(features)\n                    loss = criterion(outputs, labels)\n                    \n                    # Backpropagation and optimization\n                    classifier_optimizer.zero_grad()\n                    loss.backward()\n                    classifier_optimizer.step()\n                    \n                    running_loss += loss.item()\n                avg_loss = running_loss / len(train_feature_loader)\n                print(f\"Classifier Epoch {epoch+1}: loss = {avg_loss:.4f}\")\n\n        # Evaluate the classifier on test data.\n        # Here we extract features from the RBM for each test image.\n        if CLASSIFIER != 'LogisticRegression':\n            classifier.eval()\n            correct = 0\n            total = 0\n        features_list = []\n        labels_list = []\n        with torch.no_grad():\n            for images, labels in test_loader:\n                v = images.view(-1, 784).to(device)\n                # Extract hidden activations; you can use either h_prob or h_sample.\n                h_prob, _ = rbm.sample_h(v)\n                if CLASSIFIER == 'LogisticRegression':\n                    features_list.append(h_prob.cpu().detach().numpy())\n                    labels_list.append(labels.numpy())\n                else:\n                    outputs = classifier(h_prob)\n                    _, predicted = torch.max(outputs.data, 1)\n                    total += labels.size(0)\n                    correct += (predicted.cpu() == labels).sum().item()\n\n        if CLASSIFIER == 'LogisticRegression':\n            test_features = np.concatenate(features_list)\n            test_labels = np.concatenate(labels_list)\n            predictions = classifier.predict(test_features)\n            accuracy = accuracy_score(test_labels, predictions) * 100\n        \n            macro_f1 = f1_score(test_labels, predictions, average=\"macro\") \n        \n        else:\n            accuracy = 100 * correct / total\n        \n            all_preds = [] \n            all_labels = [] \n            classifier.eval()\n            with torch.no_grad():\n                for images, labels in test_loader:\n                    v = images.view(-1, 784).to(device)\n                    h_prob, _ = rbm.sample_h(v)\n                    outputs = classifier(h_prob)\n                    _, predicted = torch.max(outputs.data, 1)\n                    all_preds.extend(predicted.cpu().numpy()) \n                    all_labels.extend(labels.numpy()) \n        \n            macro_f1 = f1_score(all_labels, all_preds, average=\"macro\") \n        \n        print(f\"Test Accuracy: {accuracy:.2f}%\")\n        print(f\"Macro F1 Score: {macro_f1:.4f}\") \n        \n        mlflow.log_metric(\"test_accuracy\", accuracy)\n        mlflow.log_metric(\"macro_f1\", macro_f1) \n        mlflow.end_run()\n        return float(accuracy if accuracy is not None else 0.0)\n\nif __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=50)\n    print(study.best_params)\n    print(study.best_value)\n    print(study.best_trial)\n\n\nEpoch 1: avg free-energy loss = 196.3455\nEpoch 2: avg free-energy loss = 57.6702\nEpoch 3: avg free-energy loss = 38.4611\nEpoch 4: avg free-energy loss = 29.1952\nEpoch 5: avg free-energy loss = 23.7323\nTest Accuracy: 86.70%\nMacro F1 Score: 0.8661\nEpoch 1: avg free-energy loss = 193.4051\nEpoch 2: avg free-energy loss = 56.0932\nEpoch 3: avg free-energy loss = 38.1652\nEpoch 4: avg free-energy loss = 29.3080\nEpoch 5: avg free-energy loss = 23.3318\nTest Accuracy: 85.46%\nMacro F1 Score: 0.8536\nEpoch 1: avg free-energy loss = 68.6174\nEpoch 2: avg free-energy loss = 18.3964\nEpoch 3: avg free-energy loss = 12.4841\nEpoch 4: avg free-energy loss = 9.9436\nEpoch 5: avg free-energy loss = 8.4359\nTest Accuracy: 85.89%\nMacro F1 Score: 0.8572\nEpoch 1: avg free-energy loss = 153.2774\nEpoch 2: avg free-energy loss = 45.3368\nEpoch 3: avg free-energy loss = 31.0316\nEpoch 4: avg free-energy loss = 24.8075\nEpoch 5: avg free-energy loss = 20.9564\nTest Accuracy: 86.49%\nMacro F1 Score: 0.8631\nEpoch 1: avg free-energy loss = 5.0666\nEpoch 2: avg free-energy loss = -2.4851\nEpoch 3: avg free-energy loss = -2.6238\nEpoch 4: avg free-energy loss = -2.6816\nEpoch 5: avg free-energy loss = -2.3391\nTest Accuracy: 85.80%\nMacro F1 Score: 0.8564\nEpoch 1: avg free-energy loss = 172.9010\nEpoch 2: avg free-energy loss = 38.1210\nEpoch 3: avg free-energy loss = 26.1850\nEpoch 4: avg free-energy loss = 20.3122\nEpoch 5: avg free-energy loss = 17.5632\nTest Accuracy: 86.32%\nMacro F1 Score: 0.8626\nEpoch 1: avg free-energy loss = 11.2064\nEpoch 2: avg free-energy loss = -4.4292\nEpoch 3: avg free-energy loss = -4.5026\nEpoch 4: avg free-energy loss = -4.8340\nEpoch 5: avg free-energy loss = -4.8850\nTest Accuracy: 85.29%\nMacro F1 Score: 0.8517\nEpoch 1: avg free-energy loss = -9.3893\nEpoch 2: avg free-energy loss = -13.4576\nEpoch 3: avg free-energy loss = -11.4321\nEpoch 4: avg free-energy loss = -10.5042\nEpoch 5: avg free-energy loss = -9.7352\nTest Accuracy: 85.17%\nMacro F1 Score: 0.8506\nEpoch 1: avg free-energy loss = 9.0801\nEpoch 2: avg free-energy loss = 0.1499\nEpoch 3: avg free-energy loss = 0.1757\nEpoch 4: avg free-energy loss = -0.0278\nEpoch 5: avg free-energy loss = 0.0360\nTest Accuracy: 86.10%\nMacro F1 Score: 0.8605\nEpoch 1: avg free-energy loss = 46.4491\nEpoch 2: avg free-energy loss = 12.1879\nEpoch 3: avg free-energy loss = 9.0373\nEpoch 4: avg free-energy loss = 7.4208\nEpoch 5: avg free-energy loss = 6.5886\nTest Accuracy: 86.51%\nMacro F1 Score: 0.8644\nEpoch 1: avg free-energy loss = 458.1128\nEpoch 2: avg free-energy loss = 132.8139\nEpoch 3: avg free-energy loss = 86.0433\nEpoch 4: avg free-energy loss = 67.3414\nEpoch 5: avg free-energy loss = 55.3441\nTest Accuracy: 86.51%\nMacro F1 Score: 0.8644\nEpoch 1: avg free-energy loss = 101.0917\nEpoch 2: avg free-energy loss = 27.5174\nEpoch 3: avg free-energy loss = 17.7405\nEpoch 4: avg free-energy loss = 13.9116\nEpoch 5: avg free-energy loss = 11.9980\nTest Accuracy: 86.19%\nMacro F1 Score: 0.8619\nEpoch 1: avg free-energy loss = 64.8062\nEpoch 2: avg free-energy loss = 22.3084\nEpoch 3: avg free-energy loss = 17.7644\nEpoch 4: avg free-energy loss = 15.4338\nEpoch 5: avg free-energy loss = 13.7978\nTest Accuracy: 86.75%\nMacro F1 Score: 0.8667\nEpoch 1: avg free-energy loss = 78.5650\nEpoch 2: avg free-energy loss = 26.2359\nEpoch 3: avg free-energy loss = 20.0685\nEpoch 4: avg free-energy loss = 16.9519\nEpoch 5: avg free-energy loss = 15.0379\nTest Accuracy: 86.67%\nMacro F1 Score: 0.8658\nEpoch 1: avg free-energy loss = 350.3286\nEpoch 2: avg free-energy loss = 108.6761\nEpoch 3: avg free-energy loss = 72.9799\nEpoch 4: avg free-energy loss = 55.5765\nEpoch 5: avg free-energy loss = 44.3201\nTest Accuracy: 86.37%\nMacro F1 Score: 0.8623\nEpoch 1: avg free-energy loss = 107.4147\nEpoch 2: avg free-energy loss = 36.1246\nEpoch 3: avg free-energy loss = 27.0512\nEpoch 4: avg free-energy loss = 22.4735\nEpoch 5: avg free-energy loss = 18.8480\nTest Accuracy: 86.78%\nMacro F1 Score: 0.8667\nEpoch 1: avg free-energy loss = 62.1499\nEpoch 2: avg free-energy loss = 20.3970\nEpoch 3: avg free-energy loss = 15.6454\nEpoch 4: avg free-energy loss = 13.4197\nEpoch 5: avg free-energy loss = 11.8039\nTest Accuracy: 86.82%\nMacro F1 Score: 0.8676\nEpoch 1: avg free-energy loss = 210.0479\nEpoch 2: avg free-energy loss = 61.5162\nEpoch 3: avg free-energy loss = 40.8242\nEpoch 4: avg free-energy loss = 32.0612\nEpoch 5: avg free-energy loss = 26.5049\nTest Accuracy: 86.56%\nMacro F1 Score: 0.8650\nEpoch 1: avg free-energy loss = 53.0006\nEpoch 2: avg free-energy loss = 17.9123\nEpoch 3: avg free-energy loss = 14.0345\nEpoch 4: avg free-energy loss = 12.1547\nEpoch 5: avg free-energy loss = 10.7802\nTest Accuracy: 86.39%\nMacro F1 Score: 0.8635\nEpoch 1: avg free-energy loss = 139.4416\nEpoch 2: avg free-energy loss = 39.5729\nEpoch 3: avg free-energy loss = 27.4402\nEpoch 4: avg free-energy loss = 21.8295\nEpoch 5: avg free-energy loss = 18.7981\nTest Accuracy: 86.46%\nMacro F1 Score: 0.8639\nEpoch 1: avg free-energy loss = 63.5245\nEpoch 2: avg free-energy loss = 15.8937\nEpoch 3: avg free-energy loss = 10.5403\nEpoch 4: avg free-energy loss = 8.2907\nEpoch 5: avg free-energy loss = 6.9727\nTest Accuracy: 86.52%\nMacro F1 Score: 0.8645\nEpoch 1: avg free-energy loss = 69.5283\nEpoch 2: avg free-energy loss = 22.5561\nEpoch 3: avg free-energy loss = 17.4453\nEpoch 4: avg free-energy loss = 14.9944\nEpoch 5: avg free-energy loss = 13.4487\nTest Accuracy: 86.69%\nMacro F1 Score: 0.8662\nEpoch 1: avg free-energy loss = 65.5971\nEpoch 2: avg free-energy loss = 23.8052\nEpoch 3: avg free-energy loss = 19.6401\nEpoch 4: avg free-energy loss = 17.0054\nEpoch 5: avg free-energy loss = 15.6487\nTest Accuracy: 86.56%\nMacro F1 Score: 0.8646\nEpoch 1: avg free-energy loss = 97.8708\nEpoch 2: avg free-energy loss = 26.8091\nEpoch 3: avg free-energy loss = 19.4019\nEpoch 4: avg free-energy loss = 15.8896\nEpoch 5: avg free-energy loss = 13.8290\nTest Accuracy: 86.46%\nMacro F1 Score: 0.8639\nEpoch 1: avg free-energy loss = 62.3792\nEpoch 2: avg free-energy loss = 18.5537\nEpoch 3: avg free-energy loss = 13.8053\nEpoch 4: avg free-energy loss = 11.6182\nEpoch 5: avg free-energy loss = 10.1345\nTest Accuracy: 86.41%\nMacro F1 Score: 0.8624\nEpoch 1: avg free-energy loss = 140.3730\nEpoch 2: avg free-energy loss = 40.8663\nEpoch 3: avg free-energy loss = 28.1880\nEpoch 4: avg free-energy loss = 22.3220\nEpoch 5: avg free-energy loss = 18.8671\nTest Accuracy: 86.72%\nMacro F1 Score: 0.8661\nEpoch 1: avg free-energy loss = 35.9828\nEpoch 2: avg free-energy loss = 9.1321\nEpoch 3: avg free-energy loss = 6.1407\nEpoch 4: avg free-energy loss = 4.7699\nEpoch 5: avg free-energy loss = 4.0476\nTest Accuracy: 86.65%\nMacro F1 Score: 0.8655\nEpoch 1: avg free-energy loss = 53.7467\nEpoch 2: avg free-energy loss = 18.0739\nEpoch 3: avg free-energy loss = 14.2344\nEpoch 4: avg free-energy loss = 12.3961\nEpoch 5: avg free-energy loss = 11.0257\nTest Accuracy: 86.56%\nMacro F1 Score: 0.8648\nEpoch 1: avg free-energy loss = 188.3934\nEpoch 2: avg free-energy loss = 54.3442\nEpoch 3: avg free-energy loss = 36.2564\nEpoch 4: avg free-energy loss = 27.5151\nEpoch 5: avg free-energy loss = 22.8576\nTest Accuracy: 86.58%\nMacro F1 Score: 0.8651\nEpoch 1: avg free-energy loss = 113.5767\nEpoch 2: avg free-energy loss = 35.7491\nEpoch 3: avg free-energy loss = 25.8849\nEpoch 4: avg free-energy loss = 21.0765\nEpoch 5: avg free-energy loss = 18.4651\nTest Accuracy: 86.65%\nMacro F1 Score: 0.8655\nEpoch 1: avg free-energy loss = 200.1413\nEpoch 2: avg free-energy loss = 55.9235\nEpoch 3: avg free-energy loss = 35.9069\nEpoch 4: avg free-energy loss = 29.4727\nEpoch 5: avg free-energy loss = 24.1111\nTest Accuracy: 86.25%\nMacro F1 Score: 0.8619\nEpoch 1: avg free-energy loss = 126.6828\nEpoch 2: avg free-energy loss = 36.2785\nEpoch 3: avg free-energy loss = 25.5911\nEpoch 4: avg free-energy loss = 20.6815\nEpoch 5: avg free-energy loss = 17.9402\nTest Accuracy: 86.43%\nMacro F1 Score: 0.8635\nEpoch 1: avg free-energy loss = 112.7345\nEpoch 2: avg free-energy loss = 33.4477\nEpoch 3: avg free-energy loss = 23.7942\nEpoch 4: avg free-energy loss = 19.1399\nEpoch 5: avg free-energy loss = 16.3007\nTest Accuracy: 86.74%\nMacro F1 Score: 0.8671\nEpoch 1: avg free-energy loss = 105.6741\nEpoch 2: avg free-energy loss = 31.2973\nEpoch 3: avg free-energy loss = 22.2810\nEpoch 4: avg free-energy loss = 17.9960\nEpoch 5: avg free-energy loss = 15.4486\nTest Accuracy: 86.67%\nMacro F1 Score: 0.8660\nEpoch 1: avg free-energy loss = 88.4828\nEpoch 2: avg free-energy loss = 24.1312\nEpoch 3: avg free-energy loss = 16.8558\nEpoch 4: avg free-energy loss = 13.4285\nEpoch 5: avg free-energy loss = 11.2438\nTest Accuracy: 86.05%\nMacro F1 Score: 0.8595\nEpoch 1: avg free-energy loss = 164.9203\nEpoch 2: avg free-energy loss = 47.3598\nEpoch 3: avg free-energy loss = 30.9522\nEpoch 4: avg free-energy loss = 24.6439\nEpoch 5: avg free-energy loss = 20.8669\nTest Accuracy: 86.51%\nMacro F1 Score: 0.8649\nEpoch 1: avg free-energy loss = 115.2898\nEpoch 2: avg free-energy loss = 36.4158\nEpoch 3: avg free-energy loss = 26.5679\nEpoch 4: avg free-energy loss = 20.0425\nEpoch 5: avg free-energy loss = 18.5737\nTest Accuracy: 86.55%\nMacro F1 Score: 0.8645\nEpoch 1: avg free-energy loss = 131.9440\nEpoch 2: avg free-energy loss = 34.4924\nEpoch 3: avg free-energy loss = 25.0261\nEpoch 4: avg free-energy loss = 20.8463\nEpoch 5: avg free-energy loss = 18.3044\nTest Accuracy: 86.39%\nMacro F1 Score: 0.8630\nEpoch 1: avg free-energy loss = 52.8054\nEpoch 2: avg free-energy loss = 14.8391\nEpoch 3: avg free-energy loss = 11.0946\nEpoch 4: avg free-energy loss = 9.2083\nEpoch 5: avg free-energy loss = 8.0666\nTest Accuracy: 86.34%\nMacro F1 Score: 0.8625\nEpoch 1: avg free-energy loss = 129.3992\nEpoch 2: avg free-energy loss = 36.5567\nEpoch 3: avg free-energy loss = 24.5337\nEpoch 4: avg free-energy loss = 19.3778\nEpoch 5: avg free-energy loss = 16.1646\nTest Accuracy: 85.32%\nMacro F1 Score: 0.8520\nEpoch 1: avg free-energy loss = 249.2433\nEpoch 2: avg free-energy loss = 77.2602\nEpoch 3: avg free-energy loss = 51.5478\nEpoch 4: avg free-energy loss = 40.2061\nEpoch 5: avg free-energy loss = 31.2089\nTest Accuracy: 86.31%\nMacro F1 Score: 0.8619\nEpoch 1: avg free-energy loss = 151.3303\nEpoch 2: avg free-energy loss = 44.6183\nEpoch 3: avg free-energy loss = 30.0300\nEpoch 4: avg free-energy loss = 23.6704\nEpoch 5: avg free-energy loss = 19.9832\nTest Accuracy: 86.68%\nMacro F1 Score: 0.8657\nEpoch 1: avg free-energy loss = 110.8060\nEpoch 2: avg free-energy loss = 32.9394\nEpoch 3: avg free-energy loss = 24.3679\nEpoch 4: avg free-energy loss = 20.0207\nEpoch 5: avg free-energy loss = 17.7401\nTest Accuracy: 86.55%\nMacro F1 Score: 0.8654\nEpoch 1: avg free-energy loss = 151.8053\nEpoch 2: avg free-energy loss = 42.8811\nEpoch 3: avg free-energy loss = 29.1478\nEpoch 4: avg free-energy loss = 23.6812\nEpoch 5: avg free-energy loss = 20.2689\nTest Accuracy: 86.23%\nMacro F1 Score: 0.8631\nEpoch 1: avg free-energy loss = 71.3464\nEpoch 2: avg free-energy loss = 24.3363\nEpoch 3: avg free-energy loss = 19.0738\nEpoch 4: avg free-energy loss = 16.3010\nEpoch 5: avg free-energy loss = 14.7047\nTest Accuracy: 86.37%\nMacro F1 Score: 0.8625\nEpoch 1: avg free-energy loss = 58.6644\nEpoch 2: avg free-energy loss = 17.0111\nEpoch 3: avg free-energy loss = 12.2987\nEpoch 4: avg free-energy loss = 10.0023\nEpoch 5: avg free-energy loss = 8.6010\nTest Accuracy: 86.76%\nMacro F1 Score: 0.8666\nEpoch 1: avg free-energy loss = 34.5321\nEpoch 2: avg free-energy loss = 8.6056\nEpoch 3: avg free-energy loss = 5.7289\nEpoch 4: avg free-energy loss = 4.6159\nEpoch 5: avg free-energy loss = 3.8419\nTest Accuracy: 86.56%\nMacro F1 Score: 0.8646\nEpoch 1: avg free-energy loss = 59.3033\nEpoch 2: avg free-energy loss = 17.5087\nEpoch 3: avg free-energy loss = 12.4595\nEpoch 4: avg free-energy loss = 10.1173\nEpoch 5: avg free-energy loss = 8.6744\nTest Accuracy: 86.79%\nMacro F1 Score: 0.8666\nEpoch 1: avg free-energy loss = 90.6978\nEpoch 2: avg free-energy loss = 24.3815\nEpoch 3: avg free-energy loss = 15.3271\nEpoch 4: avg free-energy loss = 11.7499\nEpoch 5: avg free-energy loss = 9.5954\nTest Accuracy: 86.43%\nMacro F1 Score: 0.8634\nEpoch 1: avg free-energy loss = 126.1584\nEpoch 2: avg free-energy loss = 36.4904\nEpoch 3: avg free-energy loss = 23.2756\nEpoch 4: avg free-energy loss = 17.2837\nEpoch 5: avg free-energy loss = 14.0218\nTest Accuracy: 86.38%\nMacro F1 Score: 0.8626\n{'num_rbm_epochs': 5, 'batch_size': 219, 'rbm_lr': 0.07193606666763605, 'rbm_hidden': 3517, 'num_classifier_epochs': 5, 'lr_C': 2.8856789323424294}\n86.82\nFrozenTrial(number=16, state=1, values=[86.82], datetime_start=datetime.datetime(2025, 3, 28, 15, 58, 3, 875747), datetime_complete=datetime.datetime(2025, 3, 28, 15, 58, 56, 977521), params={'num_rbm_epochs': 5, 'batch_size': 219, 'rbm_lr': 0.07193606666763605, 'rbm_hidden': 3517, 'num_classifier_epochs': 5, 'lr_C': 2.8856789323424294}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'num_rbm_epochs': IntDistribution(high=5, log=False, low=5, step=1), 'batch_size': IntDistribution(high=1024, log=False, low=192, step=1), 'rbm_lr': FloatDistribution(high=0.1, log=False, low=0.05, step=None), 'rbm_hidden': IntDistribution(high=8192, log=False, low=384, step=1), 'num_classifier_epochs': IntDistribution(high=5, log=False, low=5, step=1), 'lr_C': FloatDistribution(high=10.0, log=True, low=0.01, step=None)}, trial_id=16, value=None)\n\n[I 2025-03-28 15:43:28,289] A new study created in memory with name: no-name-d2c94975-041e-47d3-8353-8fdc1f881cf7\n[I 2025-03-28 15:44:36,278] Trial 0 finished with value: 86.7 and parameters: {'num_rbm_epochs': 5, 'batch_size': 475, 'rbm_lr': 0.08471941123761287, 'rbm_hidden': 5360, 'num_classifier_epochs': 5, 'lr_C': 7.759560632006714}. Best is trial 0 with value: 86.7.\n[I 2025-03-28 15:45:37,062] Trial 1 finished with value: 85.46000000000001 and parameters: {'num_rbm_epochs': 5, 'batch_size': 581, 'rbm_lr': 0.08580593423214694, 'rbm_hidden': 4840, 'num_classifier_epochs': 5, 'lr_C': 0.010436675317281851}. Best is trial 0 with value: 86.7.\n[I 2025-03-28 15:46:17,070] Trial 2 finished with value: 85.89 and parameters: {'num_rbm_epochs': 5, 'batch_size': 480, 'rbm_lr': 0.07869425006404021, 'rbm_hidden': 2736, 'num_classifier_epochs': 5, 'lr_C': 0.023336014276379}. Best is trial 0 with value: 86.7.\n[I 2025-03-28 15:47:35,007] Trial 3 finished with value: 86.49 and parameters: {'num_rbm_epochs': 5, 'batch_size': 270, 'rbm_lr': 0.07800623930294098, 'rbm_hidden': 5940, 'num_classifier_epochs': 5, 'lr_C': 6.868972291798769}. Best is trial 0 with value: 86.7.\n[I 2025-03-28 15:48:02,771] Trial 4 finished with value: 85.8 and parameters: {'num_rbm_epochs': 5, 'batch_size': 323, 'rbm_lr': 0.09497212684905881, 'rbm_hidden': 1227, 'num_classifier_epochs': 5, 'lr_C': 2.053826787090791}. Best is trial 0 with value: 86.7.\n[I 2025-03-28 15:49:24,147] Trial 5 finished with value: 86.32 and parameters: {'num_rbm_epochs': 5, 'batch_size': 499, 'rbm_lr': 0.059483581764094705, 'rbm_hidden': 6633, 'num_classifier_epochs': 5, 'lr_C': 0.8327307282767233}. Best is trial 0 with value: 86.7.\n[I 2025-03-28 15:49:48,728] Trial 6 finished with value: 85.28999999999999 and parameters: {'num_rbm_epochs': 5, 'batch_size': 830, 'rbm_lr': 0.0947445517768846, 'rbm_hidden': 1174, 'num_classifier_epochs': 5, 'lr_C': 0.08910458198589148}. Best is trial 0 with value: 86.7.\n[I 2025-03-28 15:50:11,644] Trial 7 finished with value: 85.17 and parameters: {'num_rbm_epochs': 5, 'batch_size': 573, 'rbm_lr': 0.0622047877721967, 'rbm_hidden': 866, 'num_classifier_epochs': 5, 'lr_C': 3.8974699887946844}. Best is trial 0 with value: 86.7.\n[I 2025-03-28 15:50:41,435] Trial 8 finished with value: 86.1 and parameters: {'num_rbm_epochs': 5, 'batch_size': 396, 'rbm_lr': 0.05280664101970556, 'rbm_hidden': 1527, 'num_classifier_epochs': 5, 'lr_C': 2.0938617043310113}. Best is trial 0 with value: 86.7.\n[I 2025-03-28 15:51:21,041] Trial 9 finished with value: 86.50999999999999 and parameters: {'num_rbm_epochs': 5, 'batch_size': 486, 'rbm_lr': 0.06369736391427752, 'rbm_hidden': 2425, 'num_classifier_epochs': 5, 'lr_C': 0.9812304011844255}. Best is trial 0 with value: 86.7.\n[I 2025-03-28 15:52:56,029] Trial 10 finished with value: 86.50999999999999 and parameters: {'num_rbm_epochs': 5, 'batch_size': 797, 'rbm_lr': 0.0862451943680599, 'rbm_hidden': 8148, 'num_classifier_epochs': 5, 'lr_C': 0.19658554788603524}. Best is trial 0 with value: 86.7.\n[I 2025-03-28 15:53:40,293] Trial 11 finished with value: 86.19 and parameters: {'num_rbm_epochs': 5, 'batch_size': 718, 'rbm_lr': 0.06807577425840475, 'rbm_hidden': 3160, 'num_classifier_epochs': 5, 'lr_C': 0.6822972143991152}. Best is trial 0 with value: 86.7.\n[I 2025-03-28 15:54:37,900] Trial 12 finished with value: 86.75 and parameters: {'num_rbm_epochs': 5, 'batch_size': 197, 'rbm_lr': 0.0672593918743455, 'rbm_hidden': 3891, 'num_classifier_epochs': 5, 'lr_C': 8.784761364782293}. Best is trial 12 with value: 86.75.\n[I 2025-03-28 15:55:39,902] Trial 13 finished with value: 86.67 and parameters: {'num_rbm_epochs': 5, 'batch_size': 192, 'rbm_lr': 0.07151373619461751, 'rbm_hidden': 4303, 'num_classifier_epochs': 5, 'lr_C': 8.37571699195763}. Best is trial 12 with value: 86.75.\n[I 2025-03-28 15:56:52,177] Trial 14 finished with value: 86.37 and parameters: {'num_rbm_epochs': 5, 'batch_size': 983, 'rbm_lr': 0.087013841826266, 'rbm_hidden': 5905, 'num_classifier_epochs': 5, 'lr_C': 9.802078153234888}. Best is trial 12 with value: 86.75.\n[I 2025-03-28 15:58:03,875] Trial 15 finished with value: 86.78 and parameters: {'num_rbm_epochs': 5, 'batch_size': 212, 'rbm_lr': 0.07495019459233111, 'rbm_hidden': 5066, 'num_classifier_epochs': 5, 'lr_C': 3.493191411482752}. Best is trial 15 with value: 86.78.\n[I 2025-03-28 15:58:56,977] Trial 16 finished with value: 86.82 and parameters: {'num_rbm_epochs': 5, 'batch_size': 219, 'rbm_lr': 0.07193606666763605, 'rbm_hidden': 3517, 'num_classifier_epochs': 5, 'lr_C': 2.8856789323424294}. Best is trial 16 with value: 86.82.\n[I 2025-03-28 16:00:27,317] Trial 17 finished with value: 86.56 and parameters: {'num_rbm_epochs': 5, 'batch_size': 337, 'rbm_lr': 0.0751750310772006, 'rbm_hidden': 7173, 'num_classifier_epochs': 5, 'lr_C': 0.2554998104507218}. Best is trial 16 with value: 86.82.\n[I 2025-03-28 16:01:20,557] Trial 18 finished with value: 86.39 and parameters: {'num_rbm_epochs': 5, 'batch_size': 267, 'rbm_lr': 0.05113390269488412, 'rbm_hidden': 3637, 'num_classifier_epochs': 5, 'lr_C': 2.31371612518345}. Best is trial 16 with value: 86.82.\n[I 2025-03-28 16:02:23,985] Trial 19 finished with value: 86.46000000000001 and parameters: {'num_rbm_epochs': 5, 'batch_size': 362, 'rbm_lr': 0.07924887131431342, 'rbm_hidden': 4786, 'num_classifier_epochs': 5, 'lr_C': 0.5476362288752789}. Best is trial 16 with value: 86.82.\n[I 2025-03-28 16:02:59,789] Trial 20 finished with value: 86.52 and parameters: {'num_rbm_epochs': 5, 'batch_size': 694, 'rbm_lr': 0.072089240302831, 'rbm_hidden': 2353, 'num_classifier_epochs': 5, 'lr_C': 3.120325582059925}. Best is trial 16 with value: 86.82.\n[I 2025-03-28 16:03:56,094] Trial 21 finished with value: 86.69 and parameters: {'num_rbm_epochs': 5, 'batch_size': 226, 'rbm_lr': 0.06717784476562857, 'rbm_hidden': 3876, 'num_classifier_epochs': 5, 'lr_C': 3.5407651486029876}. Best is trial 16 with value: 86.82.\n[I 2025-03-28 16:04:58,762] Trial 22 finished with value: 86.56 and parameters: {'num_rbm_epochs': 5, 'batch_size': 198, 'rbm_lr': 0.058106399403143175, 'rbm_hidden': 4369, 'num_classifier_epochs': 5, 'lr_C': 1.7095805800511936}. Best is trial 16 with value: 86.82.\n[I 2025-03-28 16:05:52,143] Trial 23 finished with value: 86.46000000000001 and parameters: {'num_rbm_epochs': 5, 'batch_size': 405, 'rbm_lr': 0.06766068429026827, 'rbm_hidden': 3885, 'num_classifier_epochs': 5, 'lr_C': 4.477028191264163}. Best is trial 16 with value: 86.82.\n[I 2025-03-28 16:06:40,208] Trial 24 finished with value: 86.41 and parameters: {'num_rbm_epochs': 5, 'batch_size': 278, 'rbm_lr': 0.07253434768410301, 'rbm_hidden': 3158, 'num_classifier_epochs': 5, 'lr_C': 1.2511154873387287}. Best is trial 16 with value: 86.82.\n[I 2025-03-28 16:07:49,657] Trial 25 finished with value: 86.72 and parameters: {'num_rbm_epochs': 5, 'batch_size': 301, 'rbm_lr': 0.08143637749521292, 'rbm_hidden': 5153, 'num_classifier_epochs': 5, 'lr_C': 4.583263918634515}. Best is trial 16 with value: 86.82.\n[I 2025-03-28 16:08:24,552] Trial 26 finished with value: 86.65 and parameters: {'num_rbm_epochs': 5, 'batch_size': 421, 'rbm_lr': 0.07491001982477367, 'rbm_hidden': 2045, 'num_classifier_epochs': 5, 'lr_C': 5.198087737600761}. Best is trial 16 with value: 86.82.\n[I 2025-03-28 16:09:14,521] Trial 27 finished with value: 86.56 and parameters: {'num_rbm_epochs': 5, 'batch_size': 244, 'rbm_lr': 0.0661987651359746, 'rbm_hidden': 3261, 'num_classifier_epochs': 5, 'lr_C': 0.37398397167067693}. Best is trial 16 with value: 86.82.\n[I 2025-03-28 16:10:29,913] Trial 28 finished with value: 86.58 and parameters: {'num_rbm_epochs': 5, 'batch_size': 346, 'rbm_lr': 0.09081361850775344, 'rbm_hidden': 5786, 'num_classifier_epochs': 5, 'lr_C': 1.264485858104714}. Best is trial 16 with value: 86.82.\n[I 2025-03-28 16:11:43,825] Trial 29 finished with value: 86.65 and parameters: {'num_rbm_epochs': 5, 'batch_size': 195, 'rbm_lr': 0.08194330768864505, 'rbm_hidden': 5315, 'num_classifier_epochs': 5, 'lr_C': 0.11351196121705188}. Best is trial 16 with value: 86.82.\n[I 2025-03-28 16:13:06,107] Trial 30 finished with value: 86.25 and parameters: {'num_rbm_epochs': 5, 'batch_size': 419, 'rbm_lr': 0.07010029879962065, 'rbm_hidden': 6580, 'num_classifier_epochs': 5, 'lr_C': 6.435433746499978}. Best is trial 16 with value: 86.82.\n[I 2025-03-28 16:14:11,022] Trial 31 finished with value: 86.42999999999999 and parameters: {'num_rbm_epochs': 5, 'batch_size': 308, 'rbm_lr': 0.07572303417240833, 'rbm_hidden': 4930, 'num_classifier_epochs': 5, 'lr_C': 9.838619339807405}. Best is trial 16 with value: 86.82.\n[I 2025-03-28 16:15:13,341] Trial 32 finished with value: 86.74 and parameters: {'num_rbm_epochs': 5, 'batch_size': 275, 'rbm_lr': 0.08197454239219443, 'rbm_hidden': 4534, 'num_classifier_epochs': 5, 'lr_C': 3.0717474749785127}. Best is trial 16 with value: 86.82.\n[I 2025-03-28 16:16:16,544] Trial 33 finished with value: 86.67 and parameters: {'num_rbm_epochs': 5, 'batch_size': 247, 'rbm_lr': 0.08287714864162257, 'rbm_hidden': 4530, 'num_classifier_epochs': 5, 'lr_C': 3.0461369518294736}. Best is trial 16 with value: 86.82.\n[I 2025-03-28 16:17:13,254] Trial 34 finished with value: 86.05000000000001 and parameters: {'num_rbm_epochs': 5, 'batch_size': 241, 'rbm_lr': 0.09905318431029067, 'rbm_hidden': 3850, 'num_classifier_epochs': 5, 'lr_C': 0.03650522628066981}. Best is trial 16 with value: 86.82.\n[I 2025-03-28 16:18:24,206] Trial 35 finished with value: 86.50999999999999 and parameters: {'num_rbm_epochs': 5, 'batch_size': 363, 'rbm_lr': 0.07828664650479517, 'rbm_hidden': 5509, 'num_classifier_epochs': 5, 'lr_C': 1.4839657304578404}. Best is trial 16 with value: 86.82.\n[I 2025-03-28 16:19:12,345] Trial 36 finished with value: 86.55000000000001 and parameters: {'num_rbm_epochs': 5, 'batch_size': 566, 'rbm_lr': 0.0887882829697821, 'rbm_hidden': 3411, 'num_classifier_epochs': 5, 'lr_C': 6.0363163478042186}. Best is trial 16 with value: 86.82.\n[I 2025-03-28 16:20:34,442] Trial 37 finished with value: 86.39 and parameters: {'num_rbm_epochs': 5, 'batch_size': 294, 'rbm_lr': 0.06250176363247048, 'rbm_hidden': 6433, 'num_classifier_epochs': 5, 'lr_C': 2.597487073433132}. Best is trial 16 with value: 86.82.\n[I 2025-03-28 16:21:14,268] Trial 38 finished with value: 86.33999999999999 and parameters: {'num_rbm_epochs': 5, 'batch_size': 521, 'rbm_lr': 0.05865245219431822, 'rbm_hidden': 2634, 'num_classifier_epochs': 5, 'lr_C': 6.512258803354044}. Best is trial 16 with value: 86.82.\n[I 2025-03-28 16:22:10,136] Trial 39 finished with value: 85.32 and parameters: {'num_rbm_epochs': 5, 'batch_size': 459, 'rbm_lr': 0.07654689776619607, 'rbm_hidden': 4189, 'num_classifier_epochs': 5, 'lr_C': 0.0108980040443134}. Best is trial 16 with value: 86.82.\n[I 2025-03-28 16:23:09,972] Trial 40 finished with value: 86.31 and parameters: {'num_rbm_epochs': 5, 'batch_size': 974, 'rbm_lr': 0.08010571895281741, 'rbm_hidden': 4751, 'num_classifier_epochs': 5, 'lr_C': 0.5058272457049481}. Best is trial 16 with value: 86.82.\n[I 2025-03-28 16:24:20,722] Trial 41 finished with value: 86.68 and parameters: {'num_rbm_epochs': 5, 'batch_size': 296, 'rbm_lr': 0.08277002363450732, 'rbm_hidden': 5443, 'num_classifier_epochs': 5, 'lr_C': 4.642558835957204}. Best is trial 16 with value: 86.82.\n[I 2025-03-28 16:25:29,834] Trial 42 finished with value: 86.55000000000001 and parameters: {'num_rbm_epochs': 5, 'batch_size': 237, 'rbm_lr': 0.07424871127913, 'rbm_hidden': 5063, 'num_classifier_epochs': 5, 'lr_C': 3.843158063929651}. Best is trial 16 with value: 86.82.\n[I 2025-03-28 16:26:48,465] Trial 43 finished with value: 86.22999999999999 and parameters: {'num_rbm_epochs': 5, 'batch_size': 300, 'rbm_lr': 0.06956126534306944, 'rbm_hidden': 6206, 'num_classifier_epochs': 5, 'lr_C': 1.9318458526470277}. Best is trial 16 with value: 86.82.\n[I 2025-03-28 16:27:49,361] Trial 44 finished with value: 86.37 and parameters: {'num_rbm_epochs': 5, 'batch_size': 215, 'rbm_lr': 0.06488286400936794, 'rbm_hidden': 4162, 'num_classifier_epochs': 5, 'lr_C': 7.322715230877536}. Best is trial 16 with value: 86.82.\n[I 2025-03-28 16:28:35,358] Trial 45 finished with value: 86.76 and parameters: {'num_rbm_epochs': 5, 'batch_size': 270, 'rbm_lr': 0.08567443169731266, 'rbm_hidden': 2947, 'num_classifier_epochs': 5, 'lr_C': 1.0407917550368337}. Best is trial 16 with value: 86.82.\n[I 2025-03-28 16:29:10,580] Trial 46 finished with value: 86.56 and parameters: {'num_rbm_epochs': 5, 'batch_size': 373, 'rbm_lr': 0.08484078082719072, 'rbm_hidden': 2031, 'num_classifier_epochs': 5, 'lr_C': 1.0235821758061208}. Best is trial 16 with value: 86.82.\n[I 2025-03-28 16:29:56,739] Trial 47 finished with value: 86.79 and parameters: {'num_rbm_epochs': 5, 'batch_size': 272, 'rbm_lr': 0.08873858134221667, 'rbm_hidden': 2942, 'num_classifier_epochs': 5, 'lr_C': 0.8053881598695234}. Best is trial 16 with value: 86.82.\n[I 2025-03-28 16:30:38,118] Trial 48 finished with value: 86.42999999999999 and parameters: {'num_rbm_epochs': 5, 'batch_size': 642, 'rbm_lr': 0.09361630741085752, 'rbm_hidden': 2785, 'num_classifier_epochs': 5, 'lr_C': 0.6023133401443783}. Best is trial 16 with value: 86.82.\n[I 2025-03-28 16:31:20,831] Trial 49 finished with value: 86.38 and parameters: {'num_rbm_epochs': 5, 'batch_size': 895, 'rbm_lr': 0.09136768154625116, 'rbm_hidden': 2986, 'num_classifier_epochs': 5, 'lr_C': 0.8668116354798511}. Best is trial 16 with value: 86.82.\n\n\n\nTest Accuracy of Logistic Regression on RBM Hidden Features by Inverse Regularization Strength \nTest Accuracy By Number of RBM Hidden Units \nModel 5: Feed Forward Network on RBM Hidden Features (of Fashion MNIST Data)\n\n\nClick to Show Code and Output\n\n\n\nCode\nfrom sklearn.metrics import accuracy_score, f1_score\nCLASSIFIER = 'FNN'\n\nif CLASSIFIER == 'LogisticRegression':\n    experiment = mlflow.set_experiment(\"new-pytorch-fmnist-lr-withrbm\")\nelse:\n    experiment = mlflow.set_experiment(\"new-pytorch-fmnist-fnn-withrbm\")\n\n\nclass RBM(nn.Module):\n    def __init__(self, n_visible=784, n_hidden=256, k=1):\n        super(RBM, self).__init__()\n        self.n_visible = n_visible\n        self.n_hidden = n_hidden\n        # Initialize weights and biases\n        self.W = nn.Parameter(torch.randn(n_hidden, n_visible) * 0.1)\n        self.v_bias = nn.Parameter(torch.zeros(n_visible))\n        self.h_bias = nn.Parameter(torch.zeros(n_hidden))\n        self.k = k  # CD-k steps\n\n    def sample_h(self, v):\n        # Given visible v, sample hidden h\n        p_h = torch.sigmoid(F.linear(v, self.W, self.h_bias))  # p(h=1|v)\n        h_sample = torch.bernoulli(p_h)                        # sample Bernoulli\n        return p_h, h_sample\n\n    def sample_v(self, h):\n        # Given hidden h, sample visible v\n        p_v = torch.sigmoid(F.linear(h, self.W.t(), self.v_bias))  # p(v=1|h)\n        v_sample = torch.bernoulli(p_v)\n        return p_v, v_sample\n\n    def forward(self, v):\n        # Perform k steps of contrastive divergence starting from v\n        v_k = v.clone()\n        for _ in range(self.k):\n            _, h_k = self.sample_h(v_k)    # sample hidden from current visible\n            _, v_k = self.sample_v(h_k)    # sample visible from hidden\n        return v_k  # k-step reconstructed visible\n\n    def free_energy(self, v):\n        # Compute the visible bias term for each sample in the batch\n        vbias_term = (v * self.v_bias).sum(dim=1)  # shape: [batch_size]\n        # Compute the activation of the hidden units\n        wx_b = F.linear(v, self.W, self.h_bias)     # shape: [batch_size, n_hidden]\n        # Compute the hidden term\n        hidden_term = torch.sum(torch.log1p(torch.exp(wx_b)), dim=1)  # shape: [batch_size]\n        # Return the mean free energy over the batch\n        return - (vbias_term + hidden_term).mean()\n    \ntransform = transforms.Compose([transforms.ToTensor()])\ntrain_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\ntest_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n\ndef objective(trial):\n    num_rbm_epochs = trial.suggest_int(\"num_rbm_epochs\", 5, 5)# 24, 33)\n    batch_size = trial.suggest_int(\"batch_size\", 192, 1024)\n    rbm_lr = trial.suggest_float(\"rbm_lr\", 0.05, 0.1)\n    rbm_hidden = trial.suggest_int(\"rbm_hidden\", 384, 8192)\n\n    mlflow.start_run(experiment_id=experiment.experiment_id)\n    if CLASSIFIER != 'LogisticRegression':\n        fnn_hidden = trial.suggest_int(\"fnn_hidden\", 192, 384)\n        fnn_lr = trial.suggest_float(\"fnn_lr\", 0.0001, 0.0025)\n        mlflow.log_param(\"fnn_hidden\", fnn_hidden)\n        mlflow.log_param(\"fnn_lr\", fnn_lr)\n\n    num_classifier_epochs = trial.suggest_int(\"num_classifier_epochs\", 5, 5)# 40, 60)\n\n    mlflow.log_param(\"num_rbm_epochs\", num_rbm_epochs)\n    mlflow.log_param(\"batch_size\", batch_size)\n    mlflow.log_param(\"rbm_lr\", rbm_lr)\n    mlflow.log_param(\"rbm_hidden\", rbm_hidden)\n    mlflow.log_param(\"num_classifier_epochs\", num_classifier_epochs)\n\n    # Instantiate RBM and optimizer\n    device = torch.device(\"mps\")\n    rbm = RBM(n_visible=784, n_hidden=rbm_hidden, k=1).to(device)\n    optimizer = torch.optim.SGD(rbm.parameters(), lr=rbm_lr)\n\n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n    rbm_training_failed = False\n    # Training loop (assuming train_loader yields batches of images and labels)\n    for epoch in range(num_rbm_epochs):\n        total_loss = 0.0\n        for images, _ in train_loader:\n            # Flatten images and binarize\n            v0 = images.view(-1, 784).to(rbm.W.device)      # shape [batch_size, 784]\n            v0 = torch.bernoulli(v0)                        # sample binary input\n            vk = rbm(v0)                                    # k-step CD reconstruction\n            # Compute contrastive divergence loss (free energy difference)\n            loss = rbm.free_energy(v0) - rbm.free_energy(vk)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n        print(f\"Epoch {epoch+1}: avg free-energy loss = {total_loss/len(train_loader):.4f}\")\n        if np.isnan(total_loss):\n            rbm_training_failed = True\n            break\n\n        if rbm_training_failed:\n            accuracy = 0.0\n            macro_f1 = 0.0 \n            print(\"RBM training failed — returning 0.0 for accuracy and macro F1\")  # 🔥 for visibility\n            mlflow.log_metric(\"test_accuracy\", accuracy)\n            mlflow.log_metric(\"macro_f1\", macro_f1)\n            mlflow.set_tag(\"status\", \"rbm_failed\")  # Optional tag\n            mlflow.end_run()\n            return float(accuracy)\n    else:\n        rbm.eval()  # set in evaluation mode if using any layers that behave differently in training\n        features_list = []\n        labels_list = []\n        for images, labels in train_loader:\n            v = images.view(-1, 784).to(rbm.W.device)\n            v = v  # (optionally binarize or use raw normalized pixels)\n            h_prob, h_sample = rbm.sample_h(v)  # get hidden activations\n            features_list.append(h_prob.cpu().detach().numpy())\n            labels_list.append(labels.numpy())\n        train_features = np.concatenate(features_list)  # shape: [N_train, n_hidden]\n        train_labels = np.concatenate(labels_list)\n\n        # Convert pre-extracted training features and labels to tensors and create a DataLoader\n        train_features_tensor = torch.tensor(train_features, dtype=torch.float32)\n        train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n        train_feature_dataset = torch.utils.data.TensorDataset(train_features_tensor, train_labels_tensor)\n        train_feature_loader = torch.utils.data.DataLoader(train_feature_dataset, batch_size=batch_size, shuffle=True)\n\n            \n        if CLASSIFIER == 'LogisticRegression':\n            # add optuna tuning same as log reg without RBM features...\n            lr_C = trial.suggest_float(\"lr_C\", 0.01, 10.0, log=True)  \n            mlflow.log_param(\"lr_C\", lr_C)  # Log the chosen C value\n\n            classifier = LogisticRegression(max_iter=num_classifier_epochs, C=lr_C, solver=\"saga\") \n            classifier.fit(train_features, train_labels)            \n            \n        else:\n            classifier = nn.Sequential(\n                nn.Linear(rbm.n_hidden, fnn_hidden),\n                nn.ReLU(),\n                nn.Linear(fnn_hidden, 10)\n            )\n\n            # Move classifier to the same device as the RBM\n            classifier = classifier.to(device)\n            criterion = nn.CrossEntropyLoss()\n            classifier_optimizer = torch.optim.Adam(classifier.parameters(), lr=fnn_lr)\n\n            classifier.train()\n            for epoch in range(num_classifier_epochs):\n                running_loss = 0.0\n                for features, labels in train_feature_loader:\n                    features = features.to(device)\n                    labels = labels.to(device)\n                    \n                    # Forward pass through classifier\n                    outputs = classifier(features)\n                    loss = criterion(outputs, labels)\n                    \n                    # Backpropagation and optimization\n                    classifier_optimizer.zero_grad()\n                    loss.backward()\n                    classifier_optimizer.step()\n                    \n                    running_loss += loss.item()\n                avg_loss = running_loss / len(train_feature_loader)\n                print(f\"Classifier Epoch {epoch+1}: loss = {avg_loss:.4f}\")\n\n        # Evaluate the classifier on test data.\n        # Here we extract features from the RBM for each test image.\n        if CLASSIFIER != 'LogisticRegression':\n            classifier.eval()\n            correct = 0\n            total = 0\n        features_list = []\n        labels_list = []\n        with torch.no_grad():\n            for images, labels in test_loader:\n                v = images.view(-1, 784).to(device)\n                # Extract hidden activations; you can use either h_prob or h_sample.\n                h_prob, _ = rbm.sample_h(v)\n                if CLASSIFIER == 'LogisticRegression':\n                    features_list.append(h_prob.cpu().detach().numpy())\n                    labels_list.append(labels.numpy())\n                else:\n                    outputs = classifier(h_prob)\n                    _, predicted = torch.max(outputs.data, 1)\n                    total += labels.size(0)\n                    correct += (predicted.cpu() == labels).sum().item()\n\n        if CLASSIFIER == 'LogisticRegression':\n            test_features = np.concatenate(features_list)\n            test_labels = np.concatenate(labels_list)\n            predictions = classifier.predict(test_features)\n            accuracy = accuracy_score(test_labels, predictions) * 100\n        \n            macro_f1 = f1_score(test_labels, predictions, average=\"macro\") \n        \n        else:\n            accuracy = 100 * correct / total\n        \n            all_preds = [] \n            all_labels = [] \n            classifier.eval()\n            with torch.no_grad():\n                for images, labels in test_loader:\n                    v = images.view(-1, 784).to(device)\n                    h_prob, _ = rbm.sample_h(v)\n                    outputs = classifier(h_prob)\n                    _, predicted = torch.max(outputs.data, 1)\n                    all_preds.extend(predicted.cpu().numpy()) \n                    all_labels.extend(labels.numpy()) \n        \n            macro_f1 = f1_score(all_labels, all_preds, average=\"macro\") \n        \n        print(f\"Test Accuracy: {accuracy:.2f}%\")\n        print(f\"Macro F1 Score: {macro_f1:.4f}\") \n        \n        mlflow.log_metric(\"test_accuracy\", accuracy)\n        mlflow.log_metric(\"macro_f1\", macro_f1) \n        mlflow.end_run()\n        return float(accuracy if accuracy is not None else 0.0)\n\nif __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=50)\n    print(study.best_params)\n    print(study.best_value)\n    print(study.best_trial)\n\n\nEpoch 1: avg free-energy loss = -7.9494\nEpoch 2: avg free-energy loss = -16.3580\nEpoch 3: avg free-energy loss = -14.2304\nEpoch 4: avg free-energy loss = -12.9966\nEpoch 5: avg free-energy loss = -12.2340\nClassifier Epoch 1: loss = 0.7704\nClassifier Epoch 2: loss = 0.5172\nClassifier Epoch 3: loss = 0.4715\nClassifier Epoch 4: loss = 0.4467\nClassifier Epoch 5: loss = 0.4292\nTest Accuracy: 83.44%\nMacro F1 Score: 0.8276\nEpoch 1: avg free-energy loss = -20.6368\nEpoch 2: avg free-energy loss = -20.9018\nEpoch 3: avg free-energy loss = -18.6343\nEpoch 4: avg free-energy loss = -17.2850\nEpoch 5: avg free-energy loss = -16.3783\nClassifier Epoch 1: loss = 0.8672\nClassifier Epoch 2: loss = 0.5673\nClassifier Epoch 3: loss = 0.5158\nClassifier Epoch 4: loss = 0.4849\nClassifier Epoch 5: loss = 0.4681\nTest Accuracy: 81.93%\nMacro F1 Score: 0.8173\nEpoch 1: avg free-energy loss = 296.3109\nEpoch 2: avg free-energy loss = 91.2839\nEpoch 3: avg free-energy loss = 62.4585\nEpoch 4: avg free-energy loss = 46.8922\nEpoch 5: avg free-energy loss = 39.4544\nClassifier Epoch 1: loss = 0.6366\nClassifier Epoch 2: loss = 0.4564\nClassifier Epoch 3: loss = 0.4169\nClassifier Epoch 4: loss = 0.3942\nClassifier Epoch 5: loss = 0.3887\nTest Accuracy: 85.08%\nMacro F1 Score: 0.8486\nEpoch 1: avg free-energy loss = 54.0006\nEpoch 2: avg free-energy loss = 23.1856\nEpoch 3: avg free-energy loss = 18.4842\nEpoch 4: avg free-energy loss = 17.9494\nEpoch 5: avg free-energy loss = 15.1389\nClassifier Epoch 1: loss = 0.5456\nClassifier Epoch 2: loss = 0.4388\nClassifier Epoch 3: loss = 0.3897\nClassifier Epoch 4: loss = 0.3624\nClassifier Epoch 5: loss = 0.3472\nTest Accuracy: 84.31%\nMacro F1 Score: 0.8381\nEpoch 1: avg free-energy loss = 226.4909\nEpoch 2: avg free-energy loss = 67.9030\nEpoch 3: avg free-energy loss = 46.5347\nEpoch 4: avg free-energy loss = 36.2489\nEpoch 5: avg free-energy loss = 31.9510\nClassifier Epoch 1: loss = 0.6282\nClassifier Epoch 2: loss = 0.4524\nClassifier Epoch 3: loss = 0.4191\nClassifier Epoch 4: loss = 0.4028\nClassifier Epoch 5: loss = 0.3858\nTest Accuracy: 84.74%\nMacro F1 Score: 0.8392\nEpoch 1: avg free-energy loss = 164.7099\nEpoch 2: avg free-energy loss = 55.8821\nEpoch 3: avg free-energy loss = 38.4960\nEpoch 4: avg free-energy loss = 28.1885\nEpoch 5: avg free-energy loss = 29.2875\nClassifier Epoch 1: loss = 0.5411\nClassifier Epoch 2: loss = 0.4347\nClassifier Epoch 3: loss = 0.4121\nClassifier Epoch 4: loss = 0.3994\nClassifier Epoch 5: loss = 0.3632\nTest Accuracy: 83.91%\nMacro F1 Score: 0.8380\nEpoch 1: avg free-energy loss = 53.6455\nEpoch 2: avg free-energy loss = 11.5056\nEpoch 3: avg free-energy loss = 7.1479\nEpoch 4: avg free-energy loss = 4.5464\nEpoch 5: avg free-energy loss = 3.2979\nClassifier Epoch 1: loss = 0.8040\nClassifier Epoch 2: loss = 0.5026\nClassifier Epoch 3: loss = 0.4601\nClassifier Epoch 4: loss = 0.4359\nClassifier Epoch 5: loss = 0.4193\nTest Accuracy: 83.97%\nMacro F1 Score: 0.8386\nEpoch 1: avg free-energy loss = 3.3027\nEpoch 2: avg free-energy loss = -0.9166\nEpoch 3: avg free-energy loss = -0.8084\nEpoch 4: avg free-energy loss = -0.7366\nEpoch 5: avg free-energy loss = -0.6085\nClassifier Epoch 1: loss = 0.5380\nClassifier Epoch 2: loss = 0.4178\nClassifier Epoch 3: loss = 0.3840\nClassifier Epoch 4: loss = 0.3659\nClassifier Epoch 5: loss = 0.3454\nTest Accuracy: 86.44%\nMacro F1 Score: 0.8637\nEpoch 1: avg free-energy loss = 196.7385\nEpoch 2: avg free-energy loss = 59.4501\nEpoch 3: avg free-energy loss = 39.2503\nEpoch 4: avg free-energy loss = 30.3215\nEpoch 5: avg free-energy loss = 24.9154\nClassifier Epoch 1: loss = 0.5843\nClassifier Epoch 2: loss = 0.4252\nClassifier Epoch 3: loss = 0.4012\nClassifier Epoch 4: loss = 0.3675\nClassifier Epoch 5: loss = 0.3511\nTest Accuracy: 85.95%\nMacro F1 Score: 0.8605\nEpoch 1: avg free-energy loss = 41.2625\nEpoch 2: avg free-energy loss = 11.5472\nEpoch 3: avg free-energy loss = 8.1285\nEpoch 4: avg free-energy loss = 6.4034\nEpoch 5: avg free-energy loss = 5.3511\nClassifier Epoch 1: loss = 0.5370\nClassifier Epoch 2: loss = 0.3989\nClassifier Epoch 3: loss = 0.3668\nClassifier Epoch 4: loss = 0.3484\nClassifier Epoch 5: loss = 0.3259\nTest Accuracy: 86.43%\nMacro F1 Score: 0.8634\nEpoch 1: avg free-energy loss = 75.1814\nEpoch 2: avg free-energy loss = 20.2457\nEpoch 3: avg free-energy loss = 14.8070\nEpoch 4: avg free-energy loss = 12.2811\nEpoch 5: avg free-energy loss = 10.7887\nClassifier Epoch 1: loss = 0.5874\nClassifier Epoch 2: loss = 0.4288\nClassifier Epoch 3: loss = 0.3944\nClassifier Epoch 4: loss = 0.3689\nClassifier Epoch 5: loss = 0.3533\nTest Accuracy: 86.48%\nMacro F1 Score: 0.8623\nEpoch 1: avg free-energy loss = 84.7989\nEpoch 2: avg free-energy loss = 22.2254\nEpoch 3: avg free-energy loss = 15.9761\nEpoch 4: avg free-energy loss = 13.1407\nEpoch 5: avg free-energy loss = 11.4755\nClassifier Epoch 1: loss = 0.5873\nClassifier Epoch 2: loss = 0.4339\nClassifier Epoch 3: loss = 0.3899\nClassifier Epoch 4: loss = 0.3677\nClassifier Epoch 5: loss = 0.3529\nTest Accuracy: 86.77%\nMacro F1 Score: 0.8651\nEpoch 1: avg free-energy loss = 92.5530\nEpoch 2: avg free-energy loss = 25.0804\nEpoch 3: avg free-energy loss = 18.2475\nEpoch 4: avg free-energy loss = 14.6714\nEpoch 5: avg free-energy loss = 12.6846\nClassifier Epoch 1: loss = 0.6079\nClassifier Epoch 2: loss = 0.4299\nClassifier Epoch 3: loss = 0.4033\nClassifier Epoch 4: loss = 0.3714\nClassifier Epoch 5: loss = 0.3539\nTest Accuracy: 86.08%\nMacro F1 Score: 0.8603\nEpoch 1: avg free-energy loss = 65.8722\nEpoch 2: avg free-energy loss = 18.0436\nEpoch 3: avg free-energy loss = 13.3529\nEpoch 4: avg free-energy loss = 10.9941\nEpoch 5: avg free-energy loss = 9.6961\nClassifier Epoch 1: loss = 0.6006\nClassifier Epoch 2: loss = 0.4269\nClassifier Epoch 3: loss = 0.3847\nClassifier Epoch 4: loss = 0.3657\nClassifier Epoch 5: loss = 0.3514\nTest Accuracy: 85.65%\nMacro F1 Score: 0.8531\nEpoch 1: avg free-energy loss = 176.3672\nEpoch 2: avg free-energy loss = 48.0087\nEpoch 3: avg free-energy loss = 31.4469\nEpoch 4: avg free-energy loss = 24.4371\nEpoch 5: avg free-energy loss = 20.4467\nClassifier Epoch 1: loss = 0.6424\nClassifier Epoch 2: loss = 0.4450\nClassifier Epoch 3: loss = 0.4096\nClassifier Epoch 4: loss = 0.3909\nClassifier Epoch 5: loss = 0.3690\nTest Accuracy: 85.61%\nMacro F1 Score: 0.8556\nEpoch 1: avg free-energy loss = 68.3595\nEpoch 2: avg free-energy loss = 18.6610\nEpoch 3: avg free-energy loss = 13.3781\nEpoch 4: avg free-energy loss = 11.2155\nEpoch 5: avg free-energy loss = 9.7690\nClassifier Epoch 1: loss = 0.6141\nClassifier Epoch 2: loss = 0.4468\nClassifier Epoch 3: loss = 0.4112\nClassifier Epoch 4: loss = 0.3930\nClassifier Epoch 5: loss = 0.3782\nTest Accuracy: 85.45%\nMacro F1 Score: 0.8489\nEpoch 1: avg free-energy loss = 103.7954\nEpoch 2: avg free-energy loss = 28.6273\nEpoch 3: avg free-energy loss = 20.5303\nEpoch 4: avg free-energy loss = 16.4765\nEpoch 5: avg free-energy loss = 14.3530\nClassifier Epoch 1: loss = 0.5685\nClassifier Epoch 2: loss = 0.4234\nClassifier Epoch 3: loss = 0.3888\nClassifier Epoch 4: loss = 0.3640\nClassifier Epoch 5: loss = 0.3481\nTest Accuracy: 85.80%\nMacro F1 Score: 0.8587\nEpoch 1: avg free-energy loss = 96.9961\nEpoch 2: avg free-energy loss = 25.8510\nEpoch 3: avg free-energy loss = 18.9783\nEpoch 4: avg free-energy loss = 16.2600\nEpoch 5: avg free-energy loss = 14.3283\nClassifier Epoch 1: loss = 0.6185\nClassifier Epoch 2: loss = 0.4455\nClassifier Epoch 3: loss = 0.4120\nClassifier Epoch 4: loss = 0.3797\nClassifier Epoch 5: loss = 0.3625\nTest Accuracy: 84.43%\nMacro F1 Score: 0.8430\nEpoch 1: avg free-energy loss = 59.5948\nEpoch 2: avg free-energy loss = 14.7077\nEpoch 3: avg free-energy loss = 10.9849\nEpoch 4: avg free-energy loss = 9.0216\nEpoch 5: avg free-energy loss = 7.8285\nClassifier Epoch 1: loss = 0.6529\nClassifier Epoch 2: loss = 0.4630\nClassifier Epoch 3: loss = 0.4274\nClassifier Epoch 4: loss = 0.4015\nClassifier Epoch 5: loss = 0.3811\nTest Accuracy: 85.17%\nMacro F1 Score: 0.8474\nEpoch 1: avg free-energy loss = 143.3618\nEpoch 2: avg free-energy loss = 39.3630\nEpoch 3: avg free-energy loss = 25.9254\nEpoch 4: avg free-energy loss = 20.3991\nEpoch 5: avg free-energy loss = 16.9682\nClassifier Epoch 1: loss = 0.6253\nClassifier Epoch 2: loss = 0.4441\nClassifier Epoch 3: loss = 0.4031\nClassifier Epoch 4: loss = 0.3860\nClassifier Epoch 5: loss = 0.3628\nTest Accuracy: 85.11%\nMacro F1 Score: 0.8536\nEpoch 1: avg free-energy loss = 123.2998\nEpoch 2: avg free-energy loss = 28.8367\nEpoch 3: avg free-energy loss = 21.0347\nEpoch 4: avg free-energy loss = 17.3812\nEpoch 5: avg free-energy loss = 15.1545\nClassifier Epoch 1: loss = 0.6562\nClassifier Epoch 2: loss = 0.4371\nClassifier Epoch 3: loss = 0.4092\nClassifier Epoch 4: loss = 0.3815\nClassifier Epoch 5: loss = 0.3629\nTest Accuracy: 85.62%\nMacro F1 Score: 0.8553\nEpoch 1: avg free-energy loss = 5.5166\nEpoch 2: avg free-energy loss = -3.1210\nEpoch 3: avg free-energy loss = -2.8285\nEpoch 4: avg free-energy loss = -2.7155\nEpoch 5: avg free-energy loss = -2.5241\nClassifier Epoch 1: loss = 0.6257\nClassifier Epoch 2: loss = 0.4527\nClassifier Epoch 3: loss = 0.4197\nClassifier Epoch 4: loss = 0.3943\nClassifier Epoch 5: loss = 0.3813\nTest Accuracy: 84.57%\nMacro F1 Score: 0.8453\nEpoch 1: avg free-energy loss = 13.2511\nEpoch 2: avg free-energy loss = 2.9139\nEpoch 3: avg free-energy loss = 2.1357\nEpoch 4: avg free-energy loss = 1.7060\nEpoch 5: avg free-energy loss = 1.4679\nClassifier Epoch 1: loss = 0.5659\nClassifier Epoch 2: loss = 0.4327\nClassifier Epoch 3: loss = 0.3950\nClassifier Epoch 4: loss = 0.3760\nClassifier Epoch 5: loss = 0.3569\nTest Accuracy: 86.06%\nMacro F1 Score: 0.8607\nEpoch 1: avg free-energy loss = 46.4948\nEpoch 2: avg free-energy loss = 16.7855\nEpoch 3: avg free-energy loss = 13.4336\nEpoch 4: avg free-energy loss = 11.5313\nEpoch 5: avg free-energy loss = 10.2795\nClassifier Epoch 1: loss = 0.5208\nClassifier Epoch 2: loss = 0.3985\nClassifier Epoch 3: loss = 0.3659\nClassifier Epoch 4: loss = 0.3468\nClassifier Epoch 5: loss = 0.3288\nTest Accuracy: 84.70%\nMacro F1 Score: 0.8453\nEpoch 1: avg free-energy loss = 42.8889\nEpoch 2: avg free-energy loss = 11.5159\nEpoch 3: avg free-energy loss = 8.3389\nEpoch 4: avg free-energy loss = 6.9427\nEpoch 5: avg free-energy loss = 5.9641\nClassifier Epoch 1: loss = 0.5889\nClassifier Epoch 2: loss = 0.4289\nClassifier Epoch 3: loss = 0.3906\nClassifier Epoch 4: loss = 0.3648\nClassifier Epoch 5: loss = 0.3476\nTest Accuracy: 86.34%\nMacro F1 Score: 0.8627\nEpoch 1: avg free-energy loss = 11.6034\nEpoch 2: avg free-energy loss = -2.5283\nEpoch 3: avg free-energy loss = -2.4783\nEpoch 4: avg free-energy loss = -2.5822\nEpoch 5: avg free-energy loss = -2.6208\nClassifier Epoch 1: loss = 0.6669\nClassifier Epoch 2: loss = 0.4696\nClassifier Epoch 3: loss = 0.4285\nClassifier Epoch 4: loss = 0.4026\nClassifier Epoch 5: loss = 0.3877\nTest Accuracy: 84.96%\nMacro F1 Score: 0.8497\nEpoch 1: avg free-energy loss = 88.5327\nEpoch 2: avg free-energy loss = 24.7575\nEpoch 3: avg free-energy loss = 17.1411\nEpoch 4: avg free-energy loss = 13.7513\nEpoch 5: avg free-energy loss = 11.7448\nClassifier Epoch 1: loss = 0.5639\nClassifier Epoch 2: loss = 0.4209\nClassifier Epoch 3: loss = 0.3793\nClassifier Epoch 4: loss = 0.3687\nClassifier Epoch 5: loss = 0.3475\nTest Accuracy: 86.94%\nMacro F1 Score: 0.8699\nEpoch 1: avg free-energy loss = 137.5824\nEpoch 2: avg free-energy loss = 40.1476\nEpoch 3: avg free-energy loss = 27.0336\nEpoch 4: avg free-energy loss = 20.9861\nEpoch 5: avg free-energy loss = 17.9831\nClassifier Epoch 1: loss = 0.5684\nClassifier Epoch 2: loss = 0.4169\nClassifier Epoch 3: loss = 0.3803\nClassifier Epoch 4: loss = 0.3669\nClassifier Epoch 5: loss = 0.3476\nTest Accuracy: 85.63%\nMacro F1 Score: 0.8532\nEpoch 1: avg free-energy loss = 105.2765\nEpoch 2: avg free-energy loss = 28.2761\nEpoch 3: avg free-energy loss = 19.5001\nEpoch 4: avg free-energy loss = 15.3584\nEpoch 5: avg free-energy loss = 13.1081\nClassifier Epoch 1: loss = 0.5846\nClassifier Epoch 2: loss = 0.4368\nClassifier Epoch 3: loss = 0.4030\nClassifier Epoch 4: loss = 0.3809\nClassifier Epoch 5: loss = 0.3601\nTest Accuracy: 86.25%\nMacro F1 Score: 0.8620\nEpoch 1: avg free-energy loss = 122.1296\nEpoch 2: avg free-energy loss = 33.2799\nEpoch 3: avg free-energy loss = 21.3056\nEpoch 4: avg free-energy loss = 16.6767\nEpoch 5: avg free-energy loss = 13.8188\nClassifier Epoch 1: loss = 0.6627\nClassifier Epoch 2: loss = 0.4575\nClassifier Epoch 3: loss = 0.4158\nClassifier Epoch 4: loss = 0.3925\nClassifier Epoch 5: loss = 0.3698\nTest Accuracy: 84.95%\nMacro F1 Score: 0.8465\nEpoch 1: avg free-energy loss = 170.8301\nEpoch 2: avg free-energy loss = 50.4010\nEpoch 3: avg free-energy loss = 34.4344\nEpoch 4: avg free-energy loss = 27.1837\nEpoch 5: avg free-energy loss = 22.9653\nClassifier Epoch 1: loss = 0.5939\nClassifier Epoch 2: loss = 0.4194\nClassifier Epoch 3: loss = 0.3862\nClassifier Epoch 4: loss = 0.3617\nClassifier Epoch 5: loss = 0.3449\nTest Accuracy: 86.08%\nMacro F1 Score: 0.8597\nEpoch 1: avg free-energy loss = -14.0955\nEpoch 2: avg free-energy loss = -14.3351\nEpoch 3: avg free-energy loss = -12.4782\nEpoch 4: avg free-energy loss = -11.4570\nEpoch 5: avg free-energy loss = -10.5970\nClassifier Epoch 1: loss = 0.6255\nClassifier Epoch 2: loss = 0.4611\nClassifier Epoch 3: loss = 0.4261\nClassifier Epoch 4: loss = 0.4080\nClassifier Epoch 5: loss = 0.3885\nTest Accuracy: 84.26%\nMacro F1 Score: 0.8362\nEpoch 1: avg free-energy loss = 44.0412\nEpoch 2: avg free-energy loss = 7.4174\nEpoch 3: avg free-energy loss = 5.9695\nEpoch 4: avg free-energy loss = 4.5568\nEpoch 5: avg free-energy loss = 3.8936\nClassifier Epoch 1: loss = 0.6950\nClassifier Epoch 2: loss = 0.4809\nClassifier Epoch 3: loss = 0.4323\nClassifier Epoch 4: loss = 0.4102\nClassifier Epoch 5: loss = 0.3911\nTest Accuracy: 85.42%\nMacro F1 Score: 0.8530\nEpoch 1: avg free-energy loss = 36.3599\nEpoch 2: avg free-energy loss = 13.5479\nEpoch 3: avg free-energy loss = 10.6614\nEpoch 4: avg free-energy loss = 9.0335\nEpoch 5: avg free-energy loss = 8.1207\nClassifier Epoch 1: loss = 0.5690\nClassifier Epoch 2: loss = 0.4272\nClassifier Epoch 3: loss = 0.3881\nClassifier Epoch 4: loss = 0.3619\nClassifier Epoch 5: loss = 0.3392\nTest Accuracy: 86.10%\nMacro F1 Score: 0.8599\nEpoch 1: avg free-energy loss = 149.3008\nEpoch 2: avg free-energy loss = 41.9853\nEpoch 3: avg free-energy loss = 27.6949\nEpoch 4: avg free-energy loss = 21.0732\nEpoch 5: avg free-energy loss = 17.2345\nClassifier Epoch 1: loss = 0.5915\nClassifier Epoch 2: loss = 0.4293\nClassifier Epoch 3: loss = 0.3952\nClassifier Epoch 4: loss = 0.3715\nClassifier Epoch 5: loss = 0.3585\nTest Accuracy: 85.87%\nMacro F1 Score: 0.8576\nEpoch 1: avg free-energy loss = -4.5546\nEpoch 2: avg free-energy loss = -7.6506\nEpoch 3: avg free-energy loss = -6.7102\nEpoch 4: avg free-energy loss = -6.0152\nEpoch 5: avg free-energy loss = -5.5372\nClassifier Epoch 1: loss = 0.5983\nClassifier Epoch 2: loss = 0.4411\nClassifier Epoch 3: loss = 0.4069\nClassifier Epoch 4: loss = 0.3867\nClassifier Epoch 5: loss = 0.3708\nTest Accuracy: 85.06%\nMacro F1 Score: 0.8522\nEpoch 1: avg free-energy loss = 184.0355\nEpoch 2: avg free-energy loss = 35.2832\nEpoch 3: avg free-energy loss = 23.2024\nEpoch 4: avg free-energy loss = 18.6540\nEpoch 5: avg free-energy loss = 15.7964\nClassifier Epoch 1: loss = 0.6869\nClassifier Epoch 2: loss = 0.4629\nClassifier Epoch 3: loss = 0.4154\nClassifier Epoch 4: loss = 0.3942\nClassifier Epoch 5: loss = 0.3809\nTest Accuracy: 85.19%\nMacro F1 Score: 0.8506\nEpoch 1: avg free-energy loss = 87.4798\nEpoch 2: avg free-energy loss = 29.4220\nEpoch 3: avg free-energy loss = 22.3986\nEpoch 4: avg free-energy loss = 18.7514\nEpoch 5: avg free-energy loss = 16.6470\nClassifier Epoch 1: loss = 0.5979\nClassifier Epoch 2: loss = 0.4341\nClassifier Epoch 3: loss = 0.4029\nClassifier Epoch 4: loss = 0.3807\nClassifier Epoch 5: loss = 0.3651\nTest Accuracy: 85.73%\nMacro F1 Score: 0.8572\nEpoch 1: avg free-energy loss = 19.5072\nEpoch 2: avg free-energy loss = 7.3500\nEpoch 3: avg free-energy loss = 5.8339\nEpoch 4: avg free-energy loss = 5.0947\nEpoch 5: avg free-energy loss = 4.5508\nClassifier Epoch 1: loss = 0.5519\nClassifier Epoch 2: loss = 0.4207\nClassifier Epoch 3: loss = 0.3848\nClassifier Epoch 4: loss = 0.3666\nClassifier Epoch 5: loss = 0.3462\nTest Accuracy: 86.52%\nMacro F1 Score: 0.8638\nEpoch 1: avg free-energy loss = 23.2072\nEpoch 2: avg free-energy loss = 8.1844\nEpoch 3: avg free-energy loss = 6.4948\nEpoch 4: avg free-energy loss = 5.6222\nEpoch 5: avg free-energy loss = 5.0531\nClassifier Epoch 1: loss = 0.5542\nClassifier Epoch 2: loss = 0.4212\nClassifier Epoch 3: loss = 0.3911\nClassifier Epoch 4: loss = 0.3580\nClassifier Epoch 5: loss = 0.3487\nTest Accuracy: 86.07%\nMacro F1 Score: 0.8559\nEpoch 1: avg free-energy loss = 80.4854\nEpoch 2: avg free-energy loss = 22.8208\nEpoch 3: avg free-energy loss = 16.2181\nEpoch 4: avg free-energy loss = 13.2542\nEpoch 5: avg free-energy loss = 11.2404\nClassifier Epoch 1: loss = 0.5416\nClassifier Epoch 2: loss = 0.4123\nClassifier Epoch 3: loss = 0.3754\nClassifier Epoch 4: loss = 0.3519\nClassifier Epoch 5: loss = 0.3364\nTest Accuracy: 86.43%\nMacro F1 Score: 0.8627\nEpoch 1: avg free-energy loss = -15.3423\nEpoch 2: avg free-energy loss = -12.9328\nEpoch 3: avg free-energy loss = -11.2064\nEpoch 4: avg free-energy loss = -9.7787\nEpoch 5: avg free-energy loss = -8.4751\nClassifier Epoch 1: loss = 0.5575\nClassifier Epoch 2: loss = 0.4289\nClassifier Epoch 3: loss = 0.3926\nClassifier Epoch 4: loss = 0.3761\nClassifier Epoch 5: loss = 0.3573\nTest Accuracy: 86.07%\nMacro F1 Score: 0.8582\nEpoch 1: avg free-energy loss = 38.7520\nEpoch 2: avg free-energy loss = 13.7721\nEpoch 3: avg free-energy loss = 10.8032\nEpoch 4: avg free-energy loss = 9.3792\nEpoch 5: avg free-energy loss = 8.3153\nClassifier Epoch 1: loss = 0.5497\nClassifier Epoch 2: loss = 0.4127\nClassifier Epoch 3: loss = 0.3809\nClassifier Epoch 4: loss = 0.3618\nClassifier Epoch 5: loss = 0.3448\nTest Accuracy: 85.11%\nMacro F1 Score: 0.8481\nEpoch 1: avg free-energy loss = 25.0917\nEpoch 2: avg free-energy loss = 5.5691\nEpoch 3: avg free-energy loss = 4.1193\nEpoch 4: avg free-energy loss = 3.3557\nEpoch 5: avg free-energy loss = 2.8216\nClassifier Epoch 1: loss = 0.6045\nClassifier Epoch 2: loss = 0.4459\nClassifier Epoch 3: loss = 0.4070\nClassifier Epoch 4: loss = 0.3874\nClassifier Epoch 5: loss = 0.3706\nTest Accuracy: 84.98%\nMacro F1 Score: 0.8463\nEpoch 1: avg free-energy loss = 11.7029\nEpoch 2: avg free-energy loss = 0.9135\nEpoch 3: avg free-energy loss = 0.1311\nEpoch 4: avg free-energy loss = -0.0756\nEpoch 5: avg free-energy loss = 0.0058\nClassifier Epoch 1: loss = 1.0872\nClassifier Epoch 2: loss = 0.5859\nClassifier Epoch 3: loss = 0.5112\nClassifier Epoch 4: loss = 0.4759\nClassifier Epoch 5: loss = 0.4545\nTest Accuracy: 82.76%\nMacro F1 Score: 0.8259\nEpoch 1: avg free-energy loss = 75.8773\nEpoch 2: avg free-energy loss = 21.8856\nEpoch 3: avg free-energy loss = 16.5895\nEpoch 4: avg free-energy loss = 13.9699\nEpoch 5: avg free-energy loss = 12.3491\nClassifier Epoch 1: loss = 0.5630\nClassifier Epoch 2: loss = 0.4183\nClassifier Epoch 3: loss = 0.3716\nClassifier Epoch 4: loss = 0.3596\nClassifier Epoch 5: loss = 0.3411\nTest Accuracy: 84.12%\nMacro F1 Score: 0.8365\nEpoch 1: avg free-energy loss = 54.9174\nEpoch 2: avg free-energy loss = 14.4931\nEpoch 3: avg free-energy loss = 10.1470\nEpoch 4: avg free-energy loss = 8.2150\nEpoch 5: avg free-energy loss = 7.0697\nClassifier Epoch 1: loss = 0.6096\nClassifier Epoch 2: loss = 0.4435\nClassifier Epoch 3: loss = 0.4143\nClassifier Epoch 4: loss = 0.3862\nClassifier Epoch 5: loss = 0.3648\nTest Accuracy: 85.29%\nMacro F1 Score: 0.8505\nEpoch 1: avg free-energy loss = 1.8340\nEpoch 2: avg free-energy loss = -5.3326\nEpoch 3: avg free-energy loss = -5.3469\nEpoch 4: avg free-energy loss = -5.1137\nEpoch 5: avg free-energy loss = -4.7659\nClassifier Epoch 1: loss = 0.6124\nClassifier Epoch 2: loss = 0.4476\nClassifier Epoch 3: loss = 0.4119\nClassifier Epoch 4: loss = 0.3928\nClassifier Epoch 5: loss = 0.3711\nTest Accuracy: 85.16%\nMacro F1 Score: 0.8503\nEpoch 1: avg free-energy loss = 20.3825\nEpoch 2: avg free-energy loss = 8.6612\nEpoch 3: avg free-energy loss = 7.0033\nEpoch 4: avg free-energy loss = 5.9337\nEpoch 5: avg free-energy loss = 5.4066\nClassifier Epoch 1: loss = 0.5380\nClassifier Epoch 2: loss = 0.4163\nClassifier Epoch 3: loss = 0.3821\nClassifier Epoch 4: loss = 0.3588\nClassifier Epoch 5: loss = 0.3448\nTest Accuracy: 86.85%\nMacro F1 Score: 0.8680\nEpoch 1: avg free-energy loss = 31.5311\nEpoch 2: avg free-energy loss = 7.5844\nEpoch 3: avg free-energy loss = 6.4449\nEpoch 4: avg free-energy loss = 5.4340\nEpoch 5: avg free-energy loss = 5.0087\nClassifier Epoch 1: loss = 0.6789\nClassifier Epoch 2: loss = 0.4783\nClassifier Epoch 3: loss = 0.4428\nClassifier Epoch 4: loss = 0.4150\nClassifier Epoch 5: loss = 0.3989\nTest Accuracy: 84.94%\nMacro F1 Score: 0.8476\n{'num_rbm_epochs': 5, 'batch_size': 420, 'rbm_lr': 0.0803892046509745, 'rbm_hidden': 3381, 'fnn_hidden': 256, 'fnn_lr': 0.0021046741128304017, 'num_classifier_epochs': 5}\n86.94\nFrozenTrial(number=26, state=1, values=[86.94], datetime_start=datetime.datetime(2025, 3, 28, 16, 44, 14, 213482), datetime_complete=datetime.datetime(2025, 3, 28, 16, 44, 42, 775743), params={'num_rbm_epochs': 5, 'batch_size': 420, 'rbm_lr': 0.0803892046509745, 'rbm_hidden': 3381, 'fnn_hidden': 256, 'fnn_lr': 0.0021046741128304017, 'num_classifier_epochs': 5}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'num_rbm_epochs': IntDistribution(high=5, log=False, low=5, step=1), 'batch_size': IntDistribution(high=1024, log=False, low=192, step=1), 'rbm_lr': FloatDistribution(high=0.1, log=False, low=0.05, step=None), 'rbm_hidden': IntDistribution(high=8192, log=False, low=384, step=1), 'fnn_hidden': IntDistribution(high=384, log=False, low=192, step=1), 'fnn_lr': FloatDistribution(high=0.0025, log=False, low=0.0001, step=None), 'num_classifier_epochs': IntDistribution(high=5, log=False, low=5, step=1)}, trial_id=26, value=None)\n\n[I 2025-03-28 16:31:21,441] A new study created in memory with name: no-name-7e800a9f-7085-4533-8667-8eca12c97736\n[I 2025-03-28 16:31:39,948] Trial 0 finished with value: 83.44 and parameters: {'num_rbm_epochs': 5, 'batch_size': 978, 'rbm_lr': 0.07685976863360014, 'rbm_hidden': 780, 'fnn_hidden': 237, 'fnn_lr': 0.0019042488508073641, 'num_classifier_epochs': 5}. Best is trial 0 with value: 83.44.\n[I 2025-03-28 16:31:58,494] Trial 1 finished with value: 81.93 and parameters: {'num_rbm_epochs': 5, 'batch_size': 841, 'rbm_lr': 0.056149697557266114, 'rbm_hidden': 462, 'fnn_hidden': 287, 'fnn_lr': 0.0010669637567547751, 'num_classifier_epochs': 5}. Best is trial 0 with value: 83.44.\n[I 2025-03-28 16:32:34,604] Trial 2 finished with value: 85.08 and parameters: {'num_rbm_epochs': 5, 'batch_size': 658, 'rbm_lr': 0.08392305096214814, 'rbm_hidden': 6552, 'fnn_hidden': 267, 'fnn_lr': 0.0007830687958329613, 'num_classifier_epochs': 5}. Best is trial 2 with value: 85.08.\n[I 2025-03-28 16:33:09,570] Trial 3 finished with value: 84.31 and parameters: {'num_rbm_epochs': 5, 'batch_size': 262, 'rbm_lr': 0.05171459844512646, 'rbm_hidden': 3715, 'fnn_hidden': 335, 'fnn_lr': 0.0019490479896028376, 'num_classifier_epochs': 5}. Best is trial 2 with value: 85.08.\n[I 2025-03-28 16:33:57,464] Trial 4 finished with value: 84.74 and parameters: {'num_rbm_epochs': 5, 'batch_size': 303, 'rbm_lr': 0.07736123992711064, 'rbm_hidden': 8054, 'fnn_hidden': 245, 'fnn_lr': 0.00031652715400081805, 'num_classifier_epochs': 5}. Best is trial 2 with value: 85.08.\n[I 2025-03-28 16:34:44,742] Trial 5 finished with value: 83.91 and parameters: {'num_rbm_epochs': 5, 'batch_size': 229, 'rbm_lr': 0.08512539443059712, 'rbm_hidden': 6476, 'fnn_hidden': 339, 'fnn_lr': 0.0010042356903125973, 'num_classifier_epochs': 5}. Best is trial 2 with value: 85.08.\n[I 2025-03-28 16:35:05,006] Trial 6 finished with value: 83.97 and parameters: {'num_rbm_epochs': 5, 'batch_size': 993, 'rbm_lr': 0.09023022044286942, 'rbm_hidden': 1853, 'fnn_hidden': 379, 'fnn_lr': 0.0006805296083867429, 'num_classifier_epochs': 5}. Best is trial 2 with value: 85.08.\n[I 2025-03-28 16:35:31,047] Trial 7 finished with value: 86.44 and parameters: {'num_rbm_epochs': 5, 'batch_size': 228, 'rbm_lr': 0.06038847577916692, 'rbm_hidden': 1324, 'fnn_hidden': 245, 'fnn_lr': 0.001829540306247736, 'num_classifier_epochs': 5}. Best is trial 7 with value: 86.44.\n[I 2025-03-28 16:36:04,574] Trial 8 finished with value: 85.95 and parameters: {'num_rbm_epochs': 5, 'batch_size': 484, 'rbm_lr': 0.08565557594087289, 'rbm_hidden': 5310, 'fnn_hidden': 366, 'fnn_lr': 0.0012960669625454216, 'num_classifier_epochs': 5}. Best is trial 7 with value: 86.44.\n[I 2025-03-28 16:36:33,760] Trial 9 finished with value: 86.43 and parameters: {'num_rbm_epochs': 5, 'batch_size': 244, 'rbm_lr': 0.09539238017567672, 'rbm_hidden': 2437, 'fnn_hidden': 316, 'fnn_lr': 0.001126868049423012, 'num_classifier_epochs': 5}. Best is trial 7 with value: 86.44.\n[I 2025-03-28 16:37:01,682] Trial 10 finished with value: 86.48 and parameters: {'num_rbm_epochs': 5, 'batch_size': 443, 'rbm_lr': 0.06479703588450385, 'rbm_hidden': 3276, 'fnn_hidden': 202, 'fnn_lr': 0.002262416032800853, 'num_classifier_epochs': 5}. Best is trial 10 with value: 86.48.\n[I 2025-03-28 16:37:30,052] Trial 11 finished with value: 86.77 and parameters: {'num_rbm_epochs': 5, 'batch_size': 449, 'rbm_lr': 0.0635280712474465, 'rbm_hidden': 3457, 'fnn_hidden': 198, 'fnn_lr': 0.00240717987522241, 'num_classifier_epochs': 5}. Best is trial 11 with value: 86.77.\n[I 2025-03-28 16:37:59,554] Trial 12 finished with value: 86.08 and parameters: {'num_rbm_epochs': 5, 'batch_size': 474, 'rbm_lr': 0.06514844007725767, 'rbm_hidden': 3681, 'fnn_hidden': 201, 'fnn_lr': 0.0024647160931277772, 'num_classifier_epochs': 5}. Best is trial 11 with value: 86.77.\n[I 2025-03-28 16:38:26,607] Trial 13 finished with value: 85.65 and parameters: {'num_rbm_epochs': 5, 'batch_size': 450, 'rbm_lr': 0.06776334272944196, 'rbm_hidden': 2971, 'fnn_hidden': 192, 'fnn_lr': 0.0024807254692986802, 'num_classifier_epochs': 5}. Best is trial 11 with value: 86.77.\n[I 2025-03-28 16:38:57,013] Trial 14 finished with value: 85.61 and parameters: {'num_rbm_epochs': 5, 'batch_size': 693, 'rbm_lr': 0.06821903319132003, 'rbm_hidden': 4834, 'fnn_hidden': 218, 'fnn_lr': 0.0021211953434633808, 'num_classifier_epochs': 5}. Best is trial 11 with value: 86.77.\n[I 2025-03-28 16:39:23,693] Trial 15 finished with value: 85.45 and parameters: {'num_rbm_epochs': 5, 'batch_size': 559, 'rbm_lr': 0.061374934806417356, 'rbm_hidden': 2961, 'fnn_hidden': 217, 'fnn_lr': 0.0016697531425210197, 'num_classifier_epochs': 5}. Best is trial 11 with value: 86.77.\n[I 2025-03-28 16:39:56,029] Trial 16 finished with value: 85.8 and parameters: {'num_rbm_epochs': 5, 'batch_size': 370, 'rbm_lr': 0.07183561912636137, 'rbm_hidden': 4051, 'fnn_hidden': 271, 'fnn_lr': 0.0015420604669476449, 'num_classifier_epochs': 5}. Best is trial 11 with value: 86.77.\n[I 2025-03-28 16:40:31,161] Trial 17 finished with value: 84.43 and parameters: {'num_rbm_epochs': 5, 'batch_size': 382, 'rbm_lr': 0.05193384554456661, 'rbm_hidden': 5232, 'fnn_hidden': 218, 'fnn_lr': 0.0021957625495489836, 'num_classifier_epochs': 5}. Best is trial 11 with value: 86.77.\n[I 2025-03-28 16:40:55,493] Trial 18 finished with value: 85.17 and parameters: {'num_rbm_epochs': 5, 'batch_size': 732, 'rbm_lr': 0.0601345265371627, 'rbm_hidden': 2468, 'fnn_hidden': 195, 'fnn_lr': 0.002215938520984163, 'num_classifier_epochs': 5}. Best is trial 11 with value: 86.77.\n[I 2025-03-28 16:41:25,629] Trial 19 finished with value: 85.11 and parameters: {'num_rbm_epochs': 5, 'batch_size': 579, 'rbm_lr': 0.07300285659006364, 'rbm_hidden': 4236, 'fnn_hidden': 261, 'fnn_lr': 0.0015739945469558448, 'num_classifier_epochs': 5}. Best is trial 11 with value: 86.77.\n[I 2025-03-28 16:42:05,863] Trial 20 finished with value: 85.62 and parameters: {'num_rbm_epochs': 5, 'batch_size': 366, 'rbm_lr': 0.05657302078966238, 'rbm_hidden': 6190, 'fnn_hidden': 302, 'fnn_lr': 0.0023177000521958156, 'num_classifier_epochs': 5}. Best is trial 11 with value: 86.77.\n[I 2025-03-28 16:42:27,541] Trial 21 finished with value: 84.57 and parameters: {'num_rbm_epochs': 5, 'batch_size': 526, 'rbm_lr': 0.06314711024848019, 'rbm_hidden': 1319, 'fnn_hidden': 240, 'fnn_lr': 0.0018888821833494105, 'num_classifier_epochs': 5}. Best is trial 11 with value: 86.77.\n[I 2025-03-28 16:42:52,038] Trial 22 finished with value: 86.06 and parameters: {'num_rbm_epochs': 5, 'batch_size': 333, 'rbm_lr': 0.05779878497371583, 'rbm_hidden': 1639, 'fnn_hidden': 227, 'fnn_lr': 0.002006485444042037, 'num_classifier_epochs': 5}. Best is trial 11 with value: 86.77.\n[I 2025-03-28 16:43:28,293] Trial 23 finished with value: 84.7 and parameters: {'num_rbm_epochs': 5, 'batch_size': 195, 'rbm_lr': 0.06758320673233065, 'rbm_hidden': 3102, 'fnn_hidden': 204, 'fnn_lr': 0.0016767186968677668, 'num_classifier_epochs': 5}. Best is trial 11 with value: 86.77.\n[I 2025-03-28 16:43:52,729] Trial 24 finished with value: 86.34 and parameters: {'num_rbm_epochs': 5, 'batch_size': 443, 'rbm_lr': 0.06359326976695494, 'rbm_hidden': 2332, 'fnn_hidden': 244, 'fnn_lr': 0.002366610247557912, 'num_classifier_epochs': 5}. Best is trial 11 with value: 86.77.\n[I 2025-03-28 16:44:14,213] Trial 25 finished with value: 84.96 and parameters: {'num_rbm_epochs': 5, 'batch_size': 626, 'rbm_lr': 0.07109375377447083, 'rbm_hidden': 1350, 'fnn_hidden': 211, 'fnn_lr': 0.00178124286827835, 'num_classifier_epochs': 5}. Best is trial 11 with value: 86.77.\n[I 2025-03-28 16:44:42,775] Trial 26 finished with value: 86.94 and parameters: {'num_rbm_epochs': 5, 'batch_size': 420, 'rbm_lr': 0.0803892046509745, 'rbm_hidden': 3381, 'fnn_hidden': 256, 'fnn_lr': 0.0021046741128304017, 'num_classifier_epochs': 5}. Best is trial 26 with value: 86.94.\n[I 2025-03-28 16:45:16,326] Trial 27 finished with value: 85.63 and parameters: {'num_rbm_epochs': 5, 'batch_size': 415, 'rbm_lr': 0.0760341429394765, 'rbm_hidden': 4646, 'fnn_hidden': 277, 'fnn_lr': 0.002127170193911178, 'num_classifier_epochs': 5}. Best is trial 26 with value: 86.94.\n[I 2025-03-28 16:45:44,672] Trial 28 finished with value: 86.25 and parameters: {'num_rbm_epochs': 5, 'batch_size': 513, 'rbm_lr': 0.07819930820339267, 'rbm_hidden': 3528, 'fnn_hidden': 257, 'fnn_lr': 0.0013928438939582132, 'num_classifier_epochs': 5}. Best is trial 26 with value: 86.94.\n[I 2025-03-28 16:46:10,733] Trial 29 finished with value: 84.95 and parameters: {'num_rbm_epochs': 5, 'batch_size': 755, 'rbm_lr': 0.07942647189415598, 'rbm_hidden': 3292, 'fnn_hidden': 226, 'fnn_lr': 0.002043828790787277, 'num_classifier_epochs': 5}. Best is trial 26 with value: 86.94.\n[I 2025-03-28 16:46:51,821] Trial 30 finished with value: 86.08 and parameters: {'num_rbm_epochs': 5, 'batch_size': 321, 'rbm_lr': 0.08186871219139809, 'rbm_hidden': 5893, 'fnn_hidden': 228, 'fnn_lr': 0.002290381822237252, 'num_classifier_epochs': 5}. Best is trial 26 with value: 86.94.\n[I 2025-03-28 16:47:12,952] Trial 31 finished with value: 84.26 and parameters: {'num_rbm_epochs': 5, 'batch_size': 419, 'rbm_lr': 0.05934462152347368, 'rbm_hidden': 731, 'fnn_hidden': 257, 'fnn_lr': 0.001840087350007031, 'num_classifier_epochs': 5}. Best is trial 26 with value: 86.94.\n[I 2025-03-28 16:47:34,786] Trial 32 finished with value: 85.42 and parameters: {'num_rbm_epochs': 5, 'batch_size': 908, 'rbm_lr': 0.06571480741451359, 'rbm_hidden': 1878, 'fnn_hidden': 284, 'fnn_lr': 0.0022925355152995383, 'num_classifier_epochs': 5}. Best is trial 26 with value: 86.94.\n[I 2025-03-28 16:48:04,924] Trial 33 finished with value: 86.1 and parameters: {'num_rbm_epochs': 5, 'batch_size': 287, 'rbm_lr': 0.05340130938663253, 'rbm_hidden': 2701, 'fnn_hidden': 301, 'fnn_lr': 0.0024647338173498577, 'num_classifier_epochs': 5}. Best is trial 26 with value: 86.94.\n[I 2025-03-28 16:48:33,413] Trial 34 finished with value: 85.87 and parameters: {'num_rbm_epochs': 5, 'batch_size': 597, 'rbm_lr': 0.08950918746667691, 'rbm_hidden': 3917, 'fnn_hidden': 206, 'fnn_lr': 0.0020608827395373684, 'num_classifier_epochs': 5}. Best is trial 26 with value: 86.94.\n[I 2025-03-28 16:48:55,594] Trial 35 finished with value: 85.06 and parameters: {'num_rbm_epochs': 5, 'batch_size': 393, 'rbm_lr': 0.07025938242651668, 'rbm_hidden': 1017, 'fnn_hidden': 234, 'fnn_lr': 0.001912352872565176, 'num_classifier_epochs': 5}. Best is trial 26 with value: 86.94.\n[I 2025-03-28 16:49:36,383] Trial 36 finished with value: 85.19 and parameters: {'num_rbm_epochs': 5, 'batch_size': 538, 'rbm_lr': 0.055149846639407245, 'rbm_hidden': 7204, 'fnn_hidden': 248, 'fnn_lr': 0.001756580048101243, 'num_classifier_epochs': 5}. Best is trial 26 with value: 86.94.\n[I 2025-03-28 16:50:17,003] Trial 37 finished with value: 85.73 and parameters: {'num_rbm_epochs': 5, 'batch_size': 197, 'rbm_lr': 0.07478335307044473, 'rbm_hidden': 4574, 'fnn_hidden': 250, 'fnn_lr': 0.0003289509829743759, 'num_classifier_epochs': 5}. Best is trial 26 with value: 86.94.\n[I 2025-03-28 16:50:44,811] Trial 38 finished with value: 86.52 and parameters: {'num_rbm_epochs': 5, 'batch_size': 271, 'rbm_lr': 0.05051354616221276, 'rbm_hidden': 2035, 'fnn_hidden': 235, 'fnn_lr': 0.0022094590698111274, 'num_classifier_epochs': 5}. Best is trial 26 with value: 86.94.\n[I 2025-03-28 16:51:12,045] Trial 39 finished with value: 86.07 and parameters: {'num_rbm_epochs': 5, 'batch_size': 289, 'rbm_lr': 0.05092809628763094, 'rbm_hidden': 2125, 'fnn_hidden': 235, 'fnn_lr': 0.002387358167458787, 'num_classifier_epochs': 5}. Best is trial 26 with value: 86.94.\n[I 2025-03-28 16:51:42,199] Trial 40 finished with value: 86.43 and parameters: {'num_rbm_epochs': 5, 'batch_size': 339, 'rbm_lr': 0.08173771866243773, 'rbm_hidden': 3358, 'fnn_hidden': 213, 'fnn_lr': 0.002198761976074191, 'num_classifier_epochs': 5}. Best is trial 26 with value: 86.94.\n[I 2025-03-28 16:52:06,075] Trial 41 finished with value: 86.07 and parameters: {'num_rbm_epochs': 5, 'batch_size': 244, 'rbm_lr': 0.061155297945488796, 'rbm_hidden': 652, 'fnn_hidden': 270, 'fnn_lr': 0.001938790689717326, 'num_classifier_epochs': 5}. Best is trial 26 with value: 86.94.\n[I 2025-03-28 16:52:36,261] Trial 42 finished with value: 85.11 and parameters: {'num_rbm_epochs': 5, 'batch_size': 272, 'rbm_lr': 0.054546146445554416, 'rbm_hidden': 2846, 'fnn_hidden': 202, 'fnn_lr': 0.0021845015895273624, 'num_classifier_epochs': 5}. Best is trial 26 with value: 86.94.\n[I 2025-03-28 16:53:00,157] Trial 43 finished with value: 84.98 and parameters: {'num_rbm_epochs': 5, 'batch_size': 479, 'rbm_lr': 0.05722994837175465, 'rbm_hidden': 1867, 'fnn_hidden': 230, 'fnn_lr': 0.0020059505572936343, 'num_classifier_epochs': 5}. Best is trial 26 with value: 86.94.\n[I 2025-03-28 16:53:27,020] Trial 44 finished with value: 82.76 and parameters: {'num_rbm_epochs': 5, 'batch_size': 241, 'rbm_lr': 0.09772530064668687, 'rbm_hidden': 1459, 'fnn_hidden': 195, 'fnn_lr': 0.00010373162465527382, 'num_classifier_epochs': 5}. Best is trial 26 with value: 86.94.\n[I 2025-03-28 16:53:59,057] Trial 45 finished with value: 84.12 and parameters: {'num_rbm_epochs': 5, 'batch_size': 319, 'rbm_lr': 0.06365930229465193, 'rbm_hidden': 3710, 'fnn_hidden': 222, 'fnn_lr': 0.002394146804012174, 'num_classifier_epochs': 5}. Best is trial 26 with value: 86.94.\n[I 2025-03-28 16:54:26,014] Trial 46 finished with value: 85.29 and parameters: {'num_rbm_epochs': 5, 'batch_size': 454, 'rbm_lr': 0.0739325096649262, 'rbm_hidden': 2477, 'fnn_hidden': 341, 'fnn_lr': 0.0008273701764834851, 'num_classifier_epochs': 5}. Best is trial 26 with value: 86.94.\n[I 2025-03-28 16:54:47,959] Trial 47 finished with value: 85.16 and parameters: {'num_rbm_epochs': 5, 'batch_size': 498, 'rbm_lr': 0.08739263811387749, 'rbm_hidden': 1111, 'fnn_hidden': 239, 'fnn_lr': 0.0021139215380483072, 'num_classifier_epochs': 5}. Best is trial 26 with value: 86.94.\n[I 2025-03-28 16:55:18,493] Trial 48 finished with value: 86.85 and parameters: {'num_rbm_epochs': 5, 'batch_size': 218, 'rbm_lr': 0.05023961955170966, 'rbm_hidden': 2133, 'fnn_hidden': 253, 'fnn_lr': 0.0013908896355394004, 'num_classifier_epochs': 5}. Best is trial 26 with value: 86.94.\n[I 2025-03-28 16:55:41,367] Trial 49 finished with value: 84.94 and parameters: {'num_rbm_epochs': 5, 'batch_size': 637, 'rbm_lr': 0.05202843720829133, 'rbm_hidden': 2113, 'fnn_hidden': 262, 'fnn_lr': 0.0010786691647612355, 'num_classifier_epochs': 5}. Best is trial 26 with value: 86.94.\n\n\n\nTest Accuracy by RBM Hidden Units \nTest Accuracy by FNN Hidden Units \n\n\n\n\n\nModel\n\n\nOptuna Best TrialMLflow Test Accuracy(%)\n\n\nMacro F1 Score\n\n\n\n\n\n\nLogistic Regression\n\n\n84.71\n\n\n0.846\n\n\n\n\nFeed Forward Network\n\n\n88.06\n\n\n0.879\n\n\n\n\nConvolutional Neural Network\n\n\n91.29\n\n\n0.913\n\n\n\n\nLogistic Regression (on RBM Hidden Features) \n\n\n87.14\n\n\n0.871\n\n\n\n\nFeed Forward Network (on RBM Hidden Features)\n\n\n86.95\n\n\n0.869\n\n\n\n\n\n\nConclusion\n\nSummarize your key findings.\n\nCNN clearly outperforms other models. Logistic Regression, which typically performs well for binary classifications tasks, underperforms on Fashion MNIST multiclassification task. Logistic Regression is improved by using a Restricted Boltzmann Machine first to extract the hidden features from the input data prior to classification. Feed Forward Network is not improved by the use of RBM. These findings clearly show the progress in machine and deep learning and how more advanced neural networks on raw pixels can outperform models that use RBM hidden features.\n\nDiscuss the implications of your results."
  },
  {
    "objectID": "testrender.html",
    "href": "testrender.html",
    "title": "test",
    "section": "",
    "text": "Code\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nimport numpy as np\nimport mlflow\nimport optuna\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom torch.utils.data import DataLoader\n\n# Set device\ndevice = torch.device(\"mps\")\n\n# Load Fashion-MNIST dataset again for the first 3 models\ntransform = transforms.Compose([transforms.ToTensor()])\ntrain_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\ntest_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n\n\n\n\nClick to Show Code and Output\n\n\n\nCode\nCLASSIFIER = \"LogisticRegression\"  # Change for FNN, LogisticRegression, or CNN\n\n# Set MLflow experiment name\nif CLASSIFIER == \"LogisticRegression\":\n    experiment = mlflow.set_experiment(\"pytorch-fmnist-lr-noRBM\")\nelif CLASSIFIER == \"FNN\":\n    experiment = mlflow.set_experiment(\"pytorch-fmnist-fnn-noRBM\")\nelif CLASSIFIER == \"CNN\":\n    experiment = mlflow.set_experiment(\"pytorch-fmnist-cnn-noRBM\")\n\n# Define CNN model\nclass FashionCNN(nn.Module):\n    def __init__(self, filters1, filters2, kernel1, kernel2):\n        super(FashionCNN, self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=filters1, kernel_size=kernel1, padding=1),\n            nn.BatchNorm2d(filters1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(in_channels=filters1, out_channels=filters2, kernel_size=kernel2),\n            nn.BatchNorm2d(filters2),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        self.fc1 = None\n        self.drop = nn.Dropout2d(0.25)\n        self.fc2 = nn.Linear(in_features=600, out_features=120)\n        self.fc3 = nn.Linear(in_features=120, out_features=10)\n        \n\n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = out.view(out.size(0), -1)\n        if self.fc1 is None:\n            self.fc1 = nn.Linear(out.shape[1], 600).to(x.device)\n        out = self.fc1(out)\n        out = self.drop(out)\n        out = self.fc2(out)\n        out = self.fc3(out)\n        return out\n\n\n\n        # Dynamically calculate flattened size\n        out = out.view(out.size(0), -1)  # Flatten\n        if self.fc1 is None:\n            self.fc1 = nn.Linear(out.shape[1], 600).to(x.device)  # ✅ Update FC layer dynamically\n\n        out = self.fc1(out)\n        out = self.drop(out)\n        out = self.fc2(out)\n        out = self.fc3(out)\n        return out\n\n\n\n\n# Define Optuna objective function\ndef objective(trial):\n    batch_size = trial.suggest_int(\"batch_size\", 64, 256, step=32)\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n    mlflow.start_run(experiment_id=experiment.experiment_id)\n    num_classifier_epochs = trial.suggest_int(\"num_classifier_epochs\", 5, 5) \n    mlflow.log_param(\"num_classifier_epochs\", num_classifier_epochs)\n\n    if CLASSIFIER == \"FNN\":\n        hidden_size = trial.suggest_int(\"fnn_hidden\", 192, 384)\n        learning_rate = trial.suggest_float(\"learning_rate\", 0.0001, 0.0025)\n\n        mlflow.log_param(\"classifier\", \"FNN\")\n        mlflow.log_param(\"fnn_hidden\", hidden_size)\n        mlflow.log_param(\"learning_rate\", learning_rate)\n\n        model = nn.Sequential(\n            nn.Linear(784, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, 10)\n        ).to(device)\n\n\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n    elif CLASSIFIER == \"CNN\":\n        filters1 = trial.suggest_int(\"filters1\", 16, 64, step=16)\n        filters2 = trial.suggest_int(\"filters2\", 32, 128, step=32)\n        kernel1 = trial.suggest_int(\"kernel1\", 3, 5)\n        kernel2 = trial.suggest_int(\"kernel2\", 3, 5)\n        learning_rate = trial.suggest_float(\"learning_rate\", 0.0001, 0.0025)\n\n        mlflow.log_param(\"classifier\", \"CNN\")\n        mlflow.log_param(\"filters1\", filters1)\n        mlflow.log_param(\"filters2\", filters2)\n        mlflow.log_param(\"kernel1\", kernel1)\n        mlflow.log_param(\"kernel2\", kernel2)\n        mlflow.log_param(\"learning_rate\", learning_rate)\n\n        model = FashionCNN(filters1, filters2, kernel1, kernel2).to(device)\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n      \n    elif CLASSIFIER == \"LogisticRegression\":\n        mlflow.log_param(\"classifier\", \"LogisticRegression\")\n    \n        # Prepare data for Logistic Regression (Flatten 28x28 images to 784 features)\n        train_features = train_dataset.data.view(-1, 784).numpy()\n        train_labels = train_dataset.targets.numpy()\n        test_features = test_dataset.data.view(-1, 784).numpy()\n        test_labels = test_dataset.targets.numpy()\n    \n        # Normalize the pixel values to [0,1] for better convergence\n        train_features = train_features / 255.0\n        test_features = test_features / 255.0\n    \n    \n        C = trial.suggest_float(\"C\", 0.01, 10.0, log=True)  \n        solver = \"saga\" \n    \n        model = LogisticRegression(C=C, max_iter=num_classifier_epochs, solver=solver)\n        model.fit(train_features, train_labels)\n    \n    \n        predictions = model.predict(test_features)\n        accuracy = accuracy_score(test_labels, predictions) * 100\n        print(f\"Logistic Regression Test Accuracy: {accuracy:.2f}%\")\n    \n        mlflow.log_param(\"C\", C)\n        mlflow.log_metric(\"test_accuracy\", accuracy)\n        mlflow.end_run()\n        return accuracy\n\n    # Training Loop for FNN and CNN\n    criterion = nn.CrossEntropyLoss()\n\n\n    model.train()\n    for epoch in range(num_classifier_epochs):\n        running_loss = 0.0\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images) if CLASSIFIER == \"CNN\" else model(images.view(images.size(0), -1))\n\n            optimizer.zero_grad()\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n\n        print(f\"{CLASSIFIER} Epoch {epoch+1}: loss = {running_loss / len(train_loader):.4f}\")\n\n    # Model Evaluation\n    model.eval()\n    correct, total = 0, 0\n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images) if CLASSIFIER == \"CNN\" else model(images.view(images.size(0), -1))\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    accuracy = 100 * correct / total\n    print(f\"Test Accuracy: {accuracy:.2f}%\")\n\n    mlflow.log_metric(\"test_accuracy\", accuracy)\n    mlflow.end_run()\n    return accuracy\n\nif __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=50)\n    print(f\"Best Parameters for {CLASSIFIER}:\", study.best_params)\n    print(\"Best Accuracy:\", study.best_value)\n\n\nLogistic Regression Test Accuracy: 84.59%\n\n\nLogistic Regression Test Accuracy: 84.49%\n\n\nLogistic Regression Test Accuracy: 84.49%\n\n\nLogistic Regression Test Accuracy: 84.59%\n\n\nLogistic Regression Test Accuracy: 84.57%\n\n\nLogistic Regression Test Accuracy: 84.57%\n\n\nLogistic Regression Test Accuracy: 84.58%\n\n\nLogistic Regression Test Accuracy: 84.53%\n\n\nLogistic Regression Test Accuracy: 84.56%\n\n\nLogistic Regression Test Accuracy: 84.15%\n\n\nLogistic Regression Test Accuracy: 84.41%\n\n\nLogistic Regression Test Accuracy: 84.69%\n\n\nLogistic Regression Test Accuracy: 84.48%\n\n\nLogistic Regression Test Accuracy: 84.45%\n\n\nLogistic Regression Test Accuracy: 84.49%\n\n\nLogistic Regression Test Accuracy: 84.46%\n\n\nLogistic Regression Test Accuracy: 84.33%\n\n\nLogistic Regression Test Accuracy: 84.56%\n\n\nLogistic Regression Test Accuracy: 84.42%\n\n\nLogistic Regression Test Accuracy: 84.36%\n\n\nLogistic Regression Test Accuracy: 84.63%\n\n\nLogistic Regression Test Accuracy: 84.42%\n\n\nLogistic Regression Test Accuracy: 84.68%\n\n\nLogistic Regression Test Accuracy: 84.57%\n\n\nLogistic Regression Test Accuracy: 84.48%\n\n\nLogistic Regression Test Accuracy: 84.46%\n\n\nLogistic Regression Test Accuracy: 84.47%\n\n\nLogistic Regression Test Accuracy: 84.55%\n\n\nLogistic Regression Test Accuracy: 84.62%\n\n\nLogistic Regression Test Accuracy: 84.58%\n\n\nLogistic Regression Test Accuracy: 84.52%\n\n\nLogistic Regression Test Accuracy: 84.68%\n\n\nLogistic Regression Test Accuracy: 84.58%\n\n\nLogistic Regression Test Accuracy: 84.40%\n\n\nLogistic Regression Test Accuracy: 84.52%\n\n\nLogistic Regression Test Accuracy: 84.35%\n\n\nLogistic Regression Test Accuracy: 84.52%\n\n\nLogistic Regression Test Accuracy: 84.53%\n\n\nLogistic Regression Test Accuracy: 84.55%\n\n\nLogistic Regression Test Accuracy: 84.54%\n\n\nLogistic Regression Test Accuracy: 84.57%\n\n\nLogistic Regression Test Accuracy: 84.67%\n\n\nLogistic Regression Test Accuracy: 84.49%\n\n\nLogistic Regression Test Accuracy: 84.50%\n\n\nLogistic Regression Test Accuracy: 84.46%\n\n\nLogistic Regression Test Accuracy: 84.57%\n\n\nLogistic Regression Test Accuracy: 84.45%\n\n\nLogistic Regression Test Accuracy: 84.48%\n\n\nLogistic Regression Test Accuracy: 84.44%\n\n\nLogistic Regression Test Accuracy: 84.41%\nBest Parameters for LogisticRegression: {'batch_size': 96, 'num_classifier_epochs': 5, 'C': 0.35628734474915175}\nBest Accuracy: 84.69"
  }
]